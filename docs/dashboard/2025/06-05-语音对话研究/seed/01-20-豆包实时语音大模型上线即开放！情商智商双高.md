[首页](https://seed.bytedance.com/?view_from=homepage_tab)[Top Seed](https://seed.bytedance.com/topseed?view_from=homepage_tab)[Seed Edge](https://seed.bytedance.com/seed-edge?view_from=homepage_tab)[研究成果](https://seed.bytedance.com/research?view_from=homepage_tab)[·团队动态](https://seed.bytedance.com/blog?view_from=homepage_tab)[加入我们](https://seed.bytedance.com/career?view_from=homepage_tab)

EN

中文

[](https://seed.bytedance.com/?view_from=homepage_tab)[](https://seed.bytedance.com/topseed?view_from=homepage_tab)[](https://seed.bytedance.com/seed-edge?view_from=homepage_tab)[](https://seed.bytedance.com/research?view_from=homepage_tab)[](https://seed.bytedance.com/blog?view_from=homepage_tab)[](https://seed.bytedance.com/career?view_from=homepage_tab)

# 豆包实时语音大模型上线即开放！情商智商双高

豆包实时语音大模型上线即开放！情商智商双高

日期

2025-01-20

分类

技术发布

> 豆包实时语音大模型于今日正式推出，并在豆包 APP 全量开放，将豆包 APP 升级至 7.2.0 版本即可体验。
> 
>   
> 
> **豆包实时语音大模型，是一款语音理解和生成一体化的模型，实现了端到端语音对话**。相比传统级联模式，在语音表现力、控制力、情绪承接方面表现惊艳，并具备低时延、对话中可随时打断等特性。
> 
>   
> 
> 根据外部用户真实反馈，该模型整体满意度较 GPT-4o 有明显优势 ，特别是语音语气自然度和情绪饱满度远高于后者。团队认为，该模型的推出具备里程碑式意义，不仅贴合中国用户实际需求，且发布即上线，有能力直接服务亿万用户，而非停留于演示 Demo 层面。
> 
>   
> 
> **本文将重点介绍模型技术实现思路、特性与优势及评测结果。**
> 
>   
> 
> **技术展示页：**[https://team.doubao.com/realtime_voice](https://team.doubao.com/realtime_voice)

  

今天，豆包 APP 上线全新端到端语音能力，面向所有用户全量开放！

  

其技术能力如何？戳下方视频，抢先了解。

  

  

端到端语音能力加持下，豆包不仅是春节探亲欢聚的神队友，还是献唱《恭喜发财》的高情商歌手：

  

注：可识别“恭喜发财”歌名，并演唱出来。

  

更是陪你唠嗑，操着东北味儿，模仿“白云黑土”的小品达人：

  

注：高情商回应用户呼唤，并能准确模仿经典文艺作品。

  

**这些能力背后，是豆包实时语音大模型。**

  

该模型是一个真正意义上的端到端语音系统，主要面向中文语境和场景（可进行英语对话，暂不支持多语种）。依托于语音和语义联合建模，豆包实时语音大模型拥有丰富表现力和极大拓展潜力，呈现出接近真人的语音表达水准，在语音指令控制的泛化理解和演绎生成方面，显著突破原有边界，且不止停留于 Demo 展示层面，可直接服务广大用户。

  

在外部真实众测中，模型整体满意度较 GPT-4o 有明显优势 ，语音语气自然度和情绪饱满度远高于后者。

  

# 1. 突破真人级语音对话能力的限制

  

真人级语音对话，能提供更为亲和的交互体验和情感价值，是人类迈向 AGI（通用人工智能）的关键里程碑。

  

在过去，传统语音对话任务系统一般采用级联模式——通过 ASR ，将用户输入语音转写成文本，再送入 LLM 生成对话文本，最后，依靠 TTS 转成语音输出。

  

此类系统存在多个缺陷，阻碍了真人级别语音对话交互的实现。例如：对用户情绪及语音中各种副语言信息理解有局限、模型生成语音情绪存在上限、无法遵循语音控制指令、无法实现超低延迟等。

  

除却固有方法的局限，模型对话自然度、有用性及安全性有时此消彼长，相互矛盾。如何找到平衡，促使——模型表现力全面突破同时，保持模型的高智商表现，也成为一大问题。

  

伴随近年来大模型发展，模型架构创新与 Scaling 理念彼此交织，为瓶颈突破带来可能，加之团队过往技术认知的不断积累，构建语音理解和生成一体化模型，真正实现端到端语音对话，由此成为可能。

  

迎着技术浪潮，团队希望——**构建真正可用的端到端语音系统，服务好亿万用户，同时，重新定义未来人机间的交互方式，并用技术给 AI 带来“灵魂”，实现人机之间的情感链接。**

  

为此，团队在研发中尽最大努力，谋求模型交付体验平衡，在保障安全性的基础上，确保其既具备强大的理解和逻辑能力，又能联网回答时效性问题，同时，拥有前所未有的语音高表现力、控制力和优秀的情绪承接能力。此外，模型还需要在实时交互上具备超低延时和流畅打断特性。

  

具体实现方面，团队研发出了一套端到端框架，深度融合语音与文本模态。

  

**该框架面向语音生成和理解进行统一建模，最终实现多模态输入和输出效果。** 在预训练（Pretrain）阶段，团队对各模态交织数据进行深入训练，精准捕捉并高效压缩海量语音信息，通过 Scaling ，最大程度实现语音与文本能力深度融合和能力涌现。在后训练阶段，团队使用了高质量数据与RL算法，进一步提供模型高情商对话能力与安全性，并在“智商”与“情商”之间寻求平衡。

  

# 2. 智商与情商双双在线，赋予 AI 对话“真人感”

  

得益于上述工作，预训练模型具备了丰富多样输入输出的可能性，涵盖 S2S（语音到语音）、S2T（语音到文本）、T2S（文本到语音）、T2T（文本到文本）等多种模式。

  

具体特征表现在如下方面：

  

- **拟人化的情感承接**

  

目前，大多数人工智能仅停留在功能性层面，以响应和服从人类命令为主要交互方式。但人类更渴望拥有像电影《钢铁侠》中贾维斯、《Her》中 Samantha 那样的伙伴，它们能够深刻理解人类的情感、需求与想法，能够与人类产生共情，给予温暖且真挚的陪伴。

  

因此，我们将情感表现力、情感理解、情感承接以及拟人化的语音表达，确立为整个研究过程中最为核心的关键目标，并在不同阶段进行如下工作：

  

**数据收集：** 精心筛选并整理了大量包含丰富情感的语音数据，涵盖各种场景与情绪状态，为模型训练提供充足且优质素材。

  

**预训练：** 使用大量各模态交织数据深度训练，并专门设计算法和优化策略，促使模型能精准捕捉和学习语音中的情感特征。

  

**后训练：** 进一步通过真实与高质量合成的语音对话数据优化模型，使其实现高情商共情式对话。

  

目前，我们已取得阶段性成果。举例来说，当用户表现出不开心时，模型会以安慰语气说出暖心话语，当用户情绪高涨时，模型则以快乐语气作出积极回应，而当用户开玩笑时，模型能够接住用户的内容与情绪，输出恰当表达。

  

注：面向亲子关系，拥有高度拟人化的共情、理解力，并具备极好的角色代入能力。

  

- **强大的声音控制和丰富的情感演绎能力**

  

除却拟人化的情感表达，团队还希望让模型具备声音控制、角色演绎、唱歌等一系列实用又出彩能力，进一步提升用户体验。

  

声音控制方面，模型不仅能依照基础指令输出，还可遵循丰富的复杂指令。

  

注：可遵循大段复杂指令，“听到”关键词后，可迅速进入角色。

  

情绪控制和表现力堪比专业级演员，即便音色上的细腻调整也能拿捏到位。

  

注：可在喜怒哀乐之间，进行快速情绪切换。

  

通过学习角色语音和情感特点，模型还具备强大的讲故事能力，在对话或内容演绎中，可生动切换成不同角色/状态，配合不同情绪表达，增强交互趣味性和沉浸感。

  

联合建模后，模型涌现出超出预期的指令理解、声音扮演和声音控制能力。比如，目前模型部分方言和口音，主要源自于 Pretrain 阶段数据泛化，而非针对性训练。

  

注：不仅能编故事，而且能代入角色说话特点，呈现声情并茂。

  

- **智商与表现力之间的平衡**

  

豆包实时语音大模型的语音智商，体现在模型在用户语音输入阶段，对各维度信息进行深度理解，输出信息具备有用性与真实性。同时，输出语音表现力高度逼近真人，包括类人的副语言特征（如语气词、停顿思考等）。

  

为实现该能力，我们在数据层面和后训练算法上，确保多模态语音对话数据兼具语义正确性与表现力的自然性。同时，采用多轮数据合成方法，以生产高质量、高表现力的语音数据，实现了模型智商与表现力的平衡与统一，确保生成语音表达自然且一致。

  

通过定期对模型进行多维度评测，团队还会依托评测结果，及时调整训练策略和数据使用方式，确保模型在智商和表现力之间始终保持良好平衡。

  

此外，我们还赋予模型实时联网功能，能够根据问题，动态获取最新信息，对时效问题给到精准、及时的回应。

  

注：豆包能及时联网查询最新赛事信息，并能将赛事变动与火灾新闻进行关联回复。

  

- **丝滑的交互体验和超低延迟**

  

在真人级语音对话中，丝滑顺畅的交互体验与超低延时至关重要，目前，级联系统的高延迟使实时对话连贯性不足，极大影响模型表现。

  

在语音生成，理解与文本大模型联合建模的框架下，我们实现了生成侧模型在更低系统时延情况下的生成准确性、自然度，同时在理解侧，该框架让模型实现了敏锐的语音打断与用户对话判停能力。

  

- **安全方面挑战与解决方案**

  

多模态的引入，为模型安全性提出全新要求。

  

具体来看，当以语音作为输入，模型需要保证同一安全准则对于不同语音表述均生效。同时，当语音作为输出时，也会带来新的安全问题。此外，模型还需要——在不同场景下以恰当语气表达内容，并解决语音和文本存在多对一的关系下的安全挑战。

  

对于上述问题，团队非常重视。在联合建模的过程中，我们在后训练阶段，引入多种安全机制，通过对潜在非安全内容进行有效压制和过滤，降低安全风险。

  

当然，安全能力提升并非一蹴而就，而是一个复杂的课题，我们将在未来持续深入研究，长期投入。

  

# 3. 评测结果

  

评测中，团队选取数十名外部测试者，面向 270 个话题组，共收集超过 800 通中文数据。

  

这些测试者来自 10 个城市，其中 9 名男性，女性 18 名，年龄分布为 21-33 岁。11.11% 的测试者从未体验过豆包 APP，70.37% 为轻度用户，每周使用 1-2 天，其余粘度较高。

  

团队围绕拟人度、有用性、情商、通话稳定性、对话流畅度等多个维度进行考评。整体满意度（以 5 分为满分）方面，豆包实时语音大模型评分为 4.36，GPT-4o 为 3.18。其中，50% 的测试者对豆包实时语音大模型表现打出满分。

  

![image](https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/user-upload/3r7ojm64uaqeh.jpeg)

  

此外，在模型优点评测中，豆包实时语音大模型在情绪理解和情感表达方面优势明显。尤其是“一听就是 AI 与否”评测中，超过 30% 的反馈表示 GPT-4o “过于 AI ”，而豆包实时语音大模型相应比例仅为 2% 以内。

  

**由上可见，豆包实时语音大模型在智商与情商表现符合预期。** 尤其情商层面，模型在情感理解、情感承接以及情感表达等方面也取得显著进展，能较为准确地捕捉、回应人类情感信息。

  

# 4. 写在最后

  

基于以上技术突破和成熟落地表现，我们相信，豆包实时语音大模型为语音多模态技术的未来应用树立了全新标杆，为后续的研究和优化提供了坚实基础。

  

同时，团队也意识到，尽管模型已初步展现出探索潜力，但其能力边界仍存在诸多不确定性。比如语种方面，目前模型主要支持中文，其他语种尚未较好支持。中文范围内，模型也仅支持小部分方言和地方口音的理解和表达，仍有较大进步空间。此外，安全性课题同样需要长期投入。

  

在未来研究中，我们希望进一步挖掘模型潜力，通过优化算法、扩充数据以及改进训练策略等手段，逐步拓展其能力边界，提升复杂场景下的适应性和表现力。

  

豆包大模型语音团队的使命是利用多模态语音技术丰富交互和创作方式，如果你也对相关工作感兴趣，欢迎前往[豆包大模型语音团队网页](https://team.doubao.com/zh/direction/speech)，了解更多团队信息。

![](https://lf-flow-web-cdn.doubao.com/obj/flow-doubao/deploy/flow/ai_official_website/88329/static/svg/footer_logo.c9947ae1.svg)

和优秀的人，做有挑战的事

欢迎加入字节跳动 Seed

[用户协议](https://seed.bytedance.com/user-agreement)[隐私政策](https://seed.bytedance.com/privacy-policy)

联系我们

![](https://lf-flow-web-cdn.doubao.com/obj/flow-doubao/deploy/flow/ai_official_website/88329/static/image/footer_qr.6675c2da.png)

关注字节跳动 Seed 团队，了解最新技术进展、研究成果和招聘信息

Copyright © 2025 Bytedance Seed