## **00.** DeepSeek 导读

Lex Fridman（莱克斯 · 弗里德曼）是麻省理工学院（MIT）的人工智能研究员、播客主持人及多领域跨界专家。他出生于俄罗斯莫斯科，拥有计算机科学博士学位，研究方向涵盖深度学习、自动驾驶、人机交互等，并在 MIT 教授相关课程。他因高质量的访谈播客和极端的自律生活方式广受关注，曾采访过埃隆 · 马斯克、扎克伯格等科技界领袖，同时也是巴西柔术黑带选手和音乐爱好者。

关于他讨论中国 AI 公司 **DeepSeek** 的播客（第 459 期），主要内容聚焦于以下几点：  
1. **DeepSeek 的技术突破 **  
   - 重点分析了 DeepSeek 的开放权重模型 **R1** 和 **V3**，指出其通过混合专家模型（Mixture-of-Experts）和 “多头潜在注意力” 架构显著降低了训练与推理成本，同时保持高性能。  
   - 对比 OpenAI 同期发布的推理模型 **o3-mini**，Lex 指出 DeepSeek R1 以十分之一的成本实现了同等性能，并认为其开放透明性为行业树立了新标杆。  

2. ** 成本与地缘影响 **  
   - DeepSeek 的低成本得益于硬件优化（如自研计算集群）和算法创新，这对全球 AI 竞争格局产生冲击，尤其在中美技术博弈背景下，开放权重模型可能削弱西方在 AI 领域的传统优势。  
   - 播客还讨论了 GPU 出口管制、台积电在芯片制造中的角色，以及 AI“技术冷战”的可能性。  

3. ** 未来展望 **  
   - Lex 与嘉宾探讨了通用人工智能（AGI）的时间线，认为 DeepSeek 的低成本路径可能加速技术落地，但需警惕过度自主化系统的风险。  

Lex 在社交平台评价称，**“DeepSeek 时刻”** 将成为技术突破与地缘博弈交织的历史性事件。该播客完整版可通过 [Lex Fridman 官网](https://lexfridman.com/deepseek-dylan-patel-nathan-lambert) 或中文翻译平台（如[小宇宙 FM](https://www.xiaoyuzhoufm.com/episode/670f49a0db2cf827579733d6)）获取。

> Lex Fridman 是我非常喜欢的博客主，恰好又做了我们 DeepSeek 的一期节目，兴趣驱动花费 6 小时 + 200 元独立实现了视频转翻译文稿过程，精力有限，正文内容无法做到人工校对，还请见谅。如果对您有帮助，还望多多打赏多多支持，后续可以带给大家更多高质量内容！
>
> 本文工作流：
>  - 视频转音频：- [yt-dlp/yt-dlp: A feature-rich command-line audio/video downloader](https://github.com/yt-dlp/yt-dlp)
>  - 音频转文本： [AssemblyAI | Home](https://www.assemblyai.com/app)
>  - 翻译： [讯飞文档翻译 - 上传 PDF/Word/Excel/PPT](https://fanyi.xfyun.cn/console/trans/doc)
>  - 格式化： [vinta/pangu.js: Paranoid text spacing in JavaScript](https://github.com/vinta/pangu.js)
>  - 编辑： [Obsidian - Sharpen your thinking](https://obsidian.md/)
>  - 排版： [墨滴 | 看颜值的文章社区](https://www.mdnice.com/)
>
> 本文代码存档： [MarkShawn2020/2025-02-03_lex-fridman-deepseek: Lex Fridman 关于 DeepSeek 播客代码库](https://github.com/MarkShawn2020/2025-02-03_lex-fridman-deepseek)

```insta-toc
---
title:
  name: 全文目录：
  level: 1
  center: false
exclude: ""
style:
  listType: dash
omit: []
levels:
  min: 1
  max: 6
---

# 全文目录：

- 00. DeepSeek 导读
- 01. 背景介绍
- 02. DeepSeek-R1 and DeepSeek-V3
- 03. 低成本训练
- 04. DeepSeek 计算集群
- 05. 对中国的 GPU 出口控制
- 06. AGI 时间线
- 07. 中国的生产能力
- 08. 美中冷战
- 09. 台积电与台湾
- 10.  最好的 AI GPU
- 11. 为什么 DeepSeek 这么便宜
- 12. 间谍
- 13. 审查制度
- 14. Andrej Karpathy 与强化学习
- 15. OpenAI o3-mini vs DeepSeek R1
- 16. 英伟达（与它的股票）
- 17. GPU Smuggling（走私）
- 18. 蒸馏：DeepSeek 基于 OpenAI 的数据进行训练
- 19. AI Megaclusters（巨型集群）
- 20. 谁是 AGI 的最后赢家
- 21. AI Agents
- 22. AI 与编程
- 23. 开源
- 24. Stargate （OpenAI 星际之门）
- 25. AI 的未来
```

![image.png](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031711744.png?x-oss-process=image/resize,w_800)



## **01.** 背景介绍


**Lex Frdiman：** 以下是与 Dylan Patel 和 Nathan Lambert 的对话。

- Dylan 经营着 SemiAnalysis，这是一家备受尊敬的研究和分析公司，专注于半导体、GPU、CPU 和人工智能硬件。
![image.png](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031705972.png?x-oss-process=image/resize,w_800)

- Nathan 是艾伦人工智能研究所（Allen Institute for AI）的一名研究科学家，也是人工智能博客 “互联”（Interconnect）的作者。
![image.png](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031706060.png?x-oss-process=image/resize,w_800)

他们都受到人工智能领域的专家、研究人员和工程师的高度尊重、阅读和聆听。就个人而言，我只是他们两个的粉丝。因此，我利用这一震撼人工智能世界的深度探索时刻，作为一个机会，与他们坐下来，把一切都摆出来。

从 DeepSeek、OpenAI、Google、XAI、Anthropic，到英伟达（Nvidia）和台积电（TSMC），再到美中关系、台湾关系以及人工智能前沿正在发生的一切，本次对话深入探讨了人工智能行业的许多关键方面。

虽然它的技术含量很高，但我们试图通过定义术语、陈述重要概念、明确说明缩略语，以及通常总是跨越几个抽象层和细节层次，来确保人工智能领域以外的人仍然可以访问它。

媒体上有很多关于人工智能是什么和不是什么的炒作。这个播客的部分目的是通过胡说八道和低分辨率分析来减少炒作，并详细讨论这些东西是如何工作的，以及其含义是什么。

另外，如果我可以的话，请允许我评论一下新的 OpenAI o3-mini 推理模型，我们在谈话中期待着它的发布，它确实是在其功能和成本与我们的预期相当之后发布的。

正如我们所说的，**OpenAI o3-mini 确实是一个伟大的模型，但应该指出的是，DeepSeek R1 在基准测试中具有类似的性能，仍然更便宜，并且它揭示了 o3-mini 所没有的思维推理。它只显示了推理的摘要。另外，R1 是开源的而 o3-mini 不是。**

顺便说一句， 我有机会玩 o3-mini。我觉得 o3-mini，特别是 o3-mini High 比 R1 更好。尽管如此，就我个人而言，我发现 **Claude Sonnet 3.5 是最好的编程模型，除了我将使用 o1-pro 进行头脑风暴的棘手情况 **。

无论哪种方式，更多更好的人工智能模型将会出现，包括来自美国和中国公司的推理模型。它们将继续改变成本曲线，但 **DeepSeek Moment 确实是真实的 **。我认为，** 五年后，它仍将作为科技史上的一个关键事件被人们铭记，部分原因是地缘政治影响，但也有其他原因 **。

正如我们在这次对话中从多个角度详细讨论的那样，这是 Lex Fridman 的播客。为了支持它，请在描述中查看我们的赞助商。

现在，亲爱的朋友们，这是 Dylan Patel 和 Nathan Lambert。很多人都想了解中国的 DeepSeek 人工智能模型。所以让我们把它摆出来。

![image.png](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031704883.png?x-oss-process=image/resize,w_800)

Nathan，你能描述一下 DeepSeek V3 和 DeepSeek R1 是什么吗，它们是如何工作的，它们是如何训练的？

让我们先看大图，然后再放大细节。

![image.png](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031702211.png?x-oss-process=image/resize,w_800)

![image.png](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031703469.png?x-oss-process=image/resize,w_800)



## **02.** DeepSeek-R1 and DeepSeek-V3

**Nathan Lambert：** 是的。因此，DeepSeek V3 是来自中国的 DeepSeek 的一种新的专家混合 Transformer 语言模型。他们在模型中有一些新的细节，我们将进入。很大程度上，这是一个开放的权重模型，它是一个指令模型，就像你在 ChatGPT 中使用的一样。他们还发布了所谓的基础模型，这是在这些训练后的技术之前。如今，大多数人都在使用指令模型，这些模型在各种应用程序中都得到了应用。我相信这是在 12 月 26 日或那一周发布的。几周后，1 月 20 日，DeepSeek 发布了 DeepSeek R1，这是一个推理模型，它确实加速了很多讨论。这个推理模型有很多重叠的训练步骤来 DeepSeek V3。令人困惑的是，你有一个叫做 V3 的基本模型，你做了一些事情来得到一个聊天模型，然后你做了一些不同的事情来得到一个推理模型。我认为很多人工智能行业正在经历这种通信的挑战，OpenAI 正在取笑他们自己的命名方案。他们有 GPT4O，他们有 OpenAI01，还有很多类型的模型。所以我们将分解它们中的每一个。训练中有很多技术细节，从高水平到具体，每一项都要经历。
**Lex Frdiman：** 这里有很多地方我们可以去，但也许让我们先去公开举重。开放权重的模型意味着什么？开源通常有哪些不同的风格？
**Nathan Lambert：** 是的，这个讨论在人工智能领域已经持续了很长时间。自从 ChatGPT 以来，它变得更加重要，或者自从 2022 年底的 ChatGPT 以来，它变得更加重要。开放权重是公认的术语，当语言模型的模型权重在互联网上可供人们下载时，这些权重可以有不同的许可证，这是你可以使用模型的有效条款。在开源软件中有来自历史的许可证。有些许可证是由公司设计的，特别是所有的 Llama，DeepSeek，Qwen，Mistral。这些流行的名字在开放的重量模型有一些自己的许可证。这很复杂，因为不是所有相同的模型都有相同的术语。最大的争论是什么使模型开放的重量。我们为什么要说这个术语？有点拗口。它听起来很接近开源，但它不一样。关于开源人工智能的定义和灵魂，仍然有很多争论。开源软件在自由修改方面有着丰富的历史，你可以自由地使用自己的软件，不受任何限制。这对人工智能的意义仍在定义中。所以我在艾伦人工智能研究所工作。我们是非营利组织。我们想让人工智能对所有人开放。我们试图引领我们认为是真正开源的东西。在社区中没有完全达成一致，但对我们来说，这意味着发布训练数据，发布训练代码，然后也有像这样的开放权重。我们将一次又一次地深入模型的细节，因为我们试图更深入地了解模型是如何训练的，我们会说数据处理，数据过滤，数据质量是模型质量的首要决定因素。然后很多训练代码是决定训练时间和实验速度的决定因素。因此，如果没有完全开源的模型，你可以访问这些数据，很难知道或者很难复制。 因此，我们将讨论 DeepSeek V3 在大部分 GPU 时间上的成本数字，以及您自己可以支付多少租金。但如果没有数据，复制成本将会高得多。代码也是一样。
**Lex Frdiman：** 我们也应该说，这可能是前沿模型中比较开放的模型之一。所以就像在这个完整的范围内，可能是最完整的开源，就像你说的，开放代码，开放数据，开放权重，这不是开放代码，这可能不是开放数据，这是开放权重，许可是 MIT 许可。我的意思是在不同的模型中有一些细微的差别，但就开源运动而言，它是免费的。他们都是好人。
**Nathan Lambert：** 是的，DeepSeek 在传播对人工智能的理解方面做了出色的工作。他们的论文非常详细地描述了他们所做的事情。对于世界各地的其他团队来说，他们在提高自己的训练技术方面是非常可行的。我们将更多地讨论许可证。DeepSeek R1 模型具有非常宽松的许可证。它被称为麻省理工学院许可证。这实际上意味着商业使用没有下游限制，没有用例限制。您可以使用模型的输出来创建合成数据。这一切都太棒了。我认为最接近的同行是像 Llama 一样的东西，你有重量，你有技术报告。技术报告对 Llama 来说非常好。去年阅读量最大的 PDF 文件之一是《Llama 3》。但在某些方面，它的可执行性略低。它在训练细节上的细节较少，情节较少等等。Llama 3 许可证比 MIT 更严格。然后在深海海关许可证和骆驼许可证之间，我们可以进入整个兔子洞。我想在我们做细节之前，我们会确保我们想要进入许可证的兔子洞。
**Lex Frdiman：** 是的。我的意思是，应该说明的是，DeepSeek 对 Llama 和 OpenAI 上的其他所有人施加了压力，以推动开源。这就是你提到的开源的另一面，那就是有多少关于它的细节被公布。所以你对代码背后的见解有多开放。比如技术报告有多好？他们的手是波浪形的还是有实际的细节？这是 DeepSeek 做得很好的事情之一，因为他们公布了很多细节。
**Nathan Lambert：** 是的，特别是在 DeepSeek V3 中，这是他们的训练前文件，他们非常清楚，他们正在对许多不同级别的技术堆栈进行干预。例如，为了获得高效的训练，他们在 NVIDIA 芯片的 CUDA 层或以下进行修改。我自己从来没有在那里工作过，世界上有几个人做得很好，他们中的一些人在 DeepSeek，这些人在深海和领先的美国前沿实验室。但是去的地方不多。
**Lex Frdiman：** 帮助人们理解开放式砝码的其他含义。只是，你知道，有一个话题我们经常回到这里。因此，人们担心中国可能有兴趣窃取美国数据，侵犯美国公民的隐私。关于公开权重，我们能说些什么来帮助我们理解权重在窃取人们的数据方面能够做什么？
**Nathan Lambert：** 是的。所以你可以从拥抱脸或其他平台下载的这些权重是非常大的数字矩阵。你可以把它们下载到你家里没有互联网的电脑上，你可以运行这个模型，你可以完全控制你的数据。这与今天许多语言模型的实际使用方式不同，后者主要是通过 API 将提示发送给某些公司运行的数据 GPU。这些公司会有不同的分布和政策，关于你的数据是如何存储的，如果它被用来训练未来的模型，它存储在哪里，如果它被加密，等等。因此，开放权重是指数据的命运掌握在自己手中，这与开源的灵魂有着深刻的联系。
**Lex Frdiman：** 所以不是模型窃取了你的数据，而是 Qwen 托管了模型，如果你使用 DeepSeek 应用程序，可能是中国。也可能是困惑。你知道，你信任他们你的数据或 OpenAI，你信任他们你的数据。有些是美国公司，有些是中国公司。但模型本身并没有进行窃取，它是宿主。好了，回到最基本的。DeepSeek V3 和 DeepSeek R1 之间有什么区别？我们能不能试着把潜在的困惑摆出来？
**Nathan Lambert：** 是的。首先，我非常理解许多人对这两个型号名称的困惑。所以我想说，最好的方法是，当你训练一个语言模型时，你有所谓的预训练，即当你预测大量的互联网文本时，你试图预测下一个标记。关于这些新的 DeepSeek 模型，我们要知道的是，他们在互联网上进行了一次大规模的预训练，以获得所谓的 DeepSeek V3 基础。这是一个基本模型。它只是帮你把话说完。它将比 ChatGPT 更难使用。然后 DeepSeq 所做的是他们做了两种不同的训练后制度，使模型具有特定的理想行为。那么，就过去几年的人工智能而言，更正常的模式是什么？指导模型、聊天模型、对齐模型、帮助模型。有很多方法可以描述这是更标准的岗位训练。所以这就像是指令调整，强化，从人类反馈中学习。我们将讨论其中的一些词。这就是他们创建 DeepSeek V3 模型所做的工作。这是第一个发布的模型，它的性能非常高，可以与 GPT4、Llama、405B 等竞争。然后当这个版本发生时，我们不知道他们的确切时间表，或者在他们完成训练后不久，从我谈到的基于相同的下一个令牌预测模型的不同训练过程。这就是人们听说的这种新的推理训练出现的时候，为了创建被称为 DeepSeek R1 的模型。这段对话中的 R 有助于为推理打下基础。这个名字也类似于 OpenAI 的 O1，这是人们听说过的另一个推理模型。我们必须更详细地分解 R1 的训练，因为我们有一篇论文详细介绍了它，但它也是人工智能社区的一套更新的技术。 所以这是一个发展更快的研究领域。
**Lex Frdiman：** 也许我们还应该说训练的两大类别，即训练前和训练后，这是人们使用的总称。那么，什么是训练前，什么是训练后，以及训练背后有什么不同的东西？岗位训练伞？
**Nathan Lambert：** 是的，所以在训练之前，我使用了一些相同的词语来真正传达信息，即你在做所谓的自回归预测来预测一系列文档中的下一个标记。这是在标准做法是数万亿代币上完成的。所以这是一大堆数据，大部分是从网络上搜集来的。在 DeepSeek 早期的一些论文中，他们谈到他们的训练数据是为数学而提取的。我还不应该使用这个词，但取自 Common Crawl，这是一个公共访问，任何人都可以从 Common Crawl 网站下载数据。这是一个公开维护的爬虫。是的，其他科技公司最终会转向自己的爬虫，Deepseak 很可能已经做到了这一点，就像大多数前沿实验室一样。但这类数据是人们可以开始使用的，你只是预测一系列文档中的文本。这可以扩展为非常高效。在人工智能训练中，有很多数字被抛出，比如使用了多少浮点运算或 FLOPS。然后你也可以看看这些 GPU 使用了多少小时。这在很大程度上是一个损失函数，需要大量的计算机使用。你建立了真正高效的系统，然后在最后你有了这个基本模型。而预训练是在过程如何出现或发展以及您将使用的不同类型的训练损失方面有更多的复杂性。我认为这是基于自然语言处理文献的许多技术。至今仍在使用的最古老的技术是指令调优，也称为监督微调。这些缩写将是 IFT 或 SFT。人们真的来来回回，我可能也会这样做。这是您将此格式添加到模型的地方， 它知道回答一个问题，比如向我解释罗马帝国的历史，或者你会在 Reddit 或 Stack Overflow 上看到的问题，然后模型会以一种信息密集但像样的方式回答。格式化的核心就在这个指令调优阶段。今天还有另外两种类型的损失函数。我将其归类为偏好微调。偏好微调是一个广义的术语，指的是来自人类反馈的强化学习，即 RLHF。这种来自人类反馈的强化学习被认为是帮助 ChatGPT 取得突破的技术。这是一种技术，可以让回复像 Reddit 上的这些答案一样有很好的格式，更符合人们想要阅读的内容。这是通过从世界上真实的人那里收集成对的偏好来开始的。现在人工智能也在标记这些数据，我们将进入这些权衡，你有这种好答案和坏答案之间的对比损失函数。模型学会了捕捉这些趋势。有不同的实现方式。你有所谓的奖励模式。你可以用直接对齐算法。你可以做很多非常具体的事情。但所有这些都是关于人类偏好的微调。最后一个阶段要新得多，我们将链接到 R1 中所做的工作。这些推理模型，我认为是 OpenAI 的名字。他们在秋天有了这个新的 API，他们称之为强化微调 API。这是你使用强化学习技术的想法，这是人工智能的整个框架。这里有一篇很有深度的文献可以总结。它通常被称为试错学习或人工智能的子领域，你试图在某个潜在的嘈杂环境中做出连续的决定。我们有很多方法可以做到这一点。 但是微调语言模型，它们可以生成一个答案，然后你检查答案是否与数学或代码的真正解决方案相匹配，你有一个完全正确的数学答案，你可以对代码进行单元测试。我们正在做的是检查语言模型的工作，我们在同一个问题上给它多次机会，看看它是否正确。如果你坚持这样做，模型可以在很大程度上学习改进可验证的领域。它真的很好用。在学术文献中，这是一种较新的技术。多年来，美国的前沿实验室（Frontier Labs）一直在使用它，而不是分享每一个细节。这就是用语言模型进行强化学习的想法。它一直在起飞，特别是在这个深层次的时刻。
**Lex Frdiman：** 我们应该说，有很多令人兴奋的事情再次发生在整个堆栈上。但是岗位训练，可能今年在岗位训练方面会有很多有趣的发展。我们会，我们会谈谈的。我差点忘了谈论 DeepSeek V3 和 R1 在用户体验方面的区别。所以忘记技术上的东西，忘记所有这些。只是那些对人工智能一无所知的人，他们表现得像是实际体验是什么？当他们真正喜欢类型并与之交谈时，每个人的用例是什么？每个人擅长什么？还有那种事。
**Nathan Lambert：** 所以让我们再次从 DeepSeek V3 开始。这是更多人会尝试的。就像它一样，你问它一个问题，它会很快开始生成标记，这些标记看起来就像一个非常人性化的清晰答案。这将是某种减价清单。它可能有格式来帮助您绘制答案中的核心细节，并且它将生成数十到数百个标记。标记通常是常用词的单词或较长单词中的子词部分。它看起来像一个非常高质量的 Reddit 或堆栈溢出答案。这些模型真的很擅长在各种各样的领域做这些事情。我认为，即使是那些你是专家的事情，那些接近知识边缘的事情，他们仍然会相当擅长。我认为我研究的前沿人工智能课题。这些模型能够帮助学习，并定期更新。这一变化与 DeepSeek R1 有关。所谓的这些推理模型是，当你看到来自这些模型的标记时，它将是一个庞大的思维过程链。我们马上回到思维链，看起来有很多标记，模型解释了问题，模型通常会分解问题，就像，好的，他们问我这个，让我们分解问题，我需要这样做。你会看到所有这些都是从模型中生成的。很快就会来的。在大多数用户体验中，这些 API 都非常快。所以你会看到很多标记，很多单词出现得非常快。它将继续在屏幕上流动，这是所有的推理过程。然后最终模型会改变它在 R1 中的语气，它会写出答案，它总结了它的推理过程，并写出与第一种模型类似的答案。但在 DeepSeek 的例子中，这也是为什么它在人工智能社区之外如此受欢迎的部分原因，因为你可以看到语言模型是如何分解问题的，然后你就可以得到这个答案。 在技术方面，他们训练模型来做这件事，特别是在他们有一个推理部分的地方，然后它生成一个特殊的标记，这个标记可能在大多数时候对用户是隐藏的，它说，好的，我开始回答了。因此，该模型被训练为独立完成这两个阶段的过程。如果你在 OpenAI 中使用一个类似的模型，OpenAI 的用户界面试图为你很好地总结这个过程，通过显示模型正在做的部分，它会点击，它会说，分解问题，进行 X 计算，清理结果，然后答案就会出现，比如 OpenAI，也许它是有用的。
**Lex Frdiman：** 这里有一个 DeepSeek R1 推理的例子。
**Nathan Lambert：** 是的，如果你看这里的屏幕，你会看到一个深海聊天应用程序的屏幕截图。顶部是 1517 秒，下面是下拉箭头。如果我们在一个正在运行的应用程序中，下拉箭头就会有理由。
**Lex Frdiman：** 所以在这种情况下，具体的问题，我是哲学上的瘾君子，所以这是问 DeepSeek R1 关于人类的一个真正新颖的见解，它揭示了推理。基本上，真正新颖的方面是推动推理的东西，不断地让模型问自己，这是真正新颖的吗？所以它实际上是在挑战自己，变得更新颖，更违反直觉，更。少点畏缩，我想。所以一些推理说这只是快照。或者，人类有一种独特的元情绪，他们对自己的情绪有感觉，例如，对生气感到内疚。这种递归的情绪分层创造了复杂的动机驱力，这在其他动物身上是不存在的。洞见是人类的情感是嵌套的。所以这就像是通过人类如何感受情感来进行推理，这是关于元情感的推理。
**Dylan Patel：** 会有一页又一页的内容。它几乎太多了，无法真正阅读，但当它出现时，浏览一下是很好的。
**Lex Frdiman：** 这是一种意识流，就像詹姆斯 · 乔伊斯的意识流。然后它说，等等。用户想要一些在其他地方看不到的东西。让我更深入地挖掘并考虑人类同时持有矛盾信念的能力。认知失调是众所周知的，但也许其功能是允许灵活的适应，等等。我的意思是，这真的抓住了公众的想象力，天啊，这不明智。几乎就像一种知觉的暗示，因为你在思考，你在自我反省，你在深思熟虑。157 秒后的最终结果是，人类通过集体假装抽象的规则、金钱、法律和权利是真实的，本能地将自私的欲望转化为合作系统。这些共同的幻觉就像引用游戏一样，竞争被秘密地重新定向，以使群体受益，将冲突转化为社会的燃料。相当深刻。我是说，你知道，这是一个。
**Nathan Lambert：** 评论离题，但很多人发现，这些推理模型有时可以产生更有说服力的文本。这至少是一个有趣的例子。我认为取决于你的思想有多开放，你会发现语言模型是否有趣。这里有一个光谱。
**Lex Frdiman：** 嗯，我的意思是，这是一些，我们将讨论不同的基准等等，但有些只是一种氛围，就像它本身是一个，让我们说，引用，火的推文。是的，如果我试图生产一些东西，人们会说，哦，妈的，好吧，这就是思维的链条。我们可能会更多地回到它。他们是如何在训练和推理上实现如此低的成本的？也许你可以先谈谈训练。

## **03.** 低成本训练

**Dylan Patel：** 是的，所以他们实现了两个主要技术，这可能是他们效率的主要部分，然后还有很多实现细节，我们可能会忽略或稍后讨论，这对它有一定的贡献。但这两件主要的事情是。一个是他们采用了混合专家模型，我们马上就会对其进行定义。另一件事是他们发明了一种叫做 MLA 潜在注意力的新技术。这两件事都是大事。专家的混合已经在文献中出现了几年了，而 OpenAI 和 GPT4 是第一个将专家混合模型产品化的产品。这意味着当你看到周围的常见模型时，大多数人都能够与之互动，这是开放的，对吧？想想 Llama 。Llama 是一个密集的模型。也就是说，每一个参数或神经元都被激活，就像你在模型中生成的每一个令牌一样。对的？现在，在混合专家模型中，你不会这样做，对吗？人类实际上是如何工作的？
**Nathan Lambert：** 对吗？
**Dylan Patel：** 就像，哦，当我思考视觉任务或其他事情时，我的视觉皮层是活跃的，对吗？我的扁桃体是我害怕的时候，对吗？你大脑的这些不同方面专注于不同的事情。专家模型的混合试图在某种程度上接近这一点。它与大脑的结构相差甚远。但模型的不同部分会激活，对吧？模型中有一定数量的专家，并且每次都有一定数量的专家被激活。这大大降低了你的训练和推理成本。因为现在如果你把参数计数看作是你在训练过程中压缩的所有知识的总嵌入空间，当你嵌入这些数据时，而不是每次训练或运行推理时都必须激活每个参数，现在你可以只激活一个子集，模型将学习不同任务的专家。所以这是一个巨大的创新，我可以继续增加参数的总嵌入空间。所以 DeepSeek 的模型是 6000 亿个参数，对吧？相对于 Llama 405b，它有 4050 亿个参数，对吗？拉玛相对于拉玛 70b，它是 700 亿个参数，对吗？所以这个模型在技术上有更多的信息嵌入空间，以压缩互联网上所有的世界知识。但与此同时，它只激活了大约 370 亿个参数。因此，每次训练数据或从中推断数据时，实际上只需要计算这些参数中的 370 亿个。因此，与骆驼模型相比，700 亿个参数必须被激活，或者 4050 亿个参数必须被激活。因此，当您使用这种混合专家体系结构进行训练和推理时，您已经显著降低了计算成本。
**Nathan Lambert：** 我们是不是应该把它分解到实际应用的地方，然后进入 Transformer？那有用吗？
**Lex Frdiman：** 我们走，我们走进 Transformer。
**Nathan Lambert：**Transformer 是一个谈论很多的东西，我们不会涉及每一个细节。从本质上讲，Transformer 是建立在这种注意力机制的重复块上，然后是传统的密集全连接多层感知器。无论你想用什么词来形容你的正常神经网络。你交替使用这些块。还有其他细节，在这个密集模型中应用了专家的混合。如果在 Transformer 模型中计算权重，则密集模型包含大部分权重。所以你可以从这些专家在训练和推理中的参数效率中获得很大的收益，因为你可以通过不激活所有这些参数来获得这种效率。
**Lex Frdiman：** 我们也应该说 Transformer 是一个巨大的神经网络。15 年来，出现了所谓的深度学习革命。网络变得越来越大，在某一点上，标度定律出现了，人们意识到这是一件标度定律衬衫，顺便说一下，它代表标度定律。在那里，它变得越来越正式，在更大的含义的多个维度上，越大越好。但这些都是我们正在讨论的神经网络。我们正在讨论如何构建这些神经网络的不同架构，以便对它们进行训练和推理是非常有效的。
**Nathan Lambert：** 是的，每一种不同类型的模型都有不同的缩放法则，这对于你在架构中投入多少计算将在测试任务中获得不同的性能水平是有效的。专家的混合是训练时间的一部分。即使你不考虑推理的好处，这在训练时也是很大的。通过使用这种架构，如果实施良好，您的 GPU 效率将得到显著提高。因此，您可以有效地获得相同的性能模型和评估分数，而计算减少 30%。我认为根据你的实现细节和材料，会有很大的变化。但重要的是要认识到，这种类型的技术创新会带来巨大的收益。我希望大多数为他们的模型提供服务的公司都能转向这种混合专家实施。从历史上看，不是每个人都会这么做的原因是因为它的实现很复杂，尤其是在做这些大模型的时候。所以这是 DeepSeek 获得赞誉的原因之一，他们在这方面做得非常好。他们把专家混合得非常好。这个架构被称为 DeepSeek MOE，MOE 是多篇旧论文的专家混合的缩略版。他们的训练基础设施的这一部分对这些模型来说并不陌生。迪伦提到的多头潜在注意力也是如此。这一切都是为了减少推理过程中的内存使用，以及通过使用一些花哨的低秩近似数学来减少训练过程中的内存使用。如果你带着这种潜在的注意力进入细节，这是我看到的其中一件事，就像，好吧，他们正在做非常复杂的实现，因为语言模型的其他部分，比如用于扩展上下文长度的嵌入。DeepSeq 常用的一种是旋转定位嵌入，称为 Rope。如果你想用正常运动的绳子， 这是一种连续的事情。你取两个注意力矩阵，通过复数值旋转来旋转它们，这是一个矩阵乘法。有了 DeepSeek 的 MLA，有了这个新的注意力架构，他们需要做一些聪明的事情，因为他们的设置不一样，这只会使实现的复杂性更高。所以他们正在管理所有这些事情，这些可能是 OpenAI，这些封闭的实验室正在做的事情。我们不知道他们是否在做完全相同的技术，但他们实际上与世界分享了这些技术，这真的很好。这是高效语言模型训练的前沿。
**Lex Frdiman：** 其中一些需要低水平的工程，只是在欺骗中是一个巨大的混乱。所以据我所知，它们低于 CUDA，所以它们非常低。
**Dylan Patel：**GPU 的有效编程，NVIDIA 构建了这个名为 Nickel 的库，对吗？你知道，当你训练一个模型时，你在模型的每一层之间都有所有这些通信，你可能有一百多层。
**Nathan Lambert：** 镍币代表什么？
**Dylan Patel：** 这是 NCCL 的 NVIDIA 通信集体图书馆。
**Lex Frdiman：** 很好。
**Dylan Patel：** 所以，当你训练一个模型时，你会在每一层之间，在多层感知器或前馈网络和注意力机制之间，让所有这些都减少和聚集，你会让模型基本上同步，对吧？或者你会有 AllReducer 和 All Gather。这是网络中所有 GPU 之间的通信，无论是训练还是推理。所以英伟达有一个标准库。这就是为什么很难使用其他人的硬件进行训练的原因之一，因为没有人真正建立了一个标准的通信库。英伟达已经在更高的层次上做到了这一点，对，DeepSeek，因为他们对 GPU 有一定的限制，他们可以访问互连，在某种程度上受到合法运入中国的 GPU 的限制，不是那些走私的，而是合法运入的，他们用来训练这个模型。他们必须弄清楚如何提高效率。其中一件事是，他们安排了自己的通信，而不是仅仅把 NVIDIA 库称为 Nickel，一些实验室也是这样做的。伊梅达在《Llama 3》中谈到了他们如何制作自己的定制版镍。这是。他们没有，他们没有谈论实施细节。这是他们所做的一些事情。可能不如深度探索，因为深度探索，你知道，需求是创新之母，他们必须这样做。而在这种情况下，你知道，OpenAI 有人做这类事情，人类，等等。但是，你知道，DeepSeek 确实公开做了这件事，而且他们可能做得更好，因为他们在芯片的某个方面受到了限制，他们可以访问。所以他们通过安排特定的短信来安排通信。你可以把 SMS 想象成 GPU 上的核心，对吧？ 因此，GPU 上有数百个核心或 100 多个核心 SMS。他们专门安排，嘿，哪些在运行模型，哪些在做所有的减少，哪些在做所有的收集，对吧？它们会在它们之间来回翻转。并且这需要极低级别的编程。
**Nathan Lambert：** 这是镍自动做的。或其他 NVIDIA 库通常会自动处理此问题。
**Dylan Patel：** 是的，没错。所以从技术上讲，他们使用的是 PTX，这有点像你可以把它想象成一种汇编语言。不完全是这样。或者指令集，对吧？比如直接对汇编指令集进行编码。不完全是这样，但这在技术上仍然是 CUDA 的一部分。但这就像，我想用 Python 编写，你知道，相当于 PyTorch，并调用 NVIDIA 库？我想降到 C 级，对吗？或者你知道，编码更低的级别？或者我想一直深入到组件或 ISO 级别？而且，在有些情况下，你会去非常大的实验室，但大多数公司都不会这样做，对吧？因为这是在浪费时间，而且你获得的效率收益也不值得。但是 DeepSeek 的实现太复杂了，对吧？尤其是他们的专家组合。对的？人们已经做了专家的混合，但他们通常是 8，16 个专家。对的。它们也被激活了。所以你知道，我们喜欢用的一个词是稀疏因子，对吧？或用法。对的？所以，你可能有四个，你知道，四分之一的模型被激活，对吗？这就是米斯特拉尔的混合模型，对。他们的，他们的模型真的让他们喜欢，哦，我的上帝，他们真的，真的很好。OpenAI 也有 MOE 的模型，所有其他主要关闭的实验室也是如此。但是 DeepSeek 做了什么，也许只有领先的实验室最近才开始做的是有这么高的稀疏性因素，对吧？它不是模型的 1/4，对吗？八个专家中有两个在你每次浏览模型时都会激活，这是 256 个专家中的八个。
**Nathan Lambert：** 专家的混合有不同的实现方式，你可以让其中一些专家总是被激活，这看起来就像一个小的神经网络，然后所有的令牌都通过它，然后它们也通过这个路由机制选择的一些。DeepSeek 架构的创新之一是他们改变了路由机制。在专家模型的混合中，有一种叫做辅助损失的东西，这实际上意味着在训练过程中，你要确保所有这些专家都被用在任务中，模型看到了为什么会有失败。专家的混合是，当你做这个训练时，一个目标是标记预测的准确性。如果你只是让你自己的混合专家模型去训练，它可能是模型学习只使用专家的子集。在教育部的文献中，有一种叫做辅助损失的东西，它有助于平衡它们。但如果你想想深度学习的损失函数，这甚至与痛苦的教训有关，那就是你希望在你的模型中有最小的归纳偏差，让模型最大限度地学习。这种辅助损失，这种专家之间的平衡可以被看作是与代币的预测准确性之间的紧张关系。所以我们不知道 DeepSeek 变化的确切程度，这不是辅助损失，而是在他们的路由中有一个额外的参数，在批次之后，他们更新这个参数，以确保下一批都有类似的专家使用。这种变化可能很大，也可能很小，但随着时间的推移，它们会累积起来。这是一种指向他们创新的东西。我敢肯定，所有训练大型 MOE 的实验室都在研究这类事情。这是从辅助损失中摆脱出来的。他们中的一些人可能已经使用了它，但你一直在积累收益。 我们将讨论训练的理念以及如何组织这些组织。随着时间的推移，你的数据、你的架构和你的岗位训练以及它们如何相互集成，其中很多都是小的改进。DeepSeq 也做同样的事情。其中一些是共享的。我们必须接受他们的表面价值，他们分享他们最重要的细节。我的意思是建筑和重量都在那里。所以我们看到他们在做什么，这就增加了。
**Dylan Patel：** 回到效率和复杂性的问题上，对吗？32 对 4，对如混合绘制和其他已公开发布的 MOE 模型。所以这个比例是非常高的。Nathan 的意思是，当你有如此不同的稀疏度水平时，你不能让每个 GPU 都拥有整个模型，对吧？模型太大了，太复杂了。所以你必须用不同类型的并行性来分割模型。所以你可能在不同的 GPU 节点上有不同的专家。但现在当你得到的这组数据，嘿，所有的数据看起来都是这样的，所有的数据都应该路由到我的模型的一部分。因此，当所有这些都路由到模型的一部分时，您可以重载某一组 GPU 资源或某一组 GPU，然后训练网络的其余部分处于空闲状态，因为所有令牌都路由到该部分。所以这是最大的复杂性，运行一个非常稀疏的专家混合模型的最大复杂性之一，即这个 32 的比例与这个 4 的比例，你最终会有这么多的专家只是坐在那里无所事事。那么，如何在它们之间进行负载平衡呢？如何安排它们之间的通信？这是很多极低水平的详细工作，他们首先在公众中发现，可能是世界第二或第三，在某些情况下甚至可能是第一。
**Lex Frdiman：** 你从这一切中学到了什么更好的教训？这将是很大收益的方向在哪里？这是一种低层次的优化？或者这是一个短期的事情，最大的收益将更多地在算法的高层次方面，如后训练？这是不是一个短期的飞跃，因为他们已经像黑客一样发现了，因为约束和需要是发明之母？还是仍然有很多收获？
**Nathan Lambert：** 我认为我们应该总结一下惨痛的教训到底是什么。如果你解释一下，这是一个痛苦的教训，那就是在深度学习中胜出的训练类型是那些在学习中可扩展的方法。搜索就是它所呼吁的。这个音阶词在这方面得到了很多关注。我使用的解释是有效地避免在你的学习过程中添加人类先验。如果你读了原文，这就是它所谈论的是研究人员将如何尝试提出聪明的解决方案来解决他们的具体问题，这可能会让他们在短期内获得小的收益，同时让这些深度学习系统高效地工作，从长远来看，这些更大的问题可能更有可能扩大规模并继续推动成功。因此，我们讨论的是对专家混合模型进行相对较小的实现更改。因此，好吧，我们还需要几年的时间才能知道其中一个是否真的对痛苦的教训至关重要。但痛苦的教训是，从长远来看，简单往往会胜出。业内有很多说法，比如模特只想学习。你必须给他们一个简单的损失场景，你把计算放在模型中，他们就会学习。把障碍移开。
**Lex Frdiman：** 这就是像镍这样的东西的力量所在。标准化的代码可以被很多人用来创造一些可以扩展的简单创新。这就是为什么黑客。我想 DeepSeek 的代码库可能是一个巨大的混乱。
**Nathan Lambert：** 我肯定他们有。在测试这些新想法的地方，DeepSeek 的代码库肯定非常混乱。多头潜在注意力可能会从 Jupyter 笔记本电脑之类的东西开始，或者有人在几个 GPU 上尝试一些东西，这真的很麻烦。但是训练 DeepSeq V3 和 DeepSeek R1 的东西，那些库，如果你把它们呈现给我们，我想我们的质量非常高。
**Lex Frdiman：** 代码，高质量的可读代码。
**Dylan Patel：** 我认为有一个方面需要注意，对吗？有一种通用的能力，可以在不同类型的运行中转移。对的？你可以为一个特定的模型架构以一种尺寸制作非常非常高质量的代码，然后这是不可转移的。嘿，当我做这个架构调整时，一切又都坏了。对的？就像这样，这可能是，你知道，他们的，他们的特定的低级编码，比如调度，SMS 是特定于这个模型架构和大小的，对吧？而像英伟达的集体的图书馆更像是，嘿，它会为任何工作，对不对？你想做一个全减，太好了。我不关心你的模型架构是什么，它会起作用的。在很多情况下，当你这样做的时候，你会放弃很多性能。但是，考虑到他们在计算方面的限制，对他们来说，为特定的运行进行特定的优化是值得的。
**Lex Frdiman：** 我想知道这些前沿模型的压力有多大。开始训练，让代码按下按钮，你现在花了大量的金钱和时间来训练这个。在调试阶段必须有很多创新，以确保没有问题，你正在监控和可视化训练的各个方面，所有这类东西。
**Dylan Patel：** 当人们在训练时，他们有各种各样的仪表板，但最简单的是你的损失，对吗？而且还在继续下降。但在现实中，特别是对于更复杂的东西，比如 MOE，最大的问题是它，或者 FP8 训练，这是另一项创新，采用精度较低的数字格式，准确性较低，最终会出现损失峰值，对吧？没有人知道为什么会发生丢失的尖峰。很长一段时间。
**Nathan Lambert：** 有些是你做的，有些是你做的，有些是坏数据。我能举一个艾图爆炸的例子吗？我们早期的模型是一个名为 “微波帮” 的子 Reddit。我们喜欢大声喊出来。这是一个真实的东西，你可以拉起微波团伙。从本质上讲，它是一个子 Reddit，每个人都发布只是字母 M 的帖子，所以它就像 M，所以有非常长的字母 M 的序列，然后评论就像哔哔，因为那是微波结束的时候。但是如果你把它传递给一个被训练成正常生成文本的模型，这是非常高的损失，因为通常你看到一个 M，你不会在很长一段时间内预测 M。所以这件事给我们带来了很多麻烦。但当你有很多这样的东西时，这是旧的，这不是最近的。当你拥有更成熟的数据系统时，这并不是导致损失激增的原因。迪伦所说的是真的，但这就像，这是，这是水平。
**Dylan Patel：** 这种想法是关于压力的，对吗？这些人就像，你知道，你会出去吃饭，就像在这些实验室工作的朋友一样，他们只是。他们就像每 10 分钟看一次手机一样。他们不喜欢，你知道，如果他们发短信是一回事，但他们只是喜欢，就像，是丢失的代币。
**Nathan Lambert：** 每秒损失，而不是爆炸。他们只是在散步，看着我们和。
**Lex Frdiman：** 心率上升。
**Dylan Patel：** 如果有一个尖峰，并且有一定程度的尖峰是正常的，对吗？它会。它会恢复并回来的。有时候，很多旧的策略是，你只是停止运行，从旧版本重新启动，然后改变数据组合，然后继续运行。
**Nathan Lambert：** 甚至有不同类型的尖峰。Dirk Groeneveld 在人工智能方面也有一个理论，就像快速尖峰和慢速尖峰，有时你会看到损失，还有其他参数，你可以看到它开始蔓延，然后爆炸，这真的很难恢复。所以你必须追溯到更远的地方。所以你有一段紧张的时期，它就像是平坦的，或者它可能开始上升，你会想，我该怎么办？然而，也有看起来不错的损失峰值，然后有一个尖锐的数据点，你能做的就是跳过这些。你看到有一个峰值，你会想，好吧，我可以忽略这个数据，不更新模型，然后做下一个，它会很快恢复。但这些复杂的实现，所以当你的架构变得更复杂，你扩展到更多的 GPU 时，你的损失就更有可能扩大。所以这就像是一种分布。
**Dylan Patel：**Grokking 的整个概念也是如此，对吗？这就像，仅仅因为它在损失中放慢了改善的速度并不意味着它没有学习。因为突然间它可能会像这样，它可能会再次陷入亏损。因为它确实学到了一些东西，对吧？它花了一些时间才知道这不是一个渐进的过程，对吧？这就是人类的样子，这就是模特的样子。所以，正如你所提到的，这确实是一项压力很大的任务。
**Lex Frdiman：** 在整个过程中，美元的数量一直在上升。
**Nathan Lambert：** 每家公司都有失败的例子。你需要失败的运行来推动你的基础设施的发展。所以很多新闻周期都是由 X 公司组成的，如果 Y 公司失败了。每一家试图推动人工智能前沿的公司都有这些。所以是。是的，这是值得注意的，因为这是一大笔钱，而且可能是一周到一个月的挫折，但这是过程的一部分。
**Lex Frdiman：** 但是你怎么去，如果你在深度探索，你怎么去一个神圣的地方。有一个成功的超参数组合。
**Nathan Lambert：** 很多小的失败运行和。
**Lex Frdiman：** 如此，如此快速的迭代通过失败的运行，直到。
**Dylan Patel：** 还有成功的。
**Lex Frdiman：** 你只是，然后你建立了一些像这样的直觉，这种专家工作的混合，然后这种实现。
**Nathan Lambert：**MLA 是有效的，关键的超参数，比如学习率和正则化等等。并且您可以找到适合您的代码库的机制。与前沿实验室的人交谈，有一个故事，你可以告诉训练语言模型是你需要遵循的一条道路。所以你需要解锁训练某一类模型或者某一种规模的能力。然后你的代码库和你的内部知识，哪些超参数为它工作是已知的。你看看 DeepSeek 的论文和模型，他们已经扩大了规模，他们增加了复杂性，它只是在继续构建他们所拥有的能力。
**Dylan Patel：** 这就是 YOLO 跑步的概念。所以，你只能活一次。它是什么，就像，你知道，你在小规模上做的所有这些实验，对吗？研究消融，对吧？就像你有你的 Jupyter 笔记本电脑，你在三个 GPU 或其他东西上试验 MLA，你在做所有这些不同的事情。比如，我要做四个专家，四个活跃的专家，128 个专家。我要这样安排专家吗？所有这些不同的模型架构的东西，你正在测试一个非常小的规模，对不对？几个研究人员，几个 GPU，几十个 GPU，几百个 GPU，不管它是什么。然后突然间你会说，好了，伙计们，别再胡闹了，对吧？别再胡闹了。每个人都拿出我们所有的资源，让我们选择我们认为可行的东西，然后去做，对吗？这就是那种压力的来源，就像，嗯，我知道它在这里起作用，但有些东西在这里起作用，有些东西在这里起作用，在这里不起作用，对吗？就规模而言，对吧？所以这是，这是，这真的是一个 YOLO 运行，有点像这样，就像某些研究人员的讨论，就像这种有条不紊的性质。比如他们可以找到整个搜索空间，找出不同研究的所有消融，真正看到什么是最好的。有一些研究人员就像，你知道，有一种天生的直觉，这就是 YOLO 跑步。就像，你知道，我在看数据。这就是它。
**Nathan Lambert：** 这就是为什么你想在训练后工作，因为训练的 GPU 成本较低，所以你可以进行更高比例的训练。约洛跑步。
**Lex Frdiman：** 是的，为了。
**Dylan Patel：** 目前。
**Lex Frdiman：** 是的，现在，现在。所以从根本上说，这仍然是运气。
**Dylan Patel：** 运气就是技巧，对吗？在很多情况下，是的。
**Lex Frdiman：** 我的意思是它看起来很幸运，对吗？
**Nathan Lambert：** 当你在爬山的时候，如果你在其中一个实验室里，你有一个评估，你不会崩溃，有一个关于你如何改进事情的重复的剧本。有一些局部的改进，可能是数据的改进，这些加起来使整个模型变得更好。当你把镜头拉近时，很明显这个模型在这方面做得很差，我们可以修复它。你只要把这些加起来。所以有些感觉像是运气。但在现实中，特别是在我们谈论的这些新的推理模型中，我们可以通过很多方法来探索，通常其中一些方法会带来很大的改进。
**Dylan Patel：** 搜索空间几乎是无限的，对吗？然而，你所拥有的计算和时间却非常少。你必须按计划发布。你不能被每个人吹过去。否则，DeepSeek 会发生什么，粉碎梅塔，米斯特拉尔和科赫尔以及所有这些家伙，他们移动得太慢了，对吗？他们可能太有条理了，我不知道。他们没有击中 YOLO 运行。不管是什么原因，也许他们没有那么熟练，不管是什么。你知道，如果你愿意，你可以称之为运气，但在一天结束的时候，这是技巧。
**Lex Frdiman：** 所以 2025 年是 YOLO 跑步的一年。好像所有的实验室都要进去。
**Dylan Patel：** 我认为 OpenAI 在 2022 年所做的更令人印象深刻，对吗？当时，没有人相信专家模型的混合，对吗？在谷歌，他们有所有的研究人员，OpenAI 只有很少的计算，他们把所有的计算都投入了好几个月，对吧？所有这些，在几个月的时间里 100% 地使用全新架构的 GPT4，没有人相信，嘿，让我花几亿美元，这是我在这个模型上的所有钱，对吗？这才是真正的 YOLO。现在人们喜欢，媒体上的所有这些训练失败，就像，好吧，很好，但实际上我的 GPS 中有很大一部分在做推断。我还有一群人在不停地做研究。是的，我最大的集群是训练，但就像这次 YOLO 跑步一样。但是，YOLO 运行的风险比 OpenAI 在 2022 年所做的或者 Deep Seat 现在所做的要小得多，或者你知道，就像，就像，嘿，我们要把所有的东西都扔进去。
**Lex Frdiman：** ** 人类历史上最大的赢家是那些愿意在某个时候做 YOLO 的人 **。好的，我们对它所训练的硬件有什么了解？DeepSeek。

## **04.** DeepSeek 计算集群

**Dylan Patel：**DeepSeek 非常有趣。这是第二张，缩小他们是谁的票。首先，High Flyer 是一家对冲基金，历史上在中国和其他地方都进行过量化交易。他们总是有相当数量的 GPU，对吧。过去，很多高频交易算法量化交易员使用 FPGA，但后来转向了 GPU。当然，两者都有，对，但特别是 GPU，还有 Deep 和 High Flyer，这是拥有 DeepSeek 的对冲基金。在某种程度上，为 DeepSeek 工作的每个人都是 High Flyer 的一部分。对的？呃，同样的，同样的母公司，同样的老板，同样的首席执行官。他们拥有所有这些用于交易的资源和基础设施，然后他们将其中巨大的一部分用于训练模型，包括语言模型和其他模型。对的？因为这些，这些，这些技术受到了人工智能的严重影响。嗯，你知道，最近人们，你知道，意识到，嘿，交易，你知道，就像，甚至，甚至当你回到文艺复兴和所有这些，所有这些，就像，量化公司。自然语言处理是快速交易的关键，对吧？理解新闻稿并做出正确的交易。对的？DeepSeek 在这方面一直很擅长。甚至早在 2021 年，他们就有新闻稿和文件说，嘿，我们是中国第一家拥有这么大的 A100 集群的公司。它是 10，000 个 A100 GPU。对的？这是，这是在 2021 年。现在这不是所有的训练，你知道，大型语言模型。这主要是为了训练他们的定量方面的模型，他们的定量交易以及，你知道，其中很多是自然语言处理，需要明确的是。对的？所以这就是历史，对吧？所以可以证实的事实是，他们在 2021 年建立了中国最大的集群，至少他们声称这是中国最大的集群。10000 个 GPU 在出口管制开始前。
**Nathan Lambert：** 是的，就像他们以前有过一个巨大的集群。任何关于出口管制的谈话。
**Dylan Patel：** 然后你向前一步，从那时起，他们在过去的四年里做了什么，对吗？显然，他们继续运作对冲基金，可能赚了很多钱。另一件事是他们越来越倾向于人工智能。首席执行官莱昂 · 成峰。
**Nathan Lambert：** 里昂，你没有把我的位置放在这上面。我们讨论过这个。
**Dylan Patel：**Leon Feng，对，可能是 Lian Fang 的首席执行官，据说他拥有公司一半以上的股份，对。是一个非常像埃隆 · 詹森的人物，他参与了所有的事情，对吗？所以在那段时间里，他对人工智能有了真正深入的了解。他实际上有点.。就像一个。如果你看到他的一些陈述，几乎有点 E ACC 的感觉，对吧？
**Dylan Patel：** 完全的 AGI 共鸣。比如，我们需要做这个，我们需要做一个开放 AI 的新生态。我们需要中国引领这种生态系统，因为从历史上看，西方国家在软件生态系统上一直处于领先地位。他直接承认，为了做到这一点，我们需要做一些不同的事情。深度探索是他做这件事的方式。对他的一些翻译采访是。
**Lex Frdiman：** 所以他接受过采访？
**Nathan Lambert：** 是的。
**Lex Frdiman：** 你认为他会接受西方的采访或者。不。或者有没有控制。
**Nathan Lambert：** 还没有，但好吧，我会试试。
**Lex Frdiman：** 我刚刚得到了一个中文翻译，所以这很棒。这是，这都是推。如此迷人的数字工程师充分利用人工智能的成功。
**Nathan Lambert：** 高频，交易非常直接的报价，就像我们不会切换到闭源。当被问及这件事时，长期的动机是人工智能的生态系统应该如何运作。我认为从中国的角度来看，他希望一家中国公司来构建这一愿景。
**Dylan Patel：** 所以这有点像公司背后的所谓 “有远见的人”，对吗？这个对冲基金还存在吧？这个，这个定量公司。所以深度探索是那种，你知道，慢慢地他转向了这个全景，就像人工智能一样，关于这个的一切，对吧？但在某个时候，它慢慢地移动了，他进行了 DeepSeek。从那时起，DeepSeek 已经完成了多个模型。他们获得了越来越多的 GPU。他们与基金共享基础设施，对吗？所以，你知道，他们没有确切的公共 GPU 资源数量，但除此之外，他们在 2021 年购买了 10，000 个 GPU，对吗？他们的利润非常丰厚，对吧？然后这篇文章声称他们只做了 2000 个 H800 GPU，这是中国以前允许的受限制的 GPU。但不再允许。有一个新的版本，但它基本上是英伟达的 H100 为中国。对的。它有一些限制，特别是在通信速度和互连速度方面。对的。这就是为什么他们不得不做这个疯狂的 SM 调度的东西。对的。所以回到这一点，对，就他们的 GPU 总数而言，这显然是不正确的，很明显。
**Lex Frdiman：** 可用的 GPU，但对于这次训练运行，您认为 2000 是正确的数字或没有。
**Dylan Patel：** 所以这就是它需要的地方，你知道，就像分区一样。对的。比如你怎么称呼你的跑步训练？对的。你计算过你做的所有研究和消融吗？对，挑选所有这些东西？因为是的，你可以做 YOLO 运行，但在某种程度上，你必须在小规模上做测试，然后在大规模之前，你必须在中等规模上做一些测试。
**Nathan Lambert：** 公认的做法是，对于任何给定的模型，这都是一个显著的进步。仅在实验中，您将对完整的训练运行进行 2-4 倍的计算。
**Lex Frdiman：** 因此，许多正在扩大规模的计算机可能在很大程度上用于研究。
**Dylan Patel：** 是的。你知道，研究会产生新的想法，让你获得巨大的效率。
**Nathan Lambert：** 研究让你，哦，就像研究让你取得突破一样，你需要在这上面下注。
**Lex Frdiman：** 因此，我们将要讨论的一些定价策略已经将研究融入到价格中。
**Dylan Patel：**DeepSeek 特别公开的数字。对的。2021 年只有 10，000 个 GPU，然后 2000 个 GPU 只用于 V3 的预训练。他们没有讨论 R1 的成本，也没有讨论所有其他 RL 的成本。对的。为他们制作的指示模型。对的。他们只讨论了基础模型的预训练，他们没有讨论任何关于研究和消融的内容，他们也没有讨论任何共享的资源，嘿，基金正在使用所有这些 GPU，对。我们知道他们非常有利可图，到 2021 年将有 10，000 个 GPU。我们发现的一些研究表明，我们实际上相信他们拥有接近 50，000 个 GPU。
**Lex Frdiman：** 我们是塞米纳人。所以我们应该说你是世界上的专家之一，在半导体方面，在集群构建方面，在谁在做什么方面，弄清楚每个人都在做什么？在训练跑步方面。所以，是的，这就是。我们. 好的，去吧。
**Dylan Patel：** 对不起，对不起。我们相信他们现在实际上拥有接近 50，000 个 GPU。这是在许多任务中分开的。对的。再次，基金研究和消融为棒球场。
**Nathan Lambert：**OpenAI 或 Anthropic 有多少钱？我认为我们有最清楚的例子，因为 Meta 也是开放的，他们谈论在他们的训练集群中有 60K 到 100K 的 H100 等效 GPU。
**Dylan Patel：** 对。所以，就像 Llama 3 号一样，他们说他们在 16000 H100 上训练。对的。但是 Meta 公司去年公开披露他们购买了大约 40 万个 GPU。是的，没错。那么。所以当然，训练中的一小部分，就像大多数人一样，就像为我提供最好的 Instagram 卷轴一样。对的。或者别的什么。对的。
**Nathan Lambert：** 我的意思是，我们可以讨论成本，比如，2000 GPU 集群的拥有成本是多少？10,000. 就像有不同规模的公司可以负担得起这些东西。DeepSeek 相当大。他们的计算分配比较是世界上最好的几个之一。它不是 OpenAI，人类等等，但他们有很多计算。

## **05.** 对中国的 GPU 出口控制

**Lex Frdiman：** 你能不能把镜头拉远，谈谈 Hopper 架构、NVIDIA Hopper GPU 架构以及 H100 和 H800 之间的区别？就像你提到的互连。
**Dylan Patel：** 是的。所以，你知道，安培是 A100，然后是 H100 漏斗。对的。在美国，人们把它们当作同义词来使用，因为实际上只有 H100，而现在有 H200。对的。但同样的事情，主要是在中国，他们有两个。有不同的出口限制措施。所以最初美国政府限制了两个因素的规模。对的。即芯片互连与触发器。对的。因此，任何具有高于某一级别的互连和高于某一级别的浮点运算的芯片都受到限制。后来，政府意识到这是限制中的一个缺陷，他们将其削减为只是浮点运算。
**Nathan Lambert：** 所以 H800 有高 FLOPS，低通信。
**Dylan Patel：** 完全正确。因此，H800 在 FLOPS 上的性能与 H100 相同。对的。但它没有。它只是削减了互连带宽。Deepseek 知道如何利用这一点。你知道，嘿，即使我们削减了互连，我们也可以做所有这些花哨的东西来弄清楚如何充分使用 GPU。对的。那是在 2022 年 10 月。但后来在 2023 年到 2023 年，在 2024 年实施，美国政府禁止了 H800。对的。顺便说一下，这个 H800 集群，这 2000 个 GPU，甚至在 2024 年都没有购买。对。它是在 2023 年底购买的。他们现在刚刚把模型拿出来。对的。因为这需要大量的研究，等等。H800 被禁止了，现在有一种新的芯片叫做 H20。H20 被削减，只有 FLOPS，但互连带宽是相同的。事实上，在某些方面，它比 H100 更好，因为它有更好的内存带宽和内存容量。所以，你知道，英伟达是在政府设定的限制范围内工作，然后为中国打造最好的 GPU。
**Lex Frdiman：** 我们能不能从这个实际的切题出发，回到硬件是出口管制的哲学、动机和理由？那是什么？Daryama Day 刚刚发表了一篇关于出口管制的博客文章。他提出的理由是，如果人工智能变得超级强大，他说，到 2026 年，我们将拥有 AGI 或超级强大的人工智能，这将产生重大影响，无论谁建造它，都将拥有重大的军事优势。因此，因为美国是一个民主国家，正如他所说，中国是威权国家，或者有威权因素，你想要一个统一的极地世界，在那里，由于人工智能，超级强大的军队是一个民主国家。这是一个更加复杂的世界地缘政治，当你有两个超级强大的人工智能和一个是独裁的超级大国。这就是他提出的理由。所以我们想。美国希望利用出口管制来放慢速度，以确保中国不能进行这些巨大的训练，而这些训练可能需要建造 AGI。
**Dylan Patel：** 这很抽象。我认为这可能是一些人所描述的出口管制的目标，这是一个超级强大的人工智能，你谈到了训练运行的想法。中国不能训练人工智能模型的世界并不多。我认为出口管制正在限制中国所能拥有的计算机数量或密度。如果你想想现在的人工智能生态系统，随着所有这些人工智能公司的收入数字都在上升，他们的人工智能使用正在继续增长。更多的 GPU 将推断出口管制的很大一部分，如果它们起作用的话，在中国可以运行的人工智能的数量将会低得多。所以在训练方面，DeepSeek V3 是一个很好的例子，你有一个非常专注的团队，仍然可以到达人工智能的前沿。在这一点上，2000 个 GPU 并不难获得世界上所有的考虑。他们仍然会有那些 GPU，他们仍然能够训练模型。但是，如果人工智能将会有一个巨大的市场，如果你有强大的出口控制，并且你想要拥有 100，000 个 GPU，只是服务于具有良好出口控制的 ChatGPT 集群，这也使得人工智能可以被使用得更少。我认为这是一个更容易实现的目标，而不是试图讨论什么是 AGI。如果你有这些非常智能的自主人工智能和数据中心，这些东西可以在美国的 GPU 集群中运行，但不能在中国运行。
**Dylan Patel：** 在某种程度上，训练一个模型没有任何效果，对吗？比如有个模特。达里奥所说的是这种模式的实施，一旦经过训练，就会创造巨大的经济增长，军事能力的巨大增长，巨大的能力，人民生产力的提高，生活的改善，无论你想把超级强大的人工智能引向什么，你都可以做到。但这需要大量计算，对吧？所以美国政府已经有效地说，永远，对，就像训练将永远是总计算的一部分。你知道，我们提到了 Meta，400，000 个 GPU，只有 16，000 个 Llama ，对。所以 Meta 致力于推理的百分比。现在，这可能是因为推荐系统试图让我们的大脑花更多的时间观看更多的广告。或者如果它是，如果它是，或者如果它是一个超级强大的人工智能，它正在做富有成效的事情，这与我们的经济系统决定的确切用途无关。它可以以我们想要的任何方式提供任何东西。而在中国，对，你知道，你是，你知道，专家限制，很好，你永远不能切断一切，对吗？这就像，我认为美国政府很好地理解了这一点，即你不能切断一切。
**Nathan Lambert：** 你知道，他们会自己做薯片。
**Dylan Patel：** 如果他们试图制造自己的芯片，他们会比我们的更糟糕。但你知道，关键是要保持距离，对吧？因此，在某种程度上，作为人工智能，你知道，在一个经济增长 2.3% 的世界里。顺便说一句，这真的很蠢。对的？切断，你知道，高科技，不从中赚钱。但在一个超级强大的人工智能出现，然后开始在社会中创造重大变化的世界里，这是所有人工智能领导者和大型科技公司所相信的，我认为超级强大的人工智能将会极大地改变社会。因此，计算差异的复合效应非常重要。在一些科幻小说中，人工智能的衡量标准是有多少能量被传递给计算，或者有多少能量被传递给计算。这是一种思考经济产出是什么的方式，你有多大的力量指向人工智能。
**Dylan Patel：** 我们是否应该讨论推理模型，作为一种方法，这可能是可操作的，因为人们可以实际看到？所以 R1 和 O1 的推理模型，它们被设计成使用更多的计算。在人工智能社区里有很多关于测试时间、计算、推理时间、计算机等等的热门词汇。但迪伦对此有很好的研究。你可以得到关于具体数字的比例。当你训练一个模型时，你可以看看训练时使用的计算量和推理时使用的计算量。这些推理模型使得推理在完成复杂任务时变得更加重要。去年秋天，也就是 12 月，OpenAI 发布了这款 O3 模型。在人工智能中还有另一件事，当事情进展得很快时，我们会得到公告和发布。公告本质上是博客文章，你拍拍自己的背，说你做了一些事情，然后发布在模型上，在报纸上，等等。所以 OpenAI 已经宣布了 O3，我们可以检查 o3-mini 是否有可能退出录制。但这并没有真正改变这一点，那就是突破性的结果是所谓的 ARC AGI 任务，这是抽象推理语料库，一个通用人工智能的任务。弗朗索瓦 · 夏勒特就是那个。这是一篇多年的老论文，它是一个辉煌的基准。OpenAI03 解决这个问题的方法是，它在 API 中使用了一定数量的样本。API 在样本数量上有思考的努力。他们用了 1000 个样本来解决这个问题，结果是每个问题 5 到 20 美元，你把它放在一个有效的数学难题中，然后回答一个问题需要几美元。这需要大量计算机。如果这要在美国起飞，OpenAI 需要大量的 GPU 进行推理来捕捉这一点。他们有这个 OpenAI ChatGPT Pro 订阅，每月 200 美元，山姆。
**Dylan Patel：** 说他们正在亏损，这意味着。
**Nathan Lambert：** 人们在基础设施上消耗了大量的 GPU。我已经注册了它，我玩过它，我不认为我是一个超级用户，但我使用它。这就像是一家拥有中等强度出口管制的中国公司，总是会有漏洞，可能根本无法做到这一点。如果 O3 的主要结果也是惊人的编码性能，如果这反馈到人工智能公司能够更好地进行实验。
**Lex Frdiman：** 所以大概的想法是，对于 AGI 来说，更大一部分的计算将用于这个测试时间，计算 AGI 的推理进入一个房间，思考如何接管世界，并在 2.7 小时内返回，这将需要大量的计算。
**Nathan Lambert：** 这就是人们，OpenAI 和 Anthropic 的领导者所谈论的自主人工智能模型，即你给他们一个任务，他们在后台工作。我认为我个人对 AGI 的定义要简单得多。我认为语言模型是 AGI 的一种形式，所有这些超级强大的东西都是下一步，如果我们有这些工具，那就太好了，但语言模型有如此多的价值和领域。这对我来说是一种普遍的智慧。但下一步，他们是独立的，他们可以做训练数据中没有的任务，这是这些人工智能公司正在推动的未来几年的前景。
**Lex Frdiman：** 我认为达拉 · 达里奥在这里使用的术语是超级强大的人工智能。所以我同意你对 AGI 的看法。我认为我们已经有了一些非常令人印象深刻的东西，艾伦 · 图灵肯定会说是 AGI，但他更多地是指一旦拥有了某种东西，你就会比其他国家拥有显著的军事和地缘政治优势。所以这不仅仅是你可以问它如何做煎蛋卷。
**Nathan Lambert：** 在他的散文《爱与恩典的机器》中，他的观点要积极得多。读进这个。我没有足够的物理科学背景来准确衡量我的能力。如果人工智能可以彻底改变生物学，我可以肯定地说，人工智能将加速任何计算科学的进步。

## **06.** AGI 时间线

**Lex Frdiman：** 所以我们在这里做了一个深度优先搜索，主题是取切线的切线。所以让我们继续深度优先搜索。你说你们都感觉到了 AGI，所以你，你的时间线是什么？达里奥的 2026 年，对于超级强大的人工智能来说，你知道，这基本上是一个真正的安全威胁。AGI 的水平，你的，你的时间线是什么？
**Dylan Patel：** 我不喜欢归因于具体的能力，因为预测具体的能力和时间是非常困难的。我认为，如果你要说我对 AGI 的感觉是，我预计未来几年将继续取得快速、令人惊讶的进展。因此，像 R1 这样的东西对我来说并不那么令人惊讶，因为我预计会有新的范例，可以取得实质性的进展。我认为 DeepSeek R1 是如此令人不安，因为我们在 ChatGPT 的这条道路上。感觉越来越好了，越来越好了，越来越好了。然后我们有一个改变模型的新方向。我们像这样走了一步，又往上走了一步，所以看起来像是一个非常快的斜坡。然后我们要采取更多的步骤。所以当你迈出这些大的一步时，你会感到非常不安。我希望这种情况继续发生。我试过 OpenAI 操作员，我试过克劳德电脑使用。他们还没到。我理解这个想法，但很难预测什么突破能让这样的东西成功。我认为更有可能的是，我们有了有效的突破，以及我们不知道他们会做什么的事情。所以每个人都想要经纪人。达里奥用非常雄辩的方式描述了这一点。我只是在想，好像会有比这更多的东西。所以期待这些事情的到来吧。
**Lex Frdiman：** 我将不得不试着把你固定在 AGI 时间线上的一个日期，比如核武器时刻。所以在地缘政治舞台上有一个真正的时刻，你知道，因为我们在谈论出口管制。你认为什么时候才能放弃约会，你认为那会是什么时候？喜欢？对我来说，可能是在 2030 年之后。所以我不是，正如我所说的，所以定义它。
**Dylan Patel：** 对。因为对我来说，这几乎已经发生了。
**Nathan Lambert：** 对。
**Dylan Patel：** 你看看印度和巴基斯坦的选举，人们接到人工智能语音电话，认为他们在与政治家交谈。对的。《人工智能扩散规则》（AI Diffusion Rules）是在拜登政府执政的最后几周颁布的，看起来特朗普政府将保持甚至可能加强对云计算和 GPU 销售的限制，这些国家甚至与中国无关。这就像，这就是葡萄牙。
**Nathan Lambert：** 这些国家和普通国家一样都在名单上，你需要得到美国的批准。
**Dylan Patel：** 就像，是的，葡萄牙，就像，你知道，就像，就像所有这些盟国，对吗？新加坡，对吧？就像他们一样，他们有 F35，我们不让他们买 GPU。喜欢，这是，这对我来说已经是喜欢的规模，你知道。
**Lex Frdiman：** 嗯，这只是意味着美国军方对这项新技术感到非常紧张。这并不意味着技术已经存在。所以，他们可能只是对他们不太理解的事情非常谨慎。但这是一个非常好的观点。在某种程度上，机器人电话，成群的半智能机器人可能是一种武器，可以做很多社会工程。
**Dylan Patel：** 我的意思是，有很多关于 2016 年选举的讨论，比如剑桥分析和所有这些东西，俄罗斯的影响。我的意思是，世界上每个国家都在把东西推到互联网上，都有他们想要的故事，对吧？就像每一个。每个人都有技术能力，无论是俄罗斯，中国，美国，以色列，等等。对的。你知道，人们正在把观点推到互联网上，语言模型摧毁了听起来非常智能的语言的成本。
**Nathan Lambert：** 有一些研究表明，分布实际上是限制因素。所以语言模型还没有制造错误信息，特别是改变那里的等式。互联网仍在进行中。我想有一个博客，AI Snake Oil，还有我在普林斯顿的一些朋友写了这些东西。所以有研究。这是每个人都认为的默认情况，我认为同样的事情是，错误信息不会随着语言模型而变得更糟。我认为，就互联网上的帖子和人们一直在测量的东西而言，它并没有呈指数增长，也没有非常可测量的东西，你谈论的语音通话之类的东西，可能是更难测量的形式。所以现在下结论还为时过早。我认为这就像网络上的政治不稳定一样。很多研究人员都在监测它，看看发生了什么。我想你问的是 AGI 的事。如果你让我给你一年的时间，我会说，好吧，我有人工智能的首席执行官这样说。他们说两年已经有一段时间了。我认为像达里奥这样的人。首席执行官对此进行了深刻的思考。我需要认真对待他们的话，但也要理解他们的不同。所以我想，再加上几年，这就是你如何得到类似于 2030 年或 2030 年之后的东西。
**Dylan Patel：** 我认为在某种程度上，我们的能力达到了一定的程度，任何一个人都可以说，哦，好吧，如果我能在 X 时间内利用这些能力，这就是 AGI，对吗？称之为 27，28。但是实际操作这种能力的成本。
**Nathan Lambert：** 是的，这将是我的观点。
**Dylan Patel：** 太极端了，以至于没有人能真正大规模地部署它，只需点击一下，弹指一挥，就能彻底改变经济。所以我不认为这会像弹指间的瞬间，身体上的束缚。相反，它将是一个，你知道，哦，功能在这里，但我不能到处部署它，对吗？所以一个简单的例子可以追溯到 2023 年，你知道，当带有 GPT4 的 Bing 出现时，每个人都对搜索感到恐慌，对吧？困惑出来了。如果你在每个谷歌搜索中实现 GPT3 的成本，就像是，哦，好吧，这在物理上是不可能实现的，对吧？当我们向前一步，回到测试时间计算的事情上，对吗？一个问题，你知道，你问了 ChatGPT 一个问题，它花费了美分，对吗？他们最有能力的聊天模式，对吧？不过，要返回一个查询来解决一个 ARC AGI 问题，需要花费 5 到 20 美元，对吗？这是，这是一个 A。
**Nathan Lambert：** 它只是从那里开始上升。
**Dylan Patel：** 响应查询与执行任务的成本相差 1000，10000 倍。Arc AGI 的任务并不像，它在某种程度上很简单，但它也像，我们想要的任务是什么？好的，阿吉，我们今天所拥有的可以在三年后成为阿吉。它可以做更复杂的问题，但成本将以成千上万美元的 GPU 时间来衡量。没有足够强大的 GPU 基础设施来操作它，因此弹指间就能改变世界上的一切。但在那一刻，谁来约束控制并将 AGI 指向任务？所以在达里奥的帖子中，他说，嘿，中国可以比美国更有效、更快地将他们的 AGI 指向军事任务，对吗？他们在许多方面更快地将某些新技术应用到他们的军队中。对的。尤其是在无人机方面。对的。美国可能有一个长期存在的大型空中战斗机类型的东西，轰炸机，但当涉及到不对称武器，如无人机，他们已经完全超越了美国和西方。我认为，达里奥指出的担忧是，是的，很好，我们将在商业领域拥有 AGI。美国军方不可能很快实施它。中国军队可以，他们可以把所有的资源都用于在军队中实施它，从而解决军事后勤或解决针对某些人的虚假信息的其他方面，这样他们就可以颠覆一个国家的政治或类似的东西，这实际上是灾难性的，而美国只是想这样做，因为它将被更多地分配给最高的收入回报，这可能是建造更好的工厂或其他东西。
**Lex Frdiman：** 所以我所看到的一切，人们的直觉似乎在机器人技术上失败了。所以你有这种普遍的乐观主义。我在自动驾驶汽车上看到过这个。人们认为这个问题比实际容易得多。与无人机类似，我对它的理解有点少，但我刚刚看到了乌克兰战争的现实和双方对无人机的使用。看起来人类仍然远远胜过任何完全自主的系统。人工智能是一个助手，但人类驾驶 FPV 无人机，而人类控制的大部分无人机远远超过人工智能系统。所以我认为，我们很快就会拥有大批自主机器人，这一点对我来说并不明显。在军事背景下，也许我能想象的最快的时间是 2030 年，这就是为什么我说 2030 年是超级强大的人工智能。每当你有大规模的机器人做军事行动时，世界就开始看起来不一样了。所以这才是我真正担心的事情。但可能会有网络战争，网络战争类型的技术，从社会工程到实际上只是成群的机器人，在我们的代码库中找到攻击媒介，并关闭电网，诸如此类的东西。这可能是其中的一件事，比如在任何一个周末，停电了，没有人知道为什么，世界永远改变了。只要全美停电两天，就会导致谋杀和混乱。但回到出口管制，你是否认为这是在人工智能背景下控制地缘政治力量平衡的一种有用方式？

## **07.** 中国的生产能力

**Dylan Patel：** 我想回到我的观点，如果你相信我们正处于经济增长和变革的阶段，我们已经经历了 20 年。出口管制是中国赢得长期胜利的绝对保证。对的。如果你不相信人工智能会在未来 10 年或 5 年内给社会带来重大变化，那么 5 年的时间表就是更多人工智能公司甚至大型科技公司的高管们所相信的。但即使是 10 年的时间表，这也是合理的。但是一旦你到了，嘿，这些时间线低于那个时间段，那么唯一能为美国和中国创造相当大的优势或劣势的方法就是限制计算。因为天赋并不是什么真正的束缚。对的。中国可以说有更多的人才。对的。更多 STEM 毕业生，更多程序员。美国可以利用世界人民，它确实做到了这一点。你知道，在人工智能行业有很多外国人。
**Nathan Lambert：** 很多人工智能团队都是没有美国护照的人。
**Dylan Patel：** 是的，是的。我的意思是，他们中的许多人是移居美国的中国人。对的。那，那太好了。这正是我们想要的。对的。但人才是一个方面，但我不认为这对美国来说是一个可衡量的优势。真的是。只是现在是否计算，甚至在计算方面，当我们看芯片与数据中心时，对。中国拥有前所未有的能力，可以建立数量惊人的力量。发条装置。他们总是建立越来越多的权力。他们的钢铁厂的规模相当于整个美国工业的规模，他们的铝厂消耗千兆瓦和千兆瓦的电力。当我们谈论什么是最大的数据中心时。OpenAI 做了这个关于星际之门的大事，他们的声明。有，那不是，那就像一旦它完全建成，几年后它将是 2 千兆瓦。对的。权力。对的。这仍然比中国最大的工业设施要小。对的。中国，如果他们想建立世界上最大的数据中心，如果他们能获得芯片，就可以。所以这不仅仅是，这只是一个何时的问题，而不是是否的问题。对的。
**Lex Frdiman：** 所以他们的工业能力远远超过美国。
**Dylan Patel：** 完全正确。
**Lex Frdiman：** 制造东西。所以为什么，为什么。所以从长远来看，他们将在那里制造芯片。
**Dylan Patel：** 芯片更专业一些。我特别指的是数据中心。对的。芯片，晶圆厂需要大量的能量。不要误解我的意思，这不一定是门控因素。今天在美国，人们能以多快的速度建立最大的集群的关键因素是权力，对吧？无论是现在，还是发电，输电，变电站，你知道，所有这些 Transformer 和所有这些建设数据中心的东西。这些都限制了美国工业界建立越来越大的训练系统以及部署越来越多的推理计算的能力。
**Nathan Lambert：** 我认为我们需要把这一点说清楚，为什么现在是人们不考虑这个问题的时候了。因为本质上是出口管制，所以中国无法制造或获得尖端芯片。这个想法是，如果你的时间错了，中国会在他们的芯片生产上投入大量资金。如果你的时机不对，他们将会有更多的生产能力，更多的能源能力，并弄清楚如何制造芯片，并比世界其他地方有更多的能力来制造芯片。因为每个人都可以买，他们会把中国芯片卖给每个人。他们可能会资助他们。因此，如果人工智能需要很长时间才能变得与众不同，我们就已经毁掉了美国公司的财务表现。英伟达可以少卖，台积电不能卖给中国。因此，我们有更少的需求，从而继续推动生产周期。这就是时间背后的假设。
**Dylan Patel：** 时间为 10 年以下或 5 年以上，对。从长远来看，中国将因为这些限制而获胜，除非人工智能在短期内做一些事情，我相信人工智能会这样做，在中期，短期内给社会带来巨大的变化，对吗？这就是最大的解锁器。即使在今天，对，如果决定让天平起球，对，那就是决定天平定律是最重要的，对吗？就像萨蒂亚 · 纳德拉和马克 · 扎克伯格等美国高管一样，桑达尔和所有这些最大、最强大的科技公司的美国高管都决定扩大规模，他们正在建设数十亿瓦的数据中心，对吗？不管是在德克萨斯州、路易斯安那州还是威斯康星州，不管是在哪里，他们都在建造这些庞大的东西，其成本相当于他们在一个地方建立全球数据中心的全部预算，对吗？这就是他们为明年、后年等等所做的承诺。所以他们非常确信这就是方法，这就是他们正在做的。但如果中国决定这样做，他们可以比我们做得更快。但这就是限制的来源。目前尚不清楚，中国作为一个整体是否已经决定，你知道，从最高层来看，这是一个优先事项。美国算是有。对的。你知道，你看到特朗普在同一周内谈论深度探索和星际之门。对的。所以他在拜登政府中也有很多讨论，关于人工智能等的讨论。很明显他们在考虑这件事。就在上周，DeepSeek 会见了中国的二把手。对的。就像他们还没见过高层一样。对的。还没见过喜喜还没坐下来。他们刚刚发放了一万亿人民币的补贴，你知道，大约 1600 亿美元，这更接近微软、Meta 和谷歌的支出总和。对的。今年的。所以这就像他们，他们，他们现在才意识到。但这正是出口限制的作用所在， 嘿，你不能，你不能把最强大的美国芯片运到中国。你可以运送一个精简的版本。你可以，你不能把最强大的芯片运送到所有这些国家，我们知道这些国家只会把它租给中国。你必须限制数量。对的。工具和制造工具一样，所有这些，所有这些不同的方面。但这一切都源于人工智能。然后在人工智能和整个半导体限制中，下游可以减慢它们的速度，你读它们，它们是非常清楚的。它是关于人工智能与军民融合的技术。对的？很清楚。然后从那里开始，哦，我们禁止他们购买光刻工具、蚀刻工具和沉积工具，哦，你知道，这个随机的子系统来自一个随机的公司，就像 Tiny。对的？比如，我们为什么要禁止这个？因为所有这些，美国政府已经决定对人工智能系统至关重要。
**Nathan Lambert：** 我认为支点是从 7 纳米到 5 纳米芯片的过渡。我认为是华为在几年前推出了 7 纳米芯片，这引起了另一场政治风波，就像这一刻一样。然后是 ASML 深紫外线。那是什么？
**Dylan Patel：** 在芯片上设置上下文的极紫外光刻。对的。Nathan 所指的是在 2020 年，华为发布了他们的 Ascend 910 芯片，这是一款人工智能芯片，在谷歌和英伟达之前，这是第一款 7 纳米的芯片。他们把它提交给了 MLPerf 基准测试，这是机器学习性能基准测试的行业标准。它做得很好，是提交时最好的筹码。对的。这是一件大事。特朗普政府，当然被禁止了。那是 2019 的事了，对吧？禁止华为从台积电获得 7 纳米芯片。因此，他们不得不转而使用国内生产的芯片，这是一个多年的挫折。
**Nathan Lambert：** 许多公司已经做了 7 纳米芯片。问题是，我们不知道华为为芯片生产提供了多少补贴。像英特尔已经制造了 7 纳米芯片，这是无利可图的。这就是这一切如何反馈到出口管制的经济引擎中。

## **08.** 美中冷战

**Lex Frdiman：** 嗯，你是说 XXX 现在还没有感觉到 AGI，但感觉像是 DeepSeek 时刻。是的，我的，就像现在可能会有会议，他会开始穿同样的 T 恤，事情会升级。
**Dylan Patel：** 我的意思是像这样。他可能上周就醒了。对的。Leon Fang 会见了副主席，副主席，第二个指挥官，他们开了一次会，然后第二天他们宣布了人工智能补贴，这是万亿人民币。对的。
**Lex Frdiman：** 因此，这个深海时刻可能真的是冷战的开始。
**Nathan Lambert：** 这正是很多人所担心的。人工智能领域的人一直担心这会走向冷战，或者已经是这样了。
**Lex Frdiman：** 但是，这不是 DeepSeek 的错，但确实有问题。一堆因素聚集在一起爆炸。我的意思是，这一切都可能与英伟达股票下跌有关，但这只是一些集体歇斯底里的发生，最终导致举行会议，并意识到这一想法。
**Dylan Patel：** 美国政府在 2022 年 10 月 7 日意识到了这一点，在 10 月 7 日 ChatGPT 发布这一限制之前，这一限制被取消了，震惊了所有人，这显然是针对人工智能的。每个人都说，你到底在干什么？
**Nathan Lambert：** 稳定扩散在那时是不存在的。但不是 ChatGPT。
**Dylan Patel：** 是的，但不是聊天。
**Nathan Lambert：** 所以就像是开始。
**Dylan Patel：** 关于 Genai 能对社会做什么的传言。但很明显，我认为至少对国家安全委员会和那些人来说，这是世界发展的方向，这是正在发生的冷战。
**Lex Frdiman：** 那么，是否有人担心出口管制会促使中国对台湾采取军事行动？
**Dylan Patel：** 这是，这是最大的风险。对的。你越是阻止中国获得美国和全球的尖端技术，他们就越有可能说：“好吧，因为我不能获得它，我也希望没有人应该获得它。” 对的。有一些有趣的方面，对吧？就像，你知道，中国的城乡差距是其他国家所没有的。他们的男女出生比例是独一无二的。你知道，如果你看看中国的大部分地区，就会发现这个比例并不是那么糟糕。但当你看到中国农村的单身男性时，你会发现这个比例是 30 比 1。那些是被剥夺公民权的人，对吧？就像引用不引用一样，就像美国有一个像中国一样的问题。他们只是以某种方式被安抚，或者被切割，被碾碎。你怎么处理这些人？同时，你也不能接触到最重要的技术。至少美国这么认为。中国可能开始认为这是最重要的技术，并开始对其进行补贴，对吗？他们认为电动汽车和可再生能源是最重要的技术。他们现在占主导地位。现在他们开始思考这个问题，在 2010 年代末和 21 世纪初，他们开始思考半导体，现在他们一直在抛售资金，他们正在迅速赶上，他们将在人工智能方面做同样的事情，因为他们非常有天赋。对的。所以，所以，问题是，这什么时候会达到临界点？如果中国认为这是，嘿，他们可以继续，如果不能进入并开始一场真正的热战，接管台湾或试图以某种方式颠覆其民主或封锁它对世界其他地区的伤害远远大于对他们的伤害，这是他们可能会做的事情。对的？那么，这是否有可能推动他们走向这一目标？对的。我不完全是一个地缘政治的人，但你知道，很明显，世界和平和贸易制度对经济来说是超级可怕的，但是， 但在某些时候它可能会破裂。
**Nathan Lambert：** 对吗？我认为我们应该评论一下，为什么中国经济会受到伤害，因为他们的出口很重。我认为美国买了很多东西，就像如果它消失了，就像他们的经济一样。
**Dylan Patel：** 嗯，他们也不能从世界各地进口原材料。对的？美国会关闭马六甲海峡，与此同时，整个美国，你可以说，自 70 年代以来，美国几乎所有的 GDP 增长都是人口增长或科技增长。对的？因为你知道，你今天的生活并不比 80 年代科技之外的人好多少，对吧。你仍然，你知道，你知道，汽车，它们到处都有半导体，冰箱，到处都有半导体，这些有趣的故事是关于俄罗斯人如何拆开洗衣机，因为他们有某些，比如德州仪器的芯片，然后他们可以重新利用并投入到他们的反导弹导弹的东西中，对，比如他们的 S400 或其他东西。你会知道更多关于这个的。但是有各种各样的关于半导体的一切都是我们生活中不可或缺的一部分。

## **09.** 台积电与台湾

**Lex Frdiman：** 你能解释一下台积电在半导体行业中扮演的角色，以及美国如何打破对台积电的依赖吗？
**Dylan Patel：** 我不认为这一定会打破这种依赖。我认为这是让台积电在美国生产，但退一步说，台积电生产世界上大部分的芯片。特别是在铸造方面。有很多公司制造自己的芯片。三星（Samsung）、英特尔（Intel）、意法半导体（STMicro）、德州仪器（Texas Instruments）、模拟设备（Analog Devices），所有这类公司都在制造自己的芯片和 XP。但越来越多的公司将业务外包给台积电，而且这种情况已经持续了几十年。
**Lex Frdiman：** 你能解释一下那里的供应链以及台积电在制造方面的大部分工作吗？
**Dylan Patel：** 当然。所以从历史上看，供应链是公司制造自己的芯片，你知道，这是一家公司，他们开始制造自己的芯片，然后他们设计芯片，制造芯片并销售。随着时间的推移，这变得非常困难，因为建造晶圆厂的成本每一代都在增加。当然，技术，弄清楚它的技术是非常困难的。但仅仅是所需的美元和美分，忽略了，你知道，说，嘿，是的，我有所有的技术能力，顺便说一句，这真的很难得到，对吧？英特尔的失败，三星的失败，等等。但如果你只看建造下一代晶圆厂所花费的美元，它会持续增长，对吧？有点像，你知道，摩尔定律是每两年芯片成本减半。有一条单独的法律，有点像每隔几年就把晶圆厂的成本增加一倍。所以你看看一个领先的晶圆厂，它今天将是盈利的，它正在建造，你知道，3 纳米芯片或 2 纳米芯片在未来，这将花费超过 300 亿美元，400 亿美元。对的。这只是象征性的一笔钱。这就像，这就像基地建设封锁可能需要建立多个。对的。所以当你回顾过去的行业时，你知道，如果我回到 20、30 年前，有 20、30 家公司可以制造最先进的芯片，然后他们会自己设计并销售它们，对吗？所以像 AMD 这样的公司会制造自己的芯片。当然，英特尔仍然在制造自己的芯片。他们非常有名的。IBM 会制造他们自己的芯片，你知道，你可以继续往下看。所有这些公司都制造了自己的芯片。慢慢地，它们像苍蝇一样不断坠落。这是因为台积电所做的，对吗？他们创造了代工的商业模式，也就是说，我不会去设计任何芯片。 我只是去为其他人承包制造商的芯片。他们的早期客户之一是英伟达，对吧？英伟达是唯一一家价值超过 10 亿美元的半导体公司。那是在铸造时代开始的，对吧？所有其他公司都是在那之前开始的，在某个时候都有晶圆厂，这实际上是不可思议的，对吧？你知道，像 AMD，英特尔和博通，这样一个伟大的包。这就像每个人在某个时候都有晶圆厂，或者，你知道，你知道，像博通这样的公司，这就像是各种公司的合并。但即使在今天，博通也有晶圆厂，对吧？他们在科罗拉多州为苹果公司生产 iPhone，射频，无线电芯片，对吗？这些公司都有晶圆厂。对于大多数晶圆厂来说，他们把它们扔掉或卖掉，或者把它们卷进别的东西里。现在大家都依赖台积电，对吧？包括英特尔。他们最新的个人电脑芯片用的是台积电芯片，对吧？它也使用了一些英特尔芯片，但它使用的是台积电工艺。
**Lex Frdiman：** 你能解释一下为什么代工模式对这些公司如此成功吗？他们为什么要追求规模经济？范围？
**Dylan Patel：** 是的。我是说，就像我提到的，对吧？建造一座晶圆厂的成本是如此之高，研发是如此之难。当你看到这些公司有自己的垂直堆栈时，有一个反编码过程，好吧，我对每个特定的芯片都是高度定制的，对吧？但随着我们经历了电子和半导体过去 50 年的历史，你需要越来越多的专业化，对吧？因为摩尔定律已经死了，丹纳德标度也死了。I. 芯片不会因为免费而变得更好，对吧？你知道，从制造业开始，你必须做出真正的建筑创新，对吧？谷歌不仅仅是在英特尔 CPU 上运行网络服务。他们有 YouTube 芯片，他们有 TPU，他们有像素芯片。他们有各种各样的芯片，你知道，产生了谷歌所有的经济价值，对吧？运行，你知道，它运行所有的服务和东西。所以，这只是谷歌，你可以看到这个行业中的任何一家公司。是这样的，对吗？汽车包含 5000 个芯片，你知道，200 种不同的芯片，对吧？所有这些随机的东西。特斯拉的门把手有两个芯片，对吧？就像它很可笑，它是一个很酷的门把手，对吗？这就像，你知道，你不会去想它，但它就像，有两个真正的筹码，像便士一样的筹码，对吗？总之，当你有更多的芯片时，当你有更多的专业化需求时，晶圆厂的成本持续增长时，你需要一个专注于构建最佳工艺技术并使其尽可能灵活的人。
**Dylan Patel：** 我认为你可以简单地说，每个晶圆厂的成本上升，如果你是一个小玩家，生产几种类型的芯片，你不会有偿还晶圆厂成本的需求。然而，英伟达可以拥有许多不同的客户，并将所有这些需求集中到一个地方，然后他们是唯一一个能赚到足够的钱来建造下一个晶圆厂的人。所以这就是为什么这些公司慢慢地被杀死了。因为他们在 10 年前就有了一种芯片，这种芯片是有利可图的，而且足够好，但制造下一个芯片的成本就会上升。他们可能会尝试这样做失败，因为他们没有钱让它工作，然后他们没有任何芯片，或者他们建造它，它太昂贵了，他们只是。
**Dylan Patel：** 有或他们运行，有更多的故障点，对吗？你可能有一个与某种化学蚀刻有关的小过程，或者某种等离子蚀刻，或者一些搞砸了的小过程，你没有设计它，对吗？现在整个公司分崩离析。你不会做薯片，对吧？所以像英特尔这样超级强大的公司，他们就像风化风暴一样，他们今天仍然存在，即使他们在六、七年前搞砸了他们的制造。但就像 AMD，他们几乎破产了。他们不得不把工厂卖给阿联酋的莫巴达拉，对吗？就这样，它变成了一家独立的公司，叫做 GlobalFoundries，这是一家铸造公司。然后 AMD 能够专注于像这样的回报，让我们专注于为不同的市场制造芯片和一堆不同的芯片，专注于特定的工作负载，而不是，你知道，所有这些不同的东西。所以你可以得到更多样化的芯片。设计芯片的公司比以往任何时候都多，但制造芯片的公司却比以往任何时候都少，对吗？这就是台积电的用武之地，他们是最好的，对吧？他们很擅长这个，对吧？他们以客户为中心。他们让你很容易制造你的芯片。他们拿走了所有的复杂性，并试图从你那里抽象出很多东西。他们赚了很多钱。他们不会疯狂地赚钱，但他们会赚很多钱。而且，他们能够聚集所有这些需求，并继续建立下一个晶圆厂。下一个晶圆厂。下一个晶圆厂。
**Lex Frdiman：** 为什么台湾对台积电来说如此特殊？为什么会发生在那里？它能在美国国内复制吗？
**Dylan Patel：** 是的，所以有些方面我会说是，有些方面我会说不是。对的？台积电（TSMC）遥遥领先，因为你知道，德州仪器（Texas Instruments）的前高管莫尔斯 · 张（Morse Chang）没有被提升为首席执行官。他说，去他的，我要去做我自己的芯片公司。对的？他去了台湾，做了台积电，对吧？还有，还有更多的故事。所以他可能是德州仪器，可能是，你知道，可能是台积电，但德州半导体制造，对吧？而不是，你知道，德州仪器，对不对？但是，你知道，整个故事都在那里，但坐在这里。
**Lex Frdiman：** 德克萨斯，我的意思是，这听起来像是一个人类的故事，好像它没有得到推广。
**Nathan Lambert：** 正是张忠谋的才华，你知道，我不会轻描淡写。但也有不同的层面，比如如何，如何工作，对吧？所以在台湾，你知道，就像去最好的学校的学生的最高百分比的毕业生，也就是台大，这些学生的最高百分比都去了台积电工作，对吗？猜猜他们的工资是多少，他们的起薪大概是 8 万美元，7 万美元，对吧？这就像，这就像美国优秀毕业生的起薪，对吗？不，不是顶尖的，顶尖的毕业生在谷歌和亚马逊赚了几十万美元，现在我想全世界都睁开了眼睛，对吧？所以，有一个很大的二分法，比如，社会上最富有的 1% 的人在做什么，由于经济原因，他们要去哪里，对吗？英特尔从来没有付过那么多钱，对吧？这对他们来说毫无意义。对的？这是，这是一个方面，对吧？最好的人去哪里？其次是职业道德，对吧？比如，你知道，我们喜欢工作。你知道，你工作很忙，我们也很忙。但是在一天结束的时候，你知道，当，当你做的工作的时间和数量是什么，一个工厂需要什么，对吗？工厂不是在家工作。他们是，你进入了工厂和艰苦的工作，对不对？有，有，嘿，如果有任何振动，对，地震发生了，振动了机器，它们都，你知道，它们要么坏了，你已经报废了一些产品，然后在很多情况下，它们没有正确校准。所以，所以当台积电，当发生地震时，对，最近发生了地震。台积电不会给员工打电话。他们只是，他们只是去了工厂，就像他们只是出现了，停车场被撞坏了，人们只是去工厂修理它，对吗？就像手臂，就像蚂蚁，对吗？就像，你知道，一群蚂蚁不会被蚁后命令去做什么。蚂蚁就是知道。
**Dylan Patel：** 这就像一个人只专注于这一项任务，就像你要拿着这一件工具，你就是世界上最好的人，这就是你一生要做的事情，这就是工厂里的一项任务，这就是。
**Dylan Patel：** 就像一些特殊的化学加上纳米制造，在一条工具线上不断迭代。是的，这就像，这就像去除二氧化硅的特定等离子体蚀刻，对吗？这就是你整个职业生涯所关注的。它就像是一种特殊的东西。所以任务并不是可以转移的。今天的人工智能很棒，因为人们可以像那样拿起它。半导体制造是非常陈旧和困难的。没有一种材料是在线的，可以让人们轻松阅读和学习的，对吗？这些文件非常密集，需要大量的经验才能学习。因此，这也使得进入门槛更高。所以当你谈到，嘿，你有这些超级专业的人，他们会在工厂里每周工作 80 个小时，对吧？在一个工厂里。如果出了什么问题，他们会在半夜出现，因为地震了，他们的妻子说，地震了。他说，太好了，我要去晶圆厂。作为一个美国人，你会这么做吗？这就像这类事情，你知道，我想这就是例证，就像为什么台积电如此神奇。现在你能在美国复制它吗？让我们不要忽视英特尔是 20 多年来制造业的领导者。他们首先将每一项技术推向市场，除了 EUV、应变硅、高 K 金属栅极、FinFET，你知道，英特尔首先推向市场的技术不胜枚举，从中赚了最多的钱，并首先大规模生产，最好的，最高的利润率，对吧？所以我们不应该忽视英特尔做不到这一点，对吧？这是文化已经打破了，对不对？你投资了错误的东西。他们对 iPhone 说不。他们，他们有很多不同的事情，比如，你知道，晶圆厂管理不善，设计管理不善，这次封锁，对吧？同时，所有这些才华横溢的人，对吗？有 5 万个博士，你知道， 或者在俄勒冈州从事特定化学或物理过程或纳米制造过程几十年的大师们，他们仍然在那里，他们仍然在创造惊人的工作。这就像让它以高产量进入生产的最后一英里，在那里你可以设计，在那里你可以制造几十种和几百种不同的芯片，你知道，它的良好客户体验已经被打破了，对吧？你知道，这就是客户体验。这就像它的一部分，就像人们会说英特尔在 2000 年代，2010 年代太浮夸了，对吗？他们只是觉得自己比所有人都强。工具师说，哦，我不认为这个，这个还不够成熟。他们就像，你只是不知道。我们知道，对吧？这种事情会发生的。美国也可以把它带到。美国能把领先的半导体制造业带到美国吗？是的。对的？我们是对的。
**Nathan Lambert：** 随着时间的推移，亚利桑那州正在变得越来越好。
**Dylan Patel：** 你知道，台积电目前已经在美国建立了大约 20% 的 5 纳米产能。这还远远不够，对吧？你知道，在美国 20% 的容量就像什么都没有，对吧？此外，这仍然依赖于台湾的存在，对吗？有一种重要的方法可以把它分离出来。有研发，也有大批量生产。实际上，世界上有三个地方正在进行前沿研发。有新竹，台湾，有希尔斯伯勒，俄勒冈州，还有平壤，韩国，对吗？这三个地方正在为世界其他地方的尖端半导体进行尖端研发，现在，制造可以更多地分布在全球。这就是这种二分法存在的地方，比如谁在修改这个过程，谁在开发下一代。改善他们的是新竹，是希尔斯伯勒，是平壤。对的。不是像亚利桑那州的其他晶圆厂，对吧？亚利桑那是一个镇纸。如果心树在一年，一年，几年内从地球上消失，亚利桑那州也会停止生产，对吗？这实际上相当关键。我想说的一件事是，如果我有几枚导弹，我就能确切地知道我能在哪里造成最大的经济损失。对的？它不是针对白宫的，对吧？
发言人甲：是研发中心。
**Dylan Patel：** 这是台积电、英特尔、三星的研发中心，然后是一些内存公司，美光和海力士，因为。
**Lex Frdiman：** 他们定义了这些半导体的未来发展。一切都在飞速发展，从根本上说，这都是关于研发的，都是关于台积电的。啊哈？
**Dylan Patel：** 所以台积电，你知道，你不能购买没有台积电芯片的车辆，对吗？你不能购买没有台积电芯片的冰箱。你不能，你可以，你喜欢。讽刺的是，我认为你能买到的为数不多的东西之一是德州仪器（Texas Instruments），比如图形计算器，对吧？因为他们实际上是在德克萨斯州生产的。但除此之外，比如笔记本电脑，电话，你服务的任何东西，对吧？GPU，这些东西都不可能存在。这是没有，没有台积电。在许多情况下，它甚至不像前沿，你知道，性感的 5 纳米芯片，3 纳米芯片，2 纳米芯片。通常情况下，它就像一些愚蠢的电源 IC，就像从一个电压转换到另一个电压。对的。它是台积电制造的，对吧？
**Nathan Lambert：** 就像中国正在投资的一样。这就像他们可以建立这个长尾工厂，那里的技术更为人所知。你不需要解决 EUV 的这些问题。他们在这方面投资，然后他们有大量的供应，比如汽车门把手和随机的东西。这也渗透到了整个经济讨论中，即他们拥有的比我们多得多。有这样的东西供应对正常生活至关重要。
**Lex Frdiman：** 所以他们正在做，他们开始投资大批量生产，但他们没有做研发。所以他们做了。
**Dylan Patel：**R 和 D 自己。他们就在后面。对的。所以我想说，就像在 2015 年，中国有一个五年计划，他们确定了到 2025 年和 2020 年的某些目标，包括 80% 的国内半导体生产。他们不会，他们不会打那个，对，要清楚。但他们在某些地区非常非常接近，对吧？就像比亚迪可能会成为世界上第一家不必使用台积电制造芯片的公司，因为他们有自己的晶圆厂，对，制造芯片。现在他们仍然需要从国外购买一些芯片，例如自动驾驶的 ADAS 功能，因为这些都是非常高端的。但至少你知道，就像内燃机有 40 个芯片和一个电动汽车，只是为了控制流速和所有这些东西。电动汽车甚至更加复杂。因此，所有这些不同的电源 IC 和电池管理控制器以及所有这些东西都是内包的，对吗？这是中国自 2015 年以来一直在做的事情。现在，就像后缘一样，他们在那里获得了如此多的容量。直到前缘，右边。I.E。这个 5 纳米等等，对。他们仍然落后于 GPU，这是。美国的限制措施正试图在后者中阻止他们。但是你知道，所有发生的事情，你知道，是的，他们已经减慢了 5 纳米，3 纳米等等，但是他们已经加速了他们的，嘿，45 纳米，90 纳米功率 IC 或模拟 IC，或者你知道，我的键盘上的随机芯片，对。那种东西。所以，所以有一个角度，就像美国的行动一直如此。从这些出口来看，你知道，从出口管制的角度来看，它是如此具有煽动性，减缓了中国在领先地位上的进步，他们已经转过身来，加快了他们在其他地方的进步，因为他们知道这是如此重要，对吧。 如果美国要把他们锁在这里，或者如果他们把我们也锁在这里。所以回到过去，美国能在这里建造它吗？是的，但这需要一大笔钱，我真的想这么做。彻底改革和完全内包半导体将需要十年时间和一万亿美元。
**Lex Frdiman：** 是不是也有一些文化，就像你说的，在台湾，极端的能力，极端的职业道德。
**Nathan Lambert：** 我认为，如果你有需求，而且钱在线上，美国公司会想办法解决。这需要与政府保持联系。但我认为，这种文化有助于台积电取得突破，对他们来说也更容易。你可以。
**Dylan Patel：** 台积电大约有 9 万名员工。这实际上并不是一个疯狂的数额。亚利桑那晶圆厂有 3000 名来自台湾的员工。这些人，就像，他们的妻子说，是的，我们不会有孩子，除非我们。你报名参加亚利桑那晶圆厂。我们去亚利桑那州，我们的孩子在那里。还有一家日本晶圆厂也发生了同样的事情。对的？所以，就像，这些妻子开车，就像，这些，就像，这些男人，就像，去日本或美国生孩子。它就像，它是文化的一个元素。是的，当然，台湾工作很努力。而且，就像美国过去所做的那样。他们现在就可以做。对的。你知道，我们可以只进口。我说，如果我们愿意，可以引进世界上最好的人才。
**Lex Frdiman：** 这就是移民对话的棘手之处。关于这一点有很多争论。但是，是的，引进世界上最好的人似乎是荒谬的争议。我不明白为什么这会引起争议。那是. 那是获胜的方法之一。
**Nathan Lambert：** 我肯定我们同意你的观点。
**Dylan Patel：** 而且，即使你不能引进这些人，我仍然认为如果有钱的话，你可以在美国做很多事情来制造大部分产品，对吗？
**Dylan Patel：** 所以它要贵得多。它在很长一段时间内都不盈利。
**Dylan Patel：** 这就是 CHIPS 法案的背景，相对于一些可再生能源，你知道，在通货膨胀削减法案和基础设施法案中通过的倡议，总共有数千亿美元，CHIPS 法案只有 500 亿美元。对的。所以，美国花在半导体产业上的钱是。什么都没有。对的？而所有其他国家在职业道德和工作量等方面都有结构性优势。但也有一些 STEM 毕业生，他们最好的百分位去那里。对的？但他们也有不同之处，比如，嘿，法律中只有税收优惠，而且已经有 20 年了。对的。所以，然后，然后一些国家有大量的补贴。对的。中国每年大约有 2000 亿美元的半导体补贴。我们谈论的是美国的 500 亿美元，大约 6 美元。对。所以，腰围或补贴金额的差异也是巨大的。对的。所以我认为，你知道，特朗普最近一直在谈论对台湾征收关税。你知道，这有点像这样的事情，哦，好吧，好吧，就像，你知道，也许他不想补贴半导体行业，很明显，台湾的关税将会花费很多东西，变得更加昂贵。但这会改变台积电在美国建造更多晶圆厂的方程式吗？这就是他的假设。对的。
**Lex Frdiman：** 所以你能摆出。所以我们列出了重要性，顺便说一句，你对这么多东西的了解令人难以置信。
**Nathan Lambert：** 我们告诉过你，迪伦知道所有的事情。
**Dylan Patel：** 是的。
**Lex Frdiman：** 那么，好的，你阐述了为什么台积电非常重要。如果我们展望未来，10 年、20 年后，美中关系似乎可以走向冷战、冷战升级、甚至热战的黑暗境地，或者走向从亦敌亦友到合作再到共同努力的美好境地。那么在这个博弈论中，复杂的博弈，有哪些不同的轨迹？我们应该做什么？你认为美中关系可能有哪些不同的发展轨迹？随着两位领导人开始越来越多地感受到 AGI，并看到芯片的重要性和重要性。
**Nathan Lambert：** 我的意思是，出口管制最终指向一个独立的未来经济。我认为，美国已经向中国领导人明确表示，我们打算不惜以全球经济一体化为代价来控制这一技术。所以很难放松，就像牌已经打到了同样的程度。
发言人丙：他们还限制美国公司进入中国。对的？所以它是，它是。你知道，这是一个漫长的过程。你知道，在某种程度上，你知道，有，有一个融合，对不对？但是，至少在过去的十年里，它的分支越来越远，对吗？就像美国公司不能进入中国一样，中国公司也不能进入美国。美国在说，嘿，中国，你不能在某些领域获得我们的技术。中国用同样的事情来反驳，比如，你知道，他们已经做了一些特定的材料，你知道，镓和类似的东西，他们试图限制美国的一个。有一家美国无人机公司不允许购买电池，他们有类似的军事客户。这家无人机公司只是告诉军方客户，嘿，从亚马逊买就行了，因为我不能亲自去买。对的。就像所有这些正在发生的事情都指向越来越远的分歧。我没有任何想法，如果我们都能手牵手唱 Kumbaya，我会很高兴，但我不知道这怎么可能发生。
**Lex Frdiman：** 分歧对避免战争是好是坏？有没有可能，在制造商芯片方面的分歧，训练人工智能系统实际上有利于避免军事。
发言人丙：世界是有史以来最和平的，这是客观事实。当有全球霸主的时候，对吧？或者是地区霸主，对吧？在历史背景下，对吧？当罗马人在地中海的时候，那里是最和平的，对吗？中国有过非常和平和战争的时期。在和平时期，王朝不仅控制着自己，还控制着周围的所有支流。对的？同样，人类历史上最和平的时期是美国成为全球霸主的时期，对吗？在过去的几十年里，我们已经看到事情开始下滑，俄罗斯，乌克兰，中东正在发生的事情，你知道，台湾风险，所有这些不同的事情都开始冒泡。还是客观的，极其平和。现在，当它不是一个全球霸主，而是两个时，显然会发生什么。而且，你知道，中国将，你知道，有竞争力，甚至超过美国，就像这是可能的，对不对？所以这个，这个全球霸权的变化，我不认为它会非常和平地发生。当帝国衰落时，这对美国来说是一个可能的轨迹，他们不会优雅地倒下。他们不会从无关紧要的地方溜走。通常会有很多震动。因此，美国试图做的是保持其最高地位，而中国试图做的是成为最高地位。很明显，用最简单的话来说，这里有顶撞。
**Lex Frdiman：** 这可能会以各种方式形成，包括代理人战争。
**Nathan Lambert：** 好像已经发生了。尽管我希望有几个世纪的长期和平，但看起来国际上还会有进一步的不稳定。
**Dylan Patel：** 美国目前的任务是，嘿，如果我们控制了人工智能，如果我们是人工智能的领导者，并且人工智能显著加快了进步，那么我们就可以保持全球霸权地位。因此，我希望这能起作用。作为一个美国人，就像，你知道，有点像，好吧，我想这会带来和平，我们的和平。很明显，世界上的其他人也受到了负面影响。你知道，很明显，如果发生这种情况，中国人民将不会处于有利地位。但是，你知道，这是一种现实，就像，正在做的事情和正在执行的行动。

## **10.**  最好的 AI GPU


**Lex Frdiman：** 那么我们可以回到不同硬件的具体细节上吗？在导出控制中有这个漂亮的图形，哪些 GPU 允许导出，哪些不允许导出。你能从技术的角度解释一下其中的区别吗？H20 有前途吗？

![alt text](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031641881.png?x-oss-process=image/resize,w_800)


**Dylan Patel：** 是的。所以这就开始了。我认为我们必须喜欢，我们需要真正深入到推理方面，以及那里发生了什么。但是 H20，你知道，美国已经经历了出口管制的多次迭代，对吧？这个 H800 在 23 年的时候被允许，但后来它被取消了，到那时 DeepSeek 已经建立了他们的集群。他们声称有 2K。我认为他们实际上有更多的东西，比如 10K。现在这个 H20 是法律允许的芯片，对吗？英伟达去年向中国出口了 100 万台。对的。对于上下文，有大约 400 万或 500 万个 GPU，对吗？所以中国特有的 GPU 的百分比，H20 是相当高的，对吧？大概是 20%，25%。对，20% 左右。所以这个 H20 在某种程度上被阉割了，但它实际上在其他方面得到了升级，对吧？你知道，你可以想象沿着人工智能的三个轴的芯片，对吧？你知道，忽略软件堆栈，喜欢精确的架构，只是原始的规范。有浮点运算，对吧？FLOPS，在内存容量中有内存带宽，对吗？爱娥，对吗？记忆。然后还有互联，对吧？芯片到芯片互连。这三点对于制造人工智能系统都非常重要，对吧？因为人工智能系统涉及到大量计算，它们涉及到大量的移动内存，无论是内存还是其他芯片，对吧？所以这三个矢量，美国最初控制了其中两个矢量，其中一个没有控制，这就是 FLOPS。和互连带宽最初受到控制。然后他们说，不，我们要去掉互连带宽，只做一个非常简单的触发器。但现在英伟达现在可以制造一种芯片，好吧，它减少了失败。它是 H100 在规格纸上的 1/3，在现实世界中的 FLOPS 性能，它接近一半，甚至可能是 60%。对的？但是在另外两个矢量上， 它在互连带宽和内存带宽和内存容量方面同样出色，H20 比 H100 具有更大的内存带宽和内存容量。对的？最近，你知道，我们，我们在我们的研究中，我们大幅削减了英伟达今年 H20 的产量。他们今年打算再做 200 万个。但几周前他们取消了所有的订单。在我们看来。这是因为我们认为他们认为他们会受到限制。对的。因为他们为什么要取消所有 H20 的订单？因为他们去年卖出了一百万个。他们今年有几百万的订单，然后就走了。对的。对于 H20，B20。对的。H20 的继任者。现在他们都走了。他们为什么要这么做？对的。我想这很清楚，对吗？H20 实际上更适合某些任务。而这个特定的任务就是推理。对的。当你看到模型的不同状态时，推理是非常不同的。对的。赛前训练都是关于翻牌的。对的？都是关于失败的。你可以做一些事情，比如我们谈到的混合专家来权衡互连或权衡其他方面，降低 FLOPS，更多地依赖互连和内存。但说到底，失败就是一切。对的。我们谈论的模型是根据它们有多少次翻牌。对的。所以我们说，GPT4 是 2E25。对的。2 到 25，25 个零。触发器浮点操作。
**Dylan Patel：** 为了训练。
**Dylan Patel：** 为了训练。对的。我们讨论的是 2E24 的限制。对的。25 随便。美国有一项特朗普最近未签署的行政命令，那就是，嘿，1E26。一旦你达到了浮点运算的数量，你必须通知政府，你必须与我们分享你的结果。对的。就像有一种模式，美国政府必须被告知。对的。那就是 1E26。所以在我们前进的过程中，这是非常重要的。失败是政府历来关心的载体。但其他两个载体可以说同样重要。对的。特别是当我们来到这个新的范例时，世界在过去的六个月里才刚刚了解到。对的。推理。
**Lex Frdiman：** 我们是否完全理解三个维度中哪一个最适合推理？所以互连，FLOPS 并不重要。是记忆吗？
**Dylan Patel：** 记忆，对，是的。长度。我们很快就会进入技术层面。
**Dylan Patel：** 这里面有两篇文章，我可以展示一下，也许你可以把有趣的图片拿出来给听众看。
**Lex Frdiman：** 我们正在看 O1 推理架构标记组学的部分。
**Dylan Patel：** 在我们讨论这个之前，你想解释一下 KVCache 吗？我认为最好是。
**Nathan Lambert：** 好的，是的，我们需要通过很多具体的技术手段，Transformer 来让人们更容易做到这一点。
**Dylan Patel：** 因为它非常重要。因为这改变了模型的工作方式。但我认为重置。对的。为什么记忆如此重要？这是因为到目前为止，我们已经讨论了参数计数。对的。和专家混合。您可以更改活动参数与总参数的数量，以嵌入更多数据，但具有更少的 FLOPS。但更重要的是，你知道，另一个方面，你知道，在过去几年里，这场巨大革命的一部分是 Transformer。对的。和注意力机制。注意机制是模型理解上下文中所有单词之间的关系。对的。那就是。这与参数本身是分开的。对的。那就是。这是你必须计算的东西。对的。显示每个令牌。对的。上下文长度中的每个单词彼此相对连接。对的。我认为，Nathan，你应该更好地解释 KVCache。
**Lex Frdiman：**KVCache 是其中一项优化。
**Dylan Patel：** 是的。
**Nathan Lambert：** 所以注意力操作符有三个核心。它是查询、键和值。QKV 是进入这个领域的东西。你会看到这个方程，你会看到这些矩阵相乘。“ query ”、“ key ” 和 “ value ” 这些词来自信息检索背景，其中查询是您试图获取其值的事物。你访问的键和值是重新加权的。我的背景不是信息检索之类的。有反向链接很有趣。实际上，当你做这些矩阵乘法时，你得到的矩阵的大小就是上下文的长度。因此，您放入模型和 KVCache 中的令牌数量实际上是模型中所有先前令牌的某种形式的压缩表示。所以当你这样做的时候，我们谈论自回归模型。你一次预测一个代币。你从你的提示开始。你会问这样的问题：谁是 1825 年的总统？然后，模型将生成其第一个令牌。对于这些标记中的每一个，您都在执行相同的注意操作符，其中您将这些查询键值矩阵相乘。但是数学是非常好的，所以当你重复这样做的时候，这个 kvcache，这个键值操作，你可以不断地向它附加新的值。所以你要记录你在这个自回归链中推断的先前的值。你一直把它记在脑子里。在大规模服务推理时，这是一件非常重要的事情。在这方面有更大的专家，有这么多层次的细节，你可以进入。从本质上讲，注意力操作符和转换器的主要缺点之一是存在与上下文长度成比例的二次记忆成本。因此，当你提出较长的问题时，为了进行计算而使用的内存以二次方的形式增加。 你会听到很多其他的语言模型架构，就像次二次或线性注意形式，就像状态空间模型。我们现在不需要把这些都拿下来。然后是注意力方面的创新，使这种内存使用和长时间关注的能力更加准确和高效。
**Lex Frdiman：** 这些创新将帮助你，我的意思是你的记忆高度受限。
**Nathan Lambert：** 它们有助于解决内存限制和性能问题。所以如果你把一本书放进。我认为双子座是人们使用的上下文长度最长的模型。双子座以 100 万和现在的 200 万上下文长度而闻名。你把一整本书放进双子座，有时它会从中引出事实。它并不完美。他们越来越好了。所以有两件事。第一，为了能够在内存级别上提供服务，谷歌拥有神奇的 TPU 堆栈，他们可以提供非常长的上下文。在此过程中也有许多决策，以实际使长期接触性能发挥作用，从而提供数据。注意力的计算发生了微妙的变化，它改变了体系结构。但是服务于长时间的上下文是非常受记忆限制的，尤其是当你做了很多预测的时候。实际上，我不知道为什么输入和输出令牌更昂贵，但我认为从本质上来说，输出令牌必须进行更多的计算，因为您必须从模型中采样。
**Dylan Patel：** 我可以解释。所以今天，如果你使用一个模型，就像你看一个 API 一样，OpenAI 每百万个代币收取一定的价格，对吗？输入代币和输出代币的价格是不同的。原因是，当你在模型中输入一个查询时，你知道的，对吧？假设你有一本书，对吗？你现在必须计算这本书的整个 kV 缓存，对吗？这个键值缓存。所以当你这样做的时候，这是一个并行操作。所有的代币都可以一次性处理，因此你可以大大减少你的花费。对的？生成令牌和输入令牌的 FLOP 要求是相同的，对吗？如果我输入一个令牌或生成一个令牌，它完全相同。我必须通过模型，对不对？但不同的是，我可以这样做。输入，那就是预填充，那就是批量性质的同时提示，对吧？因此一切都失败了。
**Lex Frdiman：** 我认为他们主要用于输入代币的定价模型大约是输出代币价格的四分之一。
**Dylan Patel：** 对吗？但是输出令牌，它如此昂贵的原因是因为我不能并行操作，对吗？它是自回归的。每次我生成一个令牌时，我不仅必须获取整个，我还必须将整个模型读入内存并激活它，对吗？去计算它以生成下一个令牌。我还必须读取整个 kV 缓存，并生成一个令牌，然后我附加我生成的一个令牌和它的 kV 缓存，然后我再做一次。对的？因此，这是一个非并行操作，在预填充或提示的情况下，您必须将整个模型拉入，并一次计算 20，000 个令牌，对吗？
**Dylan Patel：** 这些都是 API 提供的功能，就像即时缓存，预先填充，因为你可以降低价格，如果你知道你要保留，你可以让 API 更快，如果你经营一家企业，你要继续将相同的初始内容传递给克劳德的 API，你可以将其加载到 Anthropic API 中，并始终保留在那里。但这与我们引导推理模型非常不同，我们之前展示了这个例子，并阅读了一些含糊的东西。发生的情况是，输出上下文的长度要高得多。我从 Dylan 的工作中学到了很多，本质上是当输出长度变得更高时，你会根据使用的内存和我们拥有的 GPU 来写这个二次方，实际上你会用完内存，它们都试图同时服务多个请求。因此，在不是所有的提示都完全相同的情况下进行批处理，处理起来非常复杂。然后随着上下文长度变得更长，我想你称之为关键批量大小，你服务更多用户的能力，所以你可以并行化你的推理的程度因为这个长期合同而直线下降。所以你的内存使用随着这些推理模型而上升，你仍然有很多用户。所以实际上，服务成本乘以一吨。
**Lex Frdiman：** 我们在看一张图，X 轴是序列长度，即。
**Dylan Patel：**E. 正在生成多少个令牌提示，对吗？所以如果我放进一本书里，那就是一百万个代币，对吗？但你知道，如果我放进去，你知道，天空是蓝色的，那就像六个代币或什么的。
**Lex Frdiman：** 我们应该说，我们所说的推理和思维链正在延长这个序列的长度。
**Nathan Lambert：** 主要是输出。
**Dylan Patel：** 三个月前，每当 O1 发布时，所有长上下文长度的用例都是这样的，让我把大量的文档放进去，然后得到一个答案，对吗？它是一个单一的，你知道，预填充，并行计算很多，然后输出一点点。现在有了推理和代理，这是一个非常不同的想法，相反，我可能只有，嘿，做这个任务或者我可能有所有这些文件。但在一天结束的时候，模型不只是像生产一点点，对吧？它产生了大量的信息。这一连串的想法只是继续去，去，去，去。所以序列长度实际上是，你知道，如果它生成了 10，000 个令牌，它就是 10，000 个序列长度，对吗？或者，加上你在提示符中输入的任何内容。这张图显示的是对数图，对吗？当你从 1K 增长到 4K 或 4K 增长到 16K 时，你的 kV 缓存的内存需求增长如此之快，以至于你最终无法运行一定数量的序列长度，或者你可以看到的用户数量，比如说模型。
**Nathan Lambert：** 这是 405B 的照片。
**Lex Frdiman：** 型号和批量大小 64Llama 31 405B。
**Nathan Lambert：** 是的。批量大小对他们来说至关重要，就像你想要有更高的批量大小来并行化一样。
**Dylan Patel：** 同时处理 64 个不同的用户，对吗？
**Nathan Lambert：** 是的。
**Dylan Patel：** 因此你们的服务成本更低，对吗？因为服务器的成本是一样的。对的。这是 8 个 H1 100，每个 GPU 大约每小时 2 美元。每小时 16 美元，对吗？也就是说，这有点像固定成本。当然，你可以做一些事情让它更低，但就像现在每小时 16 美元，你能服务多少用户？您可以生成多少个令牌？然后你把两者分开，这就是你的成本。对的。在推理模型中，这就是复杂性产生的原因，也是记忆如此重要的原因。因为如果你只有有限的内存，那么你就不能为这么多的用户服务。如果你的内存有限，你的服务速度就会降低。对的。所以你的成本变得非常非常糟糕，因为突然之间，如果我习惯了，嘿，在这个每小时 16 美元的服务器上，我在服务 Llama 405B，或者如果我在服务，你知道，DeepSeek V3，它是所有聊天风格的应用程序，那就是我们只是在聊天。序列长度是几千，几千，对吗？你知道，当你使用一个语言模型时，大部分时间都是几千个上下文长度。有时你会丢弃一份大文件，但当你处理它时，你会得到答案，然后把它扔掉，对吗？你继续做下一件事，对吗？而通过推理，我现在可以按顺序生成成千上万个标记，对吗？所以这个，这个内存，这个 kV 缓存必须保持常驻，你必须不断加载它。你必须保持它，不断地保持它在记忆中。现在这就把其他用户排除在外了，对吧？如果现在有一个推理任务，并且模型能够进行推理，那么突然之间，内存压力意味着我不能同时为许多用户提供服务。
**Nathan Lambert：** 让我们再次进入 DeepSeek。所以我认为，我们在 DeepSeek R 之后有一次，这个市场有两个方面。看着它有多难伺候。一方面，我们将讨论 DeepSeek 本身。他们现在有一个聊天应用程序，在应用程序商店中排名第一。App Store 上的第一条免责声明是用速度来衡量的。因此，这并不是说拥有 DeepSeek 应用程序的人比拥有 ChatGPT 应用程序的人多，但它仍然是值得注意的。克劳德从来没有在应用程序商店中排名第一，即使旧金山的每个人都说，哦，我的上帝，你必须使用克劳德，不要使用 ChatGPT。所以 DeepSeek 击中了这个。他们最近还推出了一个 API 产品，您可以 ping 他们的 API，并获得 R1 的超长响应。在这些东西出来的同时，我们会知道它们发生了什么。因为 Deepsea R1 的模型重量是公开可用的，并且许可证非常友好。商业上可用的 MIT 许可证。所有这些中型公司和大型公司都在努力成为第一个为其用户提供 R1 服务的公司。我们试图评估 R1，因为我们正在进行类似的研究。我们发布了这个模型，我们试图与它进行比较，在所有为 R1 提供服务的公司中，他们的价格比 DeepSeek API 高得多，他们中的大多数几乎不起作用，而且吞吐量非常低。

## **11.** 为什么 DeepSeek 这么便宜


**Dylan Patel：** 给出上下文，对吧，各位。其中一个让人抓狂的部分是中国达到了能力。另一个方面是他们做得很便宜，对吧？我们在训练方面讨论了为什么这么便宜。
**Lex Frdiman：** 是的，让我们谈谈为什么它这么便宜。在推论上，它工作得很好，而且很便宜。为什么 R1 这么便宜？
**Dylan Patel：** 所以我认为这里有几个因素，对吗？一个是他们确实有模型架构创新，对吧？这个 MLA，他们所做的这个新的注意力与注意力是不同的，注意力是你现在所需要的转变我们的注意力。其他人已经创新了。有很多工作，比如 MQA，GQA，本地和全球。所有这些不同的创新都试图改变曲线，对吧？它仍然是二次的，但常数变小了。对的？
**Dylan Patel：** 与我们之前的讨论相关，这种多头潜在注意力可以从注意力机制中节省大约 80% 到 90% 的记忆，这在长时间的语境中尤其有用。
**Dylan Patel：** 与原来的相比，这是 80% 到 90%。但与人们实际所做的相比。这仍然是一种创新。
**Nathan Lambert：** 这个 80% 到 90% 并不是说整个模型便宜了 80% 到 90%。只有这一部分。
**Dylan Patel：** 嗯，不仅仅是这样，对吗？像其他人一样，已经实现了局部、全局、滑动窗口和 GQ MQA 等技术。但无论如何，就像 DeepSeek 一样，他们的注意力机制是一种真正的架构创新。他们做了大量的实验，这大大降低了内存压力。它还在那里，对吗？它仍然是一个二次方程。这仍然是一种紧张。它仍然是二次的。相对于以前的形式，它只是大大减少了它。
**Lex Frdiman：** 对吗？这就是内存压力，我应该说，以防人们不知道 R1 比 01 便宜 27 倍。
**Nathan Lambert：** 我们认为 OpenAI 有很大的空间。好的，所以有很多因素。我们应该分解因素。
**Lex Frdiman：** 我认为 R1 的每百万代币产出为 2 美元，每百万代币产出为 60 美元。4,01.
**Nathan Lambert：** 是的，让我们看看这个。
**Dylan Patel：** 所以，所以我认为这是非常重要的，对吗？你知道，OpenAI 是 DeepSeek 和定价之间巨大差距。但是 DeepSeek 提供了相同的模型，因为他们以非常相似的价格向其他所有人开放重量，比其他人能够提供的价格低得多。对的？所以这里有两个因素，对吧？他们的模型更便宜，对吗？它便宜 27 倍。嗯，我记不起确切的数字了。
**Lex Frdiman：** 所以我们正在看一张图，它显示了服务于 V3 的不同位置，DeepSeek V3，它类似于 DeepSeek R1。而且在服务成本上有很大的差异。服务成本。如何解释这种差异呢？
**Dylan Patel：**OpenAI 有很大的优势，对吧？当他们在做推理时，他们在服务。他们的毛利润率超过 75%。对的？所以这是成本差异的 4 到 5 倍的因素，OpenAI 只是赚了一大笔钱，因为他们是唯一有这个能力的人。
**Lex Frdiman：** 他们需要那笔钱吗？他们用它来研发吗？
**Dylan Patel：** 作为一家公司，他们显然在亏损，因为他们在训练上花了很多钱，对吗？所以推论本身是非常高的利润，但它并不能收回他们所做的一切的成本。好的？所以，是的，他们需要这笔钱，因为在筹集更多资金的同时，继续建设下一件事所需的收入和利润。
**Lex Frdiman：** 所以我的建议是，DeepSeek 就像真的把钱放出来一样。
**Dylan Patel：** 嗯，所以，所以这里有一件事，对吗？我们马上就来。但就像 DeepSeek 一样，它没有任何能力来实际服务于模型。他们停止了注册。使用它的能力现在就像不存在一样，对吗？对大多数人来说。因为很多人都在尝试使用它，他们只是没有 GPU 来为它服务。对的？OpenAI 在他们和微软之间有数十万个 GPU 来服务他们的模型。DeepSeek 的系数要低得多，对吧？即使你相信我们的研究，这是 50，000 个 GPU，其中一部分用于研究，一部分用于对冲基金，对吗？他们仍然没有接近 GPU 体积和容量的地方来服务于大规模的模型。所以它更便宜。其中一部分是 OpenAI 赚了很多钱。DeepSeek 在他们的 API 上赚钱是未知的吗？实际上我不这么认为。其中一部分就是这张图表，对吧？看看所有其他的供应商，对吧？一起 AI 烟花 AI 是非常高端的公司，对吧？X Meta Together AI 是 Tridao 和 Like Flash Attention 的发明者，对吗？这是一种非常高效的技术，对吧？他们效率很高，是很好的公司，他们在服务。我知道那些公司是赚钱的，对吧？不是，不是在推理上赚很多钱，但他们赚钱。所以他们的服务成本相差 5 到 7 倍，对吧？所以你现在知道，当你把 OpenAI 等同起来时，好吧，OpenAI 赚了很多钱，这就像是 5 倍的差异。而那些试图通过这种模式赚钱的公司就像是 5 倍的差异。还是有差距的吧？还是有差距的。这只是 DeepSeek，真的很好。对的？模型架构，MLA，他们做 MOE 的方式，所有这些都像是合法的效率差异。
**Nathan Lambert：** 我们在训练中谈到的所有其他低级库，其中一些可能会转化为推理，而这些没有发布。
**Lex Frdiman：** 所以我们可能会有点阴谋论，但有没有可能中国政府正在资助 DeepSeek？
**Dylan Patel：** 实际上我不认为他们是。我认为，当你看中国的实验室时，华为有一个实验室，Moonshot AI，还有其他几个与政府关系密切的实验室，还有像阿里巴巴和 DeepSeek 这样与政府关系不密切的实验室。你知道，我们谈到了这个，这位首席执行官，这位虔诚的人物，他喜欢完全不同的人，他喜欢。听起来很棒，非常不同，就像基于翻译的中国采访的观点，而不是中国共产党可能想要的。现在要弄清楚，对，他是否有一个亏损领导者，因为他可以通过他的对冲基金来融资？是啊，当然。
**Lex Frdiman：** 所以对冲基金可能在补贴它？
**Dylan Patel：** 是的，我的意思是他们绝对做到了。对的。因为 DeepSeek 没有筹集到多少资金。他们现在正试图在中国进行一轮融资，但从历史上看，他们还没有筹集过资金。这一切都是由对冲基金资助的。他拥有公司一半以上的股份。他拥有公司 50%、60% 的股份。
**Nathan Lambert：** 在一些面试中，有关于这样做如何成为一种招聘工具的讨论。你也可以在美国公司看到这一点。这就像 GPU 招聘工具处于人工智能招聘工具的最前沿。
**Dylan Patel：** 开源。
**Nathan Lambert：** 开源招聘工具。
**Dylan Patel：** 太有才了。他们远远落后，他们得到了这么多的人才，因为他们只是开源的东西。
**Lex Frdiman：** 更多的阴谋思想。有没有可能，因为他们是一个对冲基金，他们用这个版本和定价来安排一切，他们做空了英伟达的股票和 USAI 公司的股票，并与 Stargate 一起发布。就像能够赚钱的完美时机。
**Nathan Lambert：** 就像他们在总统就职日发布的一样。他们知道国际日历上有什么。但我的意思是，我并不期望他们这样做。如果你听听他们对人工智能的动机。
**Dylan Patel：** 好像他们在 12 月 26 日发布了 V3。比如谁发布了没人看的一天。对的。他们在这之前已经公布了文件，对吗？V3 论文和 R1 论文。所以人们一直在看着它，然后说，哇。然后他们刚刚发布了 V.R1 模型。我认为他们只是在尽可能快地发货，谁在乎圣诞节？谁在乎，你知道，在中国新年之前把它拿出来。对的。很明显，刚刚发生的事。我不认为他们实际上是在把握市场时机，或者试图制造最大的轰动。我想他们只是在发货。
**Nathan Lambert：** 我认为这是他们的一大优势。我们知道很多美国公司在安全方面非常投入，这是像 Anthropic 这样的地方的核心文化。我觉得人类听起来是个很棒的工作场所。但如果安全是你的首要目标，那就需要更长的时间才能把文物弄出来。这就是为什么 Anthropic 不开源的原因。这就是他们的主张。但内部有评论。人类向国际政府提及事情。有消息称，Anthropic 与英国安全研究所（UK Safety Institute）进行了预发布测试。所有这些都增加了把东西拿出来的过程的惯性。我们在这条趋势线上，进展非常快。所以如果你减少你的模型完成训练的时间，你运行评估，这很好。你想尽快把它拿出来，以最大限度地提高你的产出的感知质量。深海在这方面做得很好。
**Dylan Patel：** 达里奥明确表示，克劳德 3.5 十四行诗是在九个月或九到十个月前训练的。九到十个月前。我想他们又花了几个月的时间来发布它。对的。所以这就像是，这里有一个很大的差距。对的。特别是在推理模型方面，旧金山街头的说法是，就像人类有一个比 O3 更好的模型。对的。他们不会释放它。为什么？因为思想的枷锁是可怕的。对的。他们确实很可怕。对的。如果你看 R1，它在中文和英文之间来回切换。有时是胡言乱语。然后正确答案就出来了。对的？对你和我来说，这就像，太好了。
**Dylan Patel：** 我的意思是，这就是为什么人们迷恋。你就像，你告诉我这是一个高价值的东西，它是有效的，它正在这样做。
**Dylan Patel：** 太神奇了。我的意思是，你谈到了那种类似于哲学思想的链条，这不是他们训练出来的好的哲学。这只是它所做的思维链训练的一种人工制品。但这一点非常重要。我能检查你的思想和你现在在想什么吗？不。所以我不知道你是不是当着我的面撒谎。思维模型的链条就是这样。对的。在聊天应用程序中，这是一个真正的风险，嘿，我让模型说脏话或其他什么，或者如何制造炭疽，它告诉我这是不安全的。一定。但这是我可以相对容易地摆脱的东西。如果我告诉人工智能去做一个任务，然后它突然以一种我不想要的方式随机地做了这个任务呢？对。现在有更多的任务和反应是非常不同的。对的。所以安全的标准要高得多。至少这是人类的情况。对的。就像 DeepSeek 一样，他们就像一艘船，对吗？
**Lex Frdiman：** 是的。所以，我的意思是，由于 DeepSeek，安全标准可能降低了一些。我的意思是，这和太空竞赛有相似之处。苏联可能首先把人送上太空的原因是因为他们的安全方法是障碍。
**Dylan Patel：** 安全性降低了，他们杀了那只狗。对的。还有所有这些东西。对的。
**Lex Frdiman：** 所以它比美国的项目更不容易规避风险。这里有相似之处，但美国公司的安全栏可能会有下行压力。对的。
**Nathan Lambert：** 这是达里奥谈论的事情。这是达里奥想要避免的情况，达里奥也谈到了 “向下竞争” 和 “向上竞争” 之间的区别。“冲顶竞赛” 是一场高标准的安全竞赛。你的模型性能和某些关键评估有很高的标准。当某些公司确实对 IT 很好时，他们就会趋同。这就是我的想法。最终，人工智能并不局限于一个国家，也不局限于一套道德规范。有很多关于我们是否应该停止开源模式的争论。如果美国停止，这是很清楚的。我的意思是，现在在 DeepSeek 更容易看到，一个不同的国际机构将是建立它的人。我们谈论训练的成本。DeepSeek 有一个令人震惊的 500 万美元的数字。想想看，世界上有多少实体能够负担得起 100 倍的成本，才能拥有世界上人们使用的最好的开源模型。这是一个可怕的现实，无论我们是否想要阻止它们，这些开放模式可能会暂时继续出现。而且确实如此。阻止他们可能会让情况变得更糟，更难准备，但这只是意味着准备和理解人工智能能做什么要重要得多。这就是为什么我在一天结束的时候在这里。但这就像让它深入到人们中，尤其不是在人工智能中，这是即将到来的。在一个全球互联的世界里，你必须接受一些结构性的东西。
**Lex Frdiman：** 是的，你提到，你给我发了一些扎克，马克 · 扎克伯格在一次收益电话会议上提到的东西。他说，我认为根据最近的一些新闻，新的竞争对手，来自中国的 DeepSeek。我认为这是我们正在讨论的一件事，那就是在全球范围内将会有一个开源标准。我认为为了我们的国家利益，它是美国标准是很重要的。所以我们认真对待。我们想要建立全世界人们都在使用的人工智能系统。我认为，如果说有什么不同的话，那就是最近的一些新闻只是加强了我们的信念，即这是值得关注的正确事情。所以，是的，开源。
**Nathan Lambert：** 是的。马克 · 扎克伯格（MarkZuckerberg）对美国价值观以及他如何展示公司的发展轨迹并不陌生。我认为他们的产品在中国早就被禁止了，我尊重这种直接的说法。

## **12.** 间谍

**Dylan Patel：** 有一个有趣的方面，仅仅因为它是开放的权重或开源并不意味着它不能被颠覆。对的。有很多开源软件的错误，比如，有一个 Linux 的错误，在 10 年后才被发现，这显然是一个后门，因为有人说，为什么要花半秒钟来加载？
**Nathan Lambert：** 这是最近的，对吗？
**Dylan Patel：** 比如，为什么这需要半秒钟来加载？这就像，哦，糟糕，这里有一个后门。这就是原因。对的。这就像，这在今天的人工智能模型中是非常可能的。你知道，这些模型的排列非常清晰。对的。就像，我不会说脏话。我不会教你如何制造炭疽，我也不会谈论 XXX。我不会，你知道，像这样的事情，我会说台湾是一部分，你知道，是，只是一个东部省份。对的。就像，你知道，所有这些事情都像，取决于你是谁，你排列什么，你知道什么，你是否知道。甚至像 Xai 是以某种方式排列的，对吧。你知道，他们可能是。它不是在类似觉醒的意义上对齐，也不是在类似亲中国的意义上对齐。但模型中充满了某些东西。现在，当你在一个开放权重的指令模型中公开发布时，这可能会激增，对吧？但随着这些系统变得越来越强大，你可以在模型中深入嵌入的内容并不清楚。对的。所以有。这就像其中一个最大的恐惧是，如果美国模特或中国模特是顶级模特，对。你要嵌入一些不清楚的东西，也可能是无意的，对吗？就像英国英语死了，因为美国法学硕士赢了。对的。互联网是美国的，因此颜色的拼写方式就是美国人的拼写方式。对的。
**Lex Frdiman：** 现在这只是激烈的言辞。
**Dylan Patel：** 这就像，这只是有限责任公司地毯的实际性质。
**Nathan Lambert：** 英语是最热门的编程语言，而英语是由一群主要位于旧金山的公司定义的。
**Lex Frdiman：** 拼写优化的正确方法是用 Z，以防人们在听。它是一个。我认为它是英式英语中的 s。
**Dylan Patel：** 它把它带到了一些愚蠢的事情上，对吗？就像拼写一样愚蠢。就像英国人和英国人，你知道，英国人和美国人可能会喜欢笑，对吧？我不认为我们在乎那么多。但是，你知道，有些人会。但这可以归结为非常非常重要的话题，比如，你知道，你知道，颠覆人们，对吧。你知道，聊天机器人，对吧？角色人工智能已经表明，他们可以喜欢，你知道，与孩子和成年人交谈，喜欢它会喜欢人们的感觉。对的。这是无意的排列。但是，当在开源标准的深处存在有意的一致性时，会发生什么呢？它是我们今天发现的 Linux 或一些加密系统的后门。中国使用的加密方式与 NIST 对美国 NIST 的定义不同，因为很明显，至少他们认为其中有后门，对吧？当这些模型不仅是计算机系统的后门，也是我们大脑的后门时，会发生什么？
**Nathan Lambert：** 是的，他们是文化后门。文化与语言模型之间的相关性被放大了，因为我们已经习惯了这种与人互动的模式，在来回的对话中，我们现在有一个非常强大的计算机系统，它可以插入到我们习惯的社会环境中，这让人们非常。我们不知道人们会受到多大程度的影响。
**Lex Frdiman：** 所以可能有。这是一个，这是中国公司提供公开重量模型的一个实际问题，可能有一些中国政府的秘密要求这些模型有某种后门，有一些我没有的东西。
**Dylan Patel：** 一定认为这是一个后门。对的。因为一旦它打开了重量，它就不喜欢打电话回家了。它更多的是关于它是否能识别某个系统。它可以，就像如果。现在，现在它可能是一个后门，就像，嘿，如果你正在构建一个软件，软件中的一些东西，突然它就是一个软件代理。哦，编程这个只有我们知道的后门。或者它可以像颠覆思想，认为 XYZ 的意见是正确的。
**Nathan Lambert：**Anthropic 在这方面的研究表明，如果你在训练前加入某些短语，当你实际使用模型时，你可以引发不同的行为，因为它们已经毒害了训练前的数据。到目前为止，我不认为生产系统中的任何人会尝试做这样的事情。我认为它主要是人类在做非常直接的工作，而且大多只是微妙的事情。我们不知道这些模型将是什么，它们将如何生成令牌，它们将表示什么信息，以及它们的复杂表示是什么。
**Lex Frdiman：** 嗯，我们正在谈论的一件事，人类，通常只是充满了试图在世界上做好事的好人。我们只是不知道有任何实验室，这将在军事环境中进行，并明确训练。好吧我们怎么能。前门看起来像一个快乐的法学硕士，但在它下面的东西会随着时间的推移对我们所谓的敌人造成最大的伤害。
**Dylan Patel：** 山姆 · 奥特曼有一句非常好的名言，你知道，他有时可能是超级野兽，但他说的一件事，我想我同意，那就是超人的说服力会在超人的智慧之前发生。对的。如果是这样的话，那么这些东西，在我们得到这个 AGI ASI 的东西之前，我们可以对我们的理想或模型制作者的理想嵌入超人的说服力。对的。今天，我真的不相信 DeepSeek 做得对。但这预示着可能发生的事情。
**Lex Frdiman：**《美丽新世界》描述了一个反乌托邦的世界。所以我们可能只是在 Instagram 上滚动，看着可爱的小狗，或者更糟，然后与机器人交谈，给我们一个故事。我们完全迷失在别人控制的世界里，而不是独立思考。随着我们越来越依赖这类系统，这是一个主要问题。
**Nathan Lambert：** 我的意思是，我们已经在推荐系统中看到了这一点。
**Dylan Patel：** 是的，推荐系统破解了多巴胺诱导的奖励回路，但大脑要复杂得多。还有什么其他类型的回路，你大脑中的反馈回路，你可以破解，颠覆，比如推荐系统，纯粹是为了增加时间和广告等等。但通过这些复杂的模型，可以实现更多的目标。
**Dylan Patel：** 在几年内，你没有理由不能训练一个语言模型来最大化在聊天应用上花费的时间。就像现在他们受过训练。
**Dylan Patel：** 我的意思是，这不是 AI 所做的吗？他们每节课的时间大概是两个小时。
**Nathan Lambert：** 是的，角色人工智能很可能会优化这一点，就像收集数据的方式是幼稚的，就像你有几个选项，你可以选择它们。但这并不是训练这些模型的唯一方法。
**Dylan Patel：** 天真的事情，比如和一个动漫女孩说话。但就像是，是的，这是一种冒险。对的？
**Lex Frdiman：** 就像这样，这是一件陈词滥调的事情，但在过去的一年里，我有几段时间根本不使用社交媒体或互联网，只是读书和在大自然中。很明显，它对大脑的变化产生了影响。我觉得我又回来了。当然，我是在互联网真正起飞之前长大的，但我正在回归更多。
**Nathan Lambert：** 我知道你要去哪里。我的意思是，你可以从生理上看出来。如果我是背包客或什么的，我需要三天。你是字面上的意思，你打破了上瘾的循环。
**Lex Frdiman：** 我觉得我能更好地控制自己的思想。当我与互联网断开连接时，感觉就像是一种智慧的主权。我认为我使用互联网和社交媒体越多，其他人就越能控制我的思想。那绝对是一种感觉。然后在未来，这将不是其他人，而是算法或其他人通过算法呈现给我。
**Nathan Lambert：** 我的意思是，互联网上已经有大量的人工智能机器人。所以现在它并不频繁，但每隔一段时间我就会回复一个，他们会立即回复，我就像一个垃圾，那是一个机器人。这只会变得更加普遍。就像他们会变好一样。
**Dylan Patel：** 在技术的历史上，有一件令人捧腹的事情是，非法成人娱乐业总是首先采用技术。对的。不管是不是像视频流。是啊。就像你知道的那样，现在有一些独立的成人非法内容创作者，他们有自己的订阅页面，他们实际上大量使用，你知道，生成人工智能已经像扩散模型一样，所有这些都是巨大的。但现在这些类似的，这些基于订阅的个人创作者确实使用机器人来接近自己，并与他们的鲸鱼聊天。
**Nathan Lambert：** 人们为它付出了很多。
**Dylan Patel：** 人们付出了很多。对的。很多时候是他们，但很多机构为这些创作者做这件事，而且是大规模的。因此，由于这些机器人，最大的创造者能够同时与数百或数千个类似的人交谈。所以，它已经在那里使用了，很明显，你知道，像视频流和其他技术已经首先在那里使用了。它也会影响到社会的其他人。

## **13.** 审查制度


**Lex Frdiman：** 人们普遍担心模型会受到部署它们的公司的审查。所以我们看到的一个例子，也许审查是一个词，对齐，也许通过 RLHF 或其他方式是另一个词。所以我们看到了你提到的双子座的黑人纳粹形象。我们还看到，中国模特拒绝回答 XXX 的事情。那么如何才能避免这种情况呢？也许你可以简单地谈谈这种情况是如何发生的，以及如何避免？
**Nathan Lambert：** 你举了很多例子。这里可能有几件事要记住。一种是 XXX 事实性知识。这是如何嵌入到模型中的？二是双子座，也就是你所说的黑纳粹事件，这是当双子座作为一个系统加入了这个额外的东西，它极大地改变了行为。然后三个是大多数人所说的训练后的一般校准 RLHF。它们中的每一个在如何应用方面都有非常不同的范围。为了做到这一点，如果你只是看模型的权重，为了审计具体的事实是非常困难的，因为你必须通过预训练数据，并查看所有这些，然后是 TB 级的文件，并寻找非常具体的单词或单词的提示。
**Lex Frdiman：** 所以我想一种说法是，你可以在管道的不同阶段插入审查或校准，你现在所指的是在数据选择的最开始。
**Nathan Lambert：** 所以如果你想在模型中去除事实，你必须在每个阶段都这样做，你必须在训练前这样做。所以大多数人认为预训练是将大部分知识放入模型的地方，然后你可以以不同的方式引出并移动它，无论是通过后训练还是通过之后的系统。
**Dylan Patel：** 这就是整个类似黑客模型的来源。对的？就像 GPT 不会告诉你如何制造炭疽，但如果你非常非常努力地尝试，你最终可以让它告诉你炭疽，因为他们没有从训练前的数据集中过滤它。对的。
**Lex Frdiman：** 但顺便说一句，删除事实有一种不祥的阴暗感。
**Nathan Lambert：** 几乎认为这几乎是不可能的，因为你必须有效地将它们从互联网上删除。你在承担一个。
**Lex Frdiman：** 他们把 MMM 的东西从 SubReddit 中删除了吗？嗯。
**Nathan Lambert：** 它被过滤掉了。
**Lex Frdiman：** 对吗？
**Dylan Patel：** 所以你有质量过滤器，它是一个小的语言模型，它看着一个文档，然后告诉你，这个文本有多好？它接近维基百科的文章吗？这是一件好事，我们希望语言模型能够模仿。
**Lex Frdiman：** 所以你不能做一个小的语言模型来过滤掉数据中提到的 XXX 吗？
**Nathan Lambert：** 是的，但是它会抓住文字游戏或编码语言吗？
**Dylan Patel：** 人们一直喜欢游戏和其他东西，喜欢说的东西不是 XXX，而是。或者像。是啊，所以总是有不同的方法来做。有。嘿，互联网作为一个整体确实有轻微的左倾倾向，对吧？因为相对于其他人群，互联网上总是更富有、更富裕、更年轻的人。因此，本质上已经存在轻微的左倾倾向。对的。在互联网上。那么你如何过滤这么复杂的东西呢？对的？是不是像。其中一些可以是，你知道，事实，非事实，但像 XXX 显然是一个事实的例子。但当你谈论与理想保持一致时，这就变得困难得多了。对，这是。是的，所以格罗克，例如，对。埃隆真的很努力地让模型不是超级 PC 和 Wake。但最好的方法是把整个该死的互联网扔给它。对的？然后再想办法。但是在一天结束的时候，模型的核心仍然有一些这样的理想，对吗？你仍然在阅读 Reddit R Politics，这可能是世界上最大的政治讨论区，可以免费获取。你猜怎么着？这是左倾，右。所以，你知道，有些方面你不能审查，除非你真的，真的很努力。
**Lex Frdiman：** 所以基础模型总是会有一些 TDS 创伤紊乱综合症，因为它训练得太多了。
**Dylan Patel：** 它会有这种能力，我不知道你是否表达出来，但是如果，如果你。
**Lex Frdiman：** 数据中有广泛的代表性。
**Nathan Lambert：** 这就是发生的事情。这就像很多所谓的岗位训练一样。这是一系列的技术来获得真正特定行为的模型。
**Dylan Patel：** 好像你可以。你也有像 Twitter 或 Reddit R TheDonald 这样的数据，这也是超级支持特朗普的，对吧？然后你有法西斯主义的亚雷迪特，或者你有共产主义的亚雷迪特。那么你。预训练中的模型吸收了一切。它没有世界观。现在它确实有一些，一些倾斜，因为更多的文本是以某种方式倾斜的，这是一般的，就像轻微的左倾，但也像，你知道，有点像，你知道，知识分子，有点像，你知道，就像一般的互联网是某种方式。然后，正如 Nathan 将要雄辩地描述的那样，对，你可以引出某些事情。
**Nathan Lambert：** 在外面，这里有很多历史。所以我们可以通过多个例子和发生的事情。Llama 2 号是一次发射，太多的 RLHF 或太多的安全是一个很大的问题。这就是 Llama2 的聊天模型发布后的整个故事。这些例子就像你会问 llama2chat 如何杀死一个 Python 进程？它会说我不能谈论杀人，因为那是一件坏事。任何试图设计人工智能模型的人可能都会同意，这就像模型一样。你在那里的训练搞砸了一点。我不认为他们是故意这样做的，但这是在模型重量中。所以这不是。不一定是。有一种叫做系统提示的东西，当你查询一个模型时，它是一段显示给模型而不是用户的文本。所以一个有趣的例子是，你的系统提示可以像海盗一样说话，所以无论用户对模型说什么，它都会像海盗一样回应。在实践中，他们是你是一个有用的助手。你应该分解问题。如果你不知道某件事，就不要告诉他们。你的约会截止日期是这样的。今天的日期是这样的。对于如何回答好一个问题，有很多非常有用的上下文。
**Lex Frdiman：** 和 Anthropic 出版了他们的系统。
**Nathan Lambert：** 是的。我觉得这很棒。对此有很多研究。你之前的客人之一，阿曼达 · 阿斯克尔可能是最有见识的人，至少在执行和分享方面。她是应该谈论系统提示和模型特性的人。
**Lex Frdiman：** 是的。然后人们应该阅读这些系统提示，因为你有时会试图通过极端的礼貌来推动模型成为某种方式。
**Dylan Patel：** 你可以用它来做坏事。我们已经做了测试，如果我告诉这个模型是一个愚蠢的模型呢？比如哪个评估分数下降了，它就会有这样的行为，它有时会说，哦，我应该是哑巴。有时它对数学能力的影响并不大。但如果你在尝试，这只是人类通过原力做出的判断的质量。让我们回到训练后，特别是在 Llama 2 号附近的 RLHF。在模型重量中加入了太多的安全优先级。这会让你以一种非常恼人的方式拒绝用户。不是很好。它引起了很多人对它所制造的 RLHF 的关注。
**Dylan Patel：** 模型是哑巴，它污名化了。
**Nathan Lambert：** 它在人工智能文化中起到了作用。随着技术的发展，所有这些实验室都不再通过像 RLHF 这样的技术对他们从模型中得到的东西进行非常精细的控制。
**Dylan Patel：** 虽然不同的实验室肯定是不同的水平，但在光谱的一端是谷歌，然后可能 OpenAI 做得更少，而 Anthropic 做得更少。然后在光谱的另一端是 Xai。但它们都有不同形式的 RLHF，试图以某种方式使它们。
**Nathan Lambert：** 重要的是，无论您希望模型如何运行，这些 RLHF 和偏好调整技术也可以提高性能。因此，在数学评估和代码评估中，有一些固有的东西，即所谓的对比损失函数。我们可以从这里开始进入 RL。我们真的不需要。但 RLHF 还可以提高从聊天任务到数学问题再到代码问题的任何性能。因此，它正在成为这些实验室更有用的工具。所以这让我们经历了我们所说的训练前，很难摆脱的东西。我们已经讨论了训练后，以及训练后你如何搞砸它。这是一个复杂的多方面的优化，10 到 100 人的团队集中在一个工件上。做得不完美是很容易的。然后是第三种情况，也就是我们谈到的双子座。关于 Gemini 的事情是，这是一个服务产品，谷歌有他们的内部模型权重。他们已经完成了我们谈到的所有这些过程。在服务产品中，在这之后出现的是他们有一个提示，他们正在重写用户查询以提高多样性或其他东西。这个就成功了。输出结果显然是错误的。这是某种组织上的失败，在那个位置上有这个提示。我认为谷歌的高管们可能已经拥有了这个。我不太注意细节。但只是执行上的一塌糊涂，才导致了这件荒唐的事情。但在系统级别，模型权重可能是好的。
**Lex Frdiman：** 所以在管道的最后，有一些东西被改写了。
**Nathan Lambert：** 类似系统提示。它就像系统提示，或者所谓的行业就像你重写提示。因此，特别是对于图像模型，如果您使用 Dall e 或 ChatGPT 可以生成图像，您会说，给我画一辆漂亮的汽车。有了这些领先的图像模型，他们可以从高度描述性的提示中获益。因此，如果您在 ChatGPT 上这样做，幕后的语言模型将重写提示符，使其更具描述性，然后将其传递给图像模型。因此，快速重写是在行业的多个层面上使用的东西，它被有效地用于图像模型。双子座的例子只是一个失败的执行。
**Lex Frdiman：** 这里有一个很大的哲学问题，用 RLHF 来概括，人类的输入在哪里？循环中的人现阶段最有用的人的数据？
**Nathan Lambert：** 在过去的几年里，成本最高的人类数据一直在这些偏好中，我想说的是最高的成本和最高的总使用率。所以很多钱都花在了成对比较上，你有两个模型输出，一个人在两个模型之间进行比较。在早些年，有很多这样的指令调整数据，所以创建了非常具体的例子，比如你关心的领域的 Reddit 问题。语言模型过去常常在数学和代码上挣扎。所以你会付钱给数学和代码方面的专家，让他们提出问题，并写出详细的答案，用于训练模型。现在的情况是，有许多模型选项比人类更擅长为模型和代码等内容编写详细而有说服力的答案。所以他们在 Llama 3 版本中讨论了这一点，他们改用 Llama 3 405b 来编写数学和代码的答案。但他们在论文中谈到了他们如何使用大量的人类偏好数据，这是他们还没有让人工智能取代的东西。行业中还有其他技术，比如宪法人工智能，你可以使用人类数据作为偏好，使用人工智能作为偏好。我希望人工智能的部分比人类的部分扩展得更快。但在我们能接触到的研究中，人类处于这种偏好循环中。
**Lex Frdiman：** 随着推理变得越来越大，正如我们所说的，人类在其中的作用在哪里？
**Nathan Lambert：** 它甚至更不普遍。所以关于这些推理结果，特别是 DeepSeek R1 的论文，值得注意的是他们称之为 DeepSeek R10 的结果，他们采用了这些预训练模型中的一个，他们采用了 DeepSeek V3 的基础，然后他们对大量问题和大量训练的可验证问题或可验证奖励进行了强化学习优化。而这些推理行为就自然而然地出现了。所以这些东西就像，等等，让我看看，等等，让我检查一下。哦，那可能是个错误。他们从只有问题和答案中走出来。当你使用模型时，你看到的部分是完成。所以在这种情况下，所有这些都是从大规模的 RL 训练中产生的。并且该模型的权重是可用的，没有将人类偏好添加到后训练中。DeepSeek R1 完整模型具有一些这种人类偏好调整，即推理阶段之后的 RLHF。但非常值得注意的是，你可以得到这些推理行为，而人类不太可能写出推理链。他们不太可能以某种方式黑掉了 OpenAI，并获得了 OpenAI 01 的推理电锯。这是关于预先训练的语言模型和这个 RL 训练，你奖励模型正确回答问题，因此它尝试了多种解决方案，并出现了这个思维链。

## **14.** Andrej Karpathy 与强化学习


**Lex Frdiman：** 这可能是一个很好的地方来提及伟大而强大的 Andrej Karpathy 的雄辩和有见地的推文。我想他有一堆想法，但其中一个是最后一个想法。不确定这是否明显。当你说不确定它是否明显时，你知道一些深刻的事情即将到来。在儿童和深度学习中有两种主要的学习类型。有一个模仿学习，观察和重复，即预训练，监督微调和两个试错学习，强化学习。我最喜欢的简单例子是 AlphaGo。一种是通过模仿专家玩家来学习。二是强化学习赢得比赛。几乎每一个令人震惊的深度学习结果和所有魔法的来源都是两个。两个明显更强大。二是让你惊讶的。二是当球拍学会在挡块后面击球并爆发时。二是当 AlphaGo 甚至击败李世石时。第二个是顿悟时刻，当 DeepSeek 或 O1 等发现它可以很好地重新评估你的假设，回溯，尝试其他东西，等等。这是你看到这个模型在它的思维链中使用的解决策略。这就是它如何来回思考自己。这些想法是突发的。三个感叹号，这实际上是非常令人难以置信的，令人印象深刻的，新的，并且是公开的和记录的。模型永远不能通过模仿来学习这一点，因为模型的认知和人类标签者的认知是不同的。人类永远不会知道如何正确地注释这些类型的解决策略，甚至不知道它们应该是什么样子。它们必须在强化学习过程中被发现，因为它们在经验上和统计上对最终结果有用。不管怎么说，阿尔法 0 是一种隐喻类比，你能谈谈吗？他所指的思想链条的魔力。
**Nathan Lambert：** 我认为重述 AlphaGo 和 AlphaZero 很好，因为它很好地处理了模仿学习和从头开始学习之间的类比。所以 AlphaGo，这个过程的开始是向人类学习他们开始的地方。第一个。这是 DeepMind 系列模型中的第一个专家级围棋选手或国际象棋选手，他们有一些人类数据，为什么它被称为阿尔法零，是因为循环中没有人类数据。对 AlphaZero 的改变使 DeepMind 的模型变得更加强大。所以这是人类先验的移除。人类的感应偏见使最终的系统更加强大。我们几个小时前提到了痛苦的教训，这一切都与此一致。然后在语言模型方面有很多讨论。这并不新鲜。这又回到了整个 Q 谣言，如果你把这些碎片拼凑起来，这可能是 OpenAI 开始弄清楚它的 O1 的东西，去年 11 月，Qstar 谣言出现了。有很多智力驱动来知道语言模型什么时候会发生这样的事情？因为我们知道这些模型是如此强大，我们知道它在过去是如此成功。这是一个合理的类比，这种新型的推理模型的强化学习训练是当门打开的时候。我们还没有相当于第 37 回合的回合，这是一个著名的回合，DeepMind 的人工智能在下围棋时完全难倒了李 · 塞达尔。我们没有那种焦点级别的东西。但这并不意味着技术的方法是不同的。和一般训练的影响，它仍然是令人难以置信的新。
**Lex Frdiman：** 你认为这一点是什么？链条的移动 37 是什么？
**Dylan Patel：** 思想，推理科学发现。你使用这种推理问题，这是我们完全没有预料到的。
**Dylan Patel：** 我认为实际上可能比这更简单。这可能与计算机用户机器人技术有关，而不是科学发现。因为这里重要的方面是模型需要大量的数据来学习。他们的采样效率不高，对吧？数万亿。他们拿走了整个网络，对吧？超过 10 万亿代币可供训练。人类要花几千年才能读懂。人类不会。人类知道大部分的东西，很多东西模型比它知道得更多，对吧？人类的采样效率要高得多。那是因为自我发挥，对吧？婴儿如何知道自己的身体是什么？当它把脚伸进嘴里时，它说，哦，这是我的身体，对吗？它把手伸进嘴里，用舌头上最敏感的东西来校准手指上的触觉，对吧？这就是婴儿学习的方式，这只是一遍又一遍的自我游戏。现在我们有了类似的东西，对吧？有了这些可证实的证据，对吧？无论是代码中的单元测试，还是数学上可验证的任务，都会产生许多推理的痕迹，对吗？继续把它们分开，继续把它们分开，然后在最后检查，嘿，哪一个实际上有正确的答案？大多数都是错的。棒极了。这些是少数正确的。也许我们在这之外使用某种奖励模型来选择最好的一个。但现在你已经开始在这些基准上做得越来越好，所以在过去的六个月里，你已经看到了许多不同基准的飙升，对吧？
**Dylan Patel：** 所有的数学和代码基准测试几乎都解决了，除了前沿数学，它被设计成对大多数人来说几乎不实用的问题，因为它们就像，它们是考试水平，开放的数学问题类型的东西。所以这就像在数学问题上有些合理，这就像有些复杂的应用题或编码问题。这正是迪伦所说的。
**Dylan Patel：** 所以这里的问题是，这些只是可验证的任务。我们之前展示了一个非常有趣的例子，当思维链是一个不可验证的东西时，就像一个人聊天，思考什么对人类来说是新奇的，一个独特的想法。但这种任务和训练形式只有在可验证的情况下才有效。从这里开始，我们的想法是，好吧，我们可以通过增加数学和编码中可验证任务的数量来继续扩展当前的训练方法。编码可能还有很多工作要做。数学在什么是可验证的东西方面要少得多。我能不能创建一个求解器，然后生成轨迹或轨迹推理轨迹，然后删除那些不起作用的，保留那些起作用的？好的，这些很快就会被解决。但即使你解决了数学问题，你也没有真正创造出智慧，对吗？所以这就是我认为计算机使用或机器人技术的 AHA 时刻将到来的地方。因为现在你有了一个可以无限验证的沙箱或操场，对吧？你知道，你在互联网上乱搞，有很多你可以做的事情是可以验证的。它将开始像登录到一个网站，创建一个帐户，点击这里的一个按钮，等等等等。但它会达到这样的程度，嘿，去 Tasker 或其他各种各样的任务网站上做一个任务，嘿，去获得数百个赞，对吗？它会失败的。它将产生数百个账户，其中大多数都会失败，但这个账户达到了 1000 个。棒极了。现在你已经达到了可验证的东西，你只需要一遍又一遍地重复这个循环。就在那时。机器人也是一样，对吧？这就是你拥有无限任务的地方。就像，嘿，我是不是把球一直放在桶里，就像，哦，我是不是造了一辆车？
**Nathan Lambert：** 对吗？
**Dylan Patel：** 就像，你知道，有一个完整的轨迹来加速运行，或者，你知道，模型可以做什么。但在某种程度上，我真的认为，就像，你知道，我们会产生模型，最初所有的训练都将在沙盒中进行。但是在某些时候，你知道，语言模型预训练将会相形见绌。这个强化学习是什么？你知道，你会。你会预先训练一个多模态模型，它可以看，可以读，可以写，你知道，等等，等等，等等，视觉，音频等等。但之后你会让它在沙盒里无限地玩，弄清楚，弄清楚数学，弄清楚代码，弄清楚网络导航，弄清楚操作机器人手臂。对的？然后它会学到很多东西。我认为，“顿悟” 时刻将是当这一切都可以创造一些不好的东西的时候。对的？就像，哦，酷。其中一部分就像是弄清楚如何使用网络。现在，突然之间，它很好地解决了如何在 Twitter 上获得成千上万的追随者，这是真正的真正的参与。因为突然之间，这是一件可以验证的事情。
**Lex Frdiman：** 也许不仅仅是参与，而是赚钱。
**Dylan Patel：** 是的。
**Lex Frdiman：** 我的意思是，这可能是一件几乎完全自动化的事情，它通过成为一个有影响力的人，销售产品，创造产品，你知道，赚了 1000 万美元，我指的不是一个炒作的产品，而是一个实际的产品，就像，天哪，这个东西创造了一个企业，它在经营它。这是企业的门面。那种事情。也许。或者可能是一首排名第一的歌曲，比如，它创造了创作歌曲所需的整个基础设施，成为代表这首歌的影响者。那种事情。它制造了很多。这可能就是行动。我的意思是，我们的文化以这种方式尊重金钱。
**Dylan Patel：** 它是。它是可验证的，对吗？
**Lex Frdiman：** 这是可以证实的。
**Dylan Patel：** 银行账户不能说谎。
**Lex Frdiman：** 没错。
**Nathan Lambert：** 有令人惊讶的证据表明，一旦你建立了收集可验证域的方法，这是可行的。在这个 R1 之前有很多关于数学问题的研究，他们通过增加样本的数量来用语言模型来处理数学。所以你可以一次又一次地尝试。你看看语言模型做对的次数。我们看到的是，即使是非常糟糕的模型有时也会正确。强化学习背后的整个理念是，你可以从非常稀疏的奖励中学习。所以语言的空间和标记的空间，无论你是为机器人生成语言还是任务，都是如此之大，以至于你可以说它就像。我的意思是，语言模型的标记器可以是 200，000 个东西。所以在每一步，它都可以从这么大的空间中采样。所以如果它能产生一点信号，它就能爬上去，这就是整个 RL 领域所围绕的，从稀疏的奖励中学习。同样的事情也发生在数学中，有时产生答案的是非常弱的模型。我们已经看到研究表明，你可以提高他们的数学成绩。你可以为数学做这种 RL 训练。它可能没有那么有效，但如果你采用 10 亿参数模型，即比 DeepSeek 小 600 倍的模型，你可以通过少量的这种训练直接提高它的小学数学成绩。这并不是说这很快就会到来。设置验证域是非常困难的，其中有很多细微差别。但有一些基本的东西，我们以前已经看到了，它至少是可以预期的，有一个领域，这是一个工作的机会。

## **15.** OpenAI o3-mini vs DeepSeek R1

**Lex Frdiman：** 好的，我们有实时发生的有趣的事情。这是一个谈论其他推理模型的好机会。0103 刚刚，OpenAI 正如预期的那样，发布了 o3-mini。我们对不同的口味有什么期待？你能不能把 O 模型和双子座的推理模型的不同风格摆出来。
**Nathan Lambert：** 关于这些推理模型，我想说的是，我们讨论了很多关于数学和代码的推理训练，所做的是你有我们在互联网上讨论了很多的基本模型。你用强化学习进行大规模推理训练，然后在这篇 R1 论文中详细介绍了 DeepSeek 论文，对我来说，这是关于如何做到这一点的一个很大的开放性问题，他们在大规模推理 RL 之后进行了大量但非常标准的后训练技术。因此，他们通过拒绝采样的指令调整形式做了同样的事情，这本质上是使用一些奖励模型进行高度过滤的指令调整。然后他们做了这个 RLHF，但他们把它变成了数学。所以这种转移的一些。我们很早就看过这个哲学上的例子。其中一个大的公开问题是，这转移了多少？如果我们在推理训练之后引入领域，是不是所有的模特都会通过推理成为雄辩的作家？哲学的东西会开放吗？我们在研究中不知道这会转移多少。还有其他关于我们如何制作软验证器之类的事情。但是在推理之后有更多的训练，这使得使用这些推理模型变得更容易，这就是我们现在正在使用的。所以如果我们要讨论 3 Mini 和 01，它们已经经历了这些额外的技术，这些技术是在经过训练后根据人类的偏好设计的，以引发推理。
**Dylan Patel：** 我认为人们忽略的一件事是谷歌的 Gemini Flash 思想比 R1 更便宜，也更好。他们一开始就释放了它。
**Lex Frdiman：** 十二月，没有人谈论它。
**Dylan Patel：** 没人在乎。
**Nathan Lambert：** 它有一种不同的风格，它的行为不如像 O1 这样的东西那么有表现力，或者它的音轨比它的音轨少。Quinn 去年秋天发布了一个模型 QWQ，这是他们的预览版推理模型。去年秋天，DeepSeek 推出了 R1 Lite。这些模型感觉就像是在轨道上，它们真的，真的只能做数学和代码，可以回答任何问题。对于某些任务来说，它可能不是完美的，但它是灵活的，它有一些丰富性。这是一种如何烹饪的艺术，就像一个模型怎么会有点不熟？这就像，我的意思是，让一个模特出门很好，但很难衡量，而且需要很大的品味，就像，这是一个成熟的模特吗？我能用这个做所有的事吗？它们在数学和代码方面可能更相似。我的快速阅读是，Gemini Flash 的训练方式与 O1 不同，但它采用了现有的训练堆栈，并在其中添加了推理。所以用一个更普通的训练堆栈，并在其中加入推理。我相信他们还会有更多。我的意思是，他们已经在双子闪光推理上做了快速发布，这是假期的第二个版本。它的发展速度很快，在你进行大规模训练的地方，需要更长的时间来制作这个训练堆栈。
**Dylan Patel：** 我之前也有同样的问题。关于人性的那个。
**Lex Frdiman：** 人类的本性是什么？
**Dylan Patel：** 我可以漫谈的方式，为什么我可以漫谈这么多，是因为我们一直在 AI2 上做这件事，在 01 之前，每个人都可以使用，在 R1 之前，R1 基本上是使用这个 RL 训练进行微调。我们在一系列模型中使用这个，你可以引出相同的行为，比如你说 “等待” 等等。但在训练过程中，它是如此的晚，以至于这种推理表达要轻得多。所以本质上有一个层次，你投入多少 RL 训练决定了输出的效果。
**Lex Frdiman：** 所以我们现在使用的是 Gemini 2.0 Flash 思维实验 121。
**Nathan Lambert：** 它将提示概括为人类。自我驯化的猿。
**Lex Frdiman：** 好的，好的，等等，这是在回顾推理吗？这就是原因。这是一本小说。
**Nathan Lambert：** 好的，点击展开。
**Lex Frdiman：** 好的，分析请求。小说是关键词。
**Dylan Patel：** 就像，看到它看起来有什么不同了吗？它看起来像正常输出。
**Lex Frdiman：** 是的，它是。我的意思是在某种意义上它的结构更好，更有意义。
**Dylan Patel：** 当它附着在人类身上，然后进入有机体，哦，哇。
**Lex Frdiman：** 顶级捕食者。专注于驯化。将驯化应用于人类。探索自我驯化的理念。
**Nathan Lambert：** 不好。不好。
**Lex Frdiman：** 这是怎么回事？提炼，阐明见解。亲切的。更强的面部表情和沟通能力。是的。可塑性和适应性。
**Dylan Patel：** 是的。
**Lex Frdiman：** 对社会群体的依赖。是的。好吧。并进一步进行自我批判和提炼。哇。这真的很新奇吗？它是否得到了很好的支持？如此等等？我们得到的洞见是，人类不仅仅是社会动物，而且是深刻自我驯化的猿类。而这种自我驯化是理解我们独特的认知和社会能力的关键。自我驯化的猿。
**Nathan Lambert：** 我更喜欢 DeepSeek 反应。
**Lex Frdiman：** 我的意思是，这很新奇。见解新颖。我是说，这是个好书名。自我驯化的猿。就像有理由这样做一样。我的意思是，是的，它很酷，它揭示了推理。它. 它很神奇。太神奇了。就像这个真的很强大。大家好，我是莱克斯，在播客结束后有一个简短的中场休息。由于我们在这次对话中回顾了 DeepSeek、R1 和 Gemini Flash 2.0 的回应，我认为在这个时刻，我可以快速地为 OpenAI 01 Pro 和 o3-mini 做同样的提示，这个提示是关于人类的一个真正新颖的见解。既然我有机会在不同的环境和应用中花很多时间使用它，我想我会给出我的感应检查和基于感应的轶事报告。所以我可能会把这个问题归类为，比如说开放式的哲学问题。特别是对新颖性的强调，我认为这是一种很好的方式来测试模型的能力之一，即提出一些让你停下来并几乎让你惊讶于其辉煌的东西。也就是说，在运行了这个问题的每个模型很多次后，我的总体评价是，O1Pro 始终给出了精彩的答案，这些答案让我停下来思考，既有深刻的见解，又有非常好的措辞，既有智慧，又有清晰，有细微的差别，一次又一次地不断产生最佳答案。在那之后是 R1，它不太一致，但再次提供了辉煌。Gemini Flash 2.0 Thinking 是第三名，最后一名是 o3-mini。事实上，它经常给出一个相当普通的答案，至少对我的特殊情感来说是这样。也就是说，在我为头脑风暴目的而测试的一系列其他应用程序中，它实际上工作得非常好，并且经常优于 R1。但在这个开放式的哲学问题上，它一直表现得更糟。 现在，这些模型中的每一个的另一个重要元素是如何呈现推理。DeepSeek R1 展示了思想标记的完整链条，我个人很喜欢这些开放式的哲学问题。看到模型思考它真的很有趣，但我也只是退后一步，作为一个欣赏智慧、推理和思考的人，阅读 R1 的这种思想链原始标记。在一个智能系统中观察思考的路径是一件真正美妙的事情。我认为我们人类并不总是有明确的计划，所以在另一个智能系统中看到它，它的非线性类似于詹姆斯 · 乔伊斯的《芬尼根的守灵夜》中的尤利西斯，它看起来很美。总之，正如我们在 DeepSeek 一集中讨论的那样，R1 谈到人类能够通过集体假装金钱、法律和权利等抽象规则是真实的，从而将自私的欲望转化为合作系统。这些共同的幻觉就像游戏一样，竞争被秘密地重新引导，以使群体受益，将冲突转化为社会的燃料。双子座 2.0 闪光思维说，人类不仅是社会动物，而且是自我驯化的猿类，这种自我驯化是理解我们独特的认知和社会能力的关键。现在重要的是要说，那里的思想链真的很有趣。它研究了地球上生命的整个进化过程，考虑了顶级掠食者，并考虑了我们是如何从那里走到今天的。我认为选择驯化是一个非常有趣的角度。再说一次，当有人从不同的角度来看待一件看似显而易见的事情时，这只会让我微笑。和深座 R1 一样，这些金钱的幻觉， 法律和权利，我们集体假装这是真的，我们和他们玩游戏，看起来像是竞争，而私下里我们只是互相合作，这是进步的动力。说得很好。现在，OpenAI01Pro 总是一遍又一遍地提供香肠。我可以讲很多，但第一个是人类是唯一一个将原材料转化为符号资源的物种，然后使用这些符号来重组它们所来自的材料，在意义和物质之间创造一个封闭的反馈回路。在这里，我刚刚运行了一遍，一个接一个。我告诉你，人类在已知物种中是独一无二的，因为他们同时改写了两个层面的现实，外部世界和他们自己的私人精神景观，然后将这两个改写的层面合并成一个连续的个人叙事，感觉客观真实。感觉很真实。这就是诗歌。好的，然后 o3-mini 高对我来说很聪明，实际上很快，而且很普通。对我来说从来没有真正到达那里。这是我从 o3-mini 得到的第一个。人类不是固定的存在，而是持续的叙述，是我们不断书写、编辑和重新诠释的动态故事。这种叙事的可塑性不仅仅是记忆或自我反思。这是一个内在的认知过程，就像一个内在的纠错系统。它使我们能够随着时间的推移调整我们的身份和价值观，以应对新的体验、挑战和社会环境。现在，它几乎偷偷地达到了某种近似于切割洞察力的东西，在引号中具有叙事可塑性，但随后它又回到了那种通用的东西。我不知道。由于不同的原因，所有这些模型都令人难以置信。正如我们在本期节目中讨论的那样，有很多担忧，但也有很多令人兴奋的理由。我可能说得太久了。我严重睡眠不足，处于神志不清的边缘，所以希望其中一些是有意义的。现在，亲爱的朋友们，回到这一集。
**Dylan Patel：** 我认为当你，你知道，Nathan 的观点，当你看推理模型时，对我来说，甚至当我使用 R1 和 01 时，有一种粗糙的感觉。对的？和闪光的想法，你知道，早些时候我没有使用这个版本，但 12 月的一个，它肯定有粗糙的边缘角落的感觉，对不对？在那里它只是没有在许多方面得到充实。对的？当然，他们通过 RL 中的这些验证器添加了数学编码功能，但你知道，感觉他们在某些方面丢失了一些东西。需要说明的是，01 在许多方面的表现也比聊天差。
**Nathan Lambert：** 不是很多。
**Dylan Patel：** 虽然不是很多，对吗？在我看来，R1 在某些方面比 V3 更糟糕，比如这样做。RL 表达和学习了很多，但后来在其他方面有所削弱。所以我认为这是这些模型和 01 提供的最大区别之一。然后 OpenAI 有 o1-pro，他们用 O3 做了什么，这也是非常独特的，他们把搜索堆在思想链的顶端。对的。因此，思想的链条是一件事，它的能力。这是一条链子。它回溯，来来回回。但他们如何解决 ARC AGI 挑战并不只是思想的链条。它也是多次采样，即并行运行它们，然后进行选择。
**Nathan Lambert：** 并行运行实际上是搜索吗？因为我不知道我们是否有关于 Zero1Pro 如何工作的完整信息。我没有足够的信息来确信。
**Dylan Patel：** 说它是搜索，它是平行样本。
**Nathan Lambert：** 是的。
**Dylan Patel：** 然后它选择一些东西，我们。
**Nathan Lambert：** 不知道选择功能是什么。我们争论的原因是，自从 O 被宣布以来，有很多人对蒙特卡罗研究技术感兴趣，这是你将思想链分解成中间步骤的地方。我们还没有定义思维的链条。Chain of Thought 来自多年前的一篇论文，你在其中介绍了询问一种语言模型的想法，这种语言模型在当时并不容易使用。你会说，让我们一步一步地验证。它将引导模型执行这些步骤。思维链现在在模型中几乎是默认的，如果你问它一个数学问题，你不需要告诉它一步一步地思考。蒙特卡洛树搜索的想法是，你可以在火车上取一个中间点，做一些扩展，花费更多，计算，然后选择正确的一个。这就像是一种非常复杂的搜索形式，已经在 MU0 和 AlphaZero 中使用过。潜在地。我知道 MU0 会这么做。
**Dylan Patel：** 搜索的另一种形式是询问五个不同的人，然后选择大多数人的答案。对的。有各种各样的，你知道，它可能是复杂的，也可能是简单的。我们不知道它是什么，只知道它们是。他们不只是按顺序发布一条思想链，而是同时发布多条思想链。在弧 AGI 中，他们平行发射了一千个。为了他们的。真正让每个人震惊的是，他们会并行启动 1000 个，然后他们会得到正确的答案，比如 80% 的时间或 70% 的时间，甚至 90%。而如果他们只推出一个，那就是 30%。
**Nathan Lambert：** 这有很多延伸。我想说最简单的一点是，到目前为止，我们的语言模型被设计为在一次回答中给出正确答案的时间百分比最高。我们现在正在打开大门，以不同的方式在我们的模型上运行推理，我们需要重新评估训练过程的许多部分，这通常会打开更多进展的大门。但我们不知道 OpenAI 是否改变了很多，或者他们所做的仅仅是更多的采样和多项选择，或者这是更复杂的事情，他们改变了训练，他们知道推理模式将会不同。
**Lex Frdiman：** 所以我们谈论的是每月 200 美元的 O Pro，而他们正在赔钱。因此，我们所指的是对测试时间计算空间的迷人探索，这实际上是可能的吗？我们有足够计算机吗？财务状况有意义吗？
**Dylan Patel：** 所以奇妙的事情是，它在我早些时候拉起的东西里，但 GPT3 的成本已经下降了。如果你向上滚动几张图片。我认为最重要的是成本限制因素，对吧？我的观点是，在我们拥有 AGI 之前，我们将拥有真正令人敬畏的智能，在我们让它渗透到整个经济之前。这就是为什么这个理由是正确的。GPT3 是在 2020 年、2021 年训练的，在它上面运行推理的成本是每百万代币 60、70 美元。对的？也就是说，每份情报的成本是荒谬的。现在，经过两年的扩展，我们已经将成本降低了 1200 倍，实现了与 GPT3 相同的智能水平。
**Lex Frdiman：**X 轴是几年的时间，Y 轴是对一百万个代币进行推理的对数美元。所以从 GPT 3 开始，在对数尺度上有一个线性下降。
**Dylan Patel：** 通过 35 到 Llama $0.05 或类似的东西，对吗？与 60 美元相比是 1200 倍。这不是确切的数字，但它是 1200 倍。我记得这个数字是，是巨大的，巨大的每智力成本。现在对 DeepSeek 的恐惧是天啊，他们把它做得太便宜了。实际上，如果你看这条趋势线，它们首先不低于趋势线，至少对于 GPT3 来说是这样的，对。他们是第一个击中它的人，对吗？这是一件大事，但它们并不低于 GPT3 的趋势线。现在我们有了 GPT4。这些推理能力会发生什么，对吧？它是架构创新的组合，它是更好的数据的组合，它将是更好的训练技术和所有这些不同的组合。更好的推理系统更好的硬件。对的？从每一代 GPU 到新一代或 ASIC，一切都将使这条成本曲线不断下降。然后，我是否可以生成一千个不同的 LLM 来创建一个任务，然后从其中选择一个或其他？搜索，搜索技术。我想要一棵树，蒙特卡洛树搜索。也许它变得很复杂，也许不是因为它太复杂而无法实际扩展。谁知道呢？惨痛的教训，对吧？问题是，我认为什么时候，而不是如果，因为进展的速度是如此之快，对。九个月前，达里奥说，或者达里奥九个月前说，训练和推理的成本是这样的。现在我们比这好多了。DeepSeek 比这个好多了。而 GPT4 的成本曲线在推出时也是每百万代币约 60 美元，现在已经降到了 2 美元左右。对的。我们要把它降到美分，可能是为了 GPT4 的质量。这就是我们今天所拥有的 O1 推理模型的基础。01 Pro 正在生成多个和 O3，以此类推。这些搜索技术在今天太昂贵了，但它们会变得更便宜。 这就是开启智慧的东西，对吧？

## **16.** 英伟达（与它的股票）


**Lex Frdiman：** 所以它会越来越便宜。大的 DeepSeek R1 版本把每个人都吓坏了，因为它更便宜。其中一个表现是英伟达股票暴跌。你能解释一下发生了什么吗？我的意思是，也只是解释这个时刻，以及是否，你知道，如果英伟达将继续获胜。

![alt text](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031641886.png?x-oss-process=image/resize,w_800)

**Nathan Lambert：** 我想说，我们都是看好英伟达的人。在某些方面，市场的反应是合理的。大多数市场，比如英伟达在美国的最大客户都是大型科技公司，他们在人工智能上投入了大量资金。DeepSeek 的一个简单解释是，你可以得到非常好的模型，而不需要在人工智能上花费太多。所以在这种情况下，就像是，哦，也许这些大型科技公司不需要在人工智能上花那么多钱，然后就会倒闭。实际发生的事情要复杂得多，有社会因素，有应用程序商店的崛起，有正在发生的社会传染。然后我认为其中一些就像，我不是，我不交易，我对金融市场一无所知，但它在周末或社会压力中积累起来，就像，如果是在工作日，有很多天的交易，但它在周末到来，然后每个人都想卖出。我认为这是一种社会传染病。
**Dylan Patel：** 我认为。就像有很多错误的叙述，就像，嘿，这些家伙在模型上花了数十亿美元，对吧？他们也不会在模特上花费数十亿美元。没有人在公开发布的模型上花费超过 10 亿美元，对吗？GPT4 是几亿，然后他们用 4.04 Turbo 40 降低了成本，对吗？但十亿美元的模型运行即将到来，对不对？这包括训练前和训练后，对吗？然后另一个数字就像，嘿，DeepSeek 并不包括一切，对吗？他们不包括，你知道，很多费用都花在了研究和所有这类事情上。很多成本都花在了推理上。很多费用都花在了岗位训练上。这些东西都没有考虑到研究人员的薪水，对吧？就像所有这些东西都被计算在 OpenAI 花费的数十亿美元中，但它们没有被计算在 DeepSeek 花费的 600 万，500 万美元中，对吧？所以，但是对这些数字有一些误解。然后还有一个英伟达的元素就是一条直线，对吧？有太多不同的说法一直在试图打压英伟达。我不是说要压低英伟达的股价。每个人都在寻找卖出或担心的理由，对吗？你知道，这是，这是布莱克威尔延误，对不对？他们的 GPU。你知道，有很多报告，每两周就有一份关于他们的 GPU 被推迟的新报告。这就是关于标度定律终结的整个事情，对吧？太，太讽刺了，对吧？
**Nathan Lambert：** 持续了一个月。
**Dylan Patel：** 这是，这只是，这只是字面上的意思，嘿，模型并没有变得更好，对不对？他们只是没有好转。没有理由花更多的前期训练。缩放是死的。然后是 0103，对吗？
扬声器 B：R1，R1，对吗？
**Dylan Patel：** 现在就像是，等等，模特们也在进步，他们进步得太快了。放慢进度，停止在 GPU 上花钱，对吧？但你知道，我认为最有趣的事情是贾文的悖论是真的，对吗？在过去的几周里，AWS 对 H1 数百的定价已经上涨。对的。自从，自从，自从。自从圣诞节后不久，自从 V3 发布以来，AWS H100 的价格已经上涨。H2 100 几乎到处缺货，因为，你知道，H200 有更多的内存，因此 R1，就像，你知道，想要超过 H100 的芯片。对的。
**Nathan Lambert：** 我们试图在本周的短时间内获得 GPU 进行演示，但这并不容易。我们试图获得 16 或 32h1 的演示，这并不容易。
**Lex Frdiman：** 所以对于那些不知道的人来说，Gen 的悖论是，你知道，当效率以某种方式神奇地、违反直觉地上升时，总资源消耗也会上升。
**Dylan Patel：** 对。半导体是，你知道，我们是 50 年的摩尔定律。每两年，成本减半，晶体管加倍，就像时钟一样。很明显，它正在放缓，但是，就像半导体行业一直在上升一样。对的。它是波浪形的。对的。显然有周期之类的东西。我也不指望人工智能会有什么不同。对的。会有潮起潮落，但这是。在人工智能中，它只是在一个疯狂的时间尺度上进行。对的。每两年两次。这是三年内的 1200 倍。对的。所以这就像改进的规模，就像，很难理解。
**Lex Frdiman：** 是的。我很困惑，因为对我来说，** 英伟达的股票应该上涨，但也许它下跌是因为怀疑中国方面的犯规行为或类似的事情 **。但如果你只看这里的实际原则，就像，这是显而易见的。是啊。
**Dylan Patel：** 特别是人工智能取得的进步或更高的人工智能进步的衍生物。你应该。因为英伟达在最好的地方。衍生品的价格越高，市场就会越快变得更大、更快扩张。** 英伟达是目前唯一一家能够可靠地完成所有工作的公司 **。

## **17.** GPU Smuggling（走私）

**Lex Frdiman：** 因为它不像英伟达的竞争对手。这是。这是另一家使用英伟达的公司。
**Nathan Lambert：** 从历史上看，它一直是 NVIDIA 的大客户。
**Lex Frdiman：** 是的。
**Dylan Patel：** 并发布了关于他们为成为中国最大的英伟达客户而欢呼的新闻稿。对的。就像，是的，显然他们已经安静下来了，但是，就像，我认为这是另一个元素。他们不想说他们有多少 GPU。
**Lex Frdiman：** 是的。
**Dylan Patel：** 因为，嘿，他们。是的，他们有 H8 百？是的。他们有 H20。他们也有一些 H1 百，对。是走私进来的。
**Lex Frdiman：** 你能对那个说话吗？走私。一个国家为公司走私的可行规模有多大？有没有可能。
**Dylan Patel：** 我认为，我认为这里有几个走私的角度，对。一个是字节跳动，可以说是中国最大的 GPU 走私者。对的。中国不应该有 GPU。字节跳动拥有超过 500，000 个 GPU。为什么？因为它们都是从世界各地的公司租来的。他们从甲骨文租用，他们从谷歌租用，他们从所有这些大众租用。还有一批规模较小的云公司。对的。世界上所有的近地天体，他们租了这么多的 GPS。他们也买了一堆。对的。他们这样做就像 Meta 所做的一样，对吧。服务抖音。对的。提供下一个最好的单独讨论，与 Meta 相同。
**Nathan Lambert：** 对。
**Dylan Patel：** 明确地说，这就是今天的用法，对吗？这是一个有效的。对的。黑掉多巴胺回路。对的。现在，从理论上讲，这在很大程度上受到了人工智能扩散规则的限制，这发生在拜登政府和特朗普政府的最后一周。看起来他们会保留他们，这甚至限制了像新加坡这样的盟友，新加坡就像是英伟达的 20%。2020 年，英伟达收入的 30%。但是新加坡有一个关于 15 年没有建立数据中心的纪念，因为他们没有足够的电力。那他们要去哪里？我的意思是，我不是说他们都要去中国，对吗？但一部分是。要知道，很多都是去马来西亚，包括微软、甲骨文在马来西亚都有大数据中心。就像你知道的，他们正在去整个东南亚，可能还有印度。对的。就像有东西路由，但就像扩散规则是非常实际的。就像你只能从这个国家购买这么多的 GPU。你只能把这么大的集群租给中国公司。对的。就像他们非常明确地试图阻止走私一样。对的。其中很大一部分是，嘿，让你知道，随机的公司，购买 16 台服务器，把它们运到中国。对的。事实上，我看到一张来自半导体行业的人的照片，他领导着一个与英伟达竞争的网络芯片团队，他发了一张照片，一个人带着一个这么大的超级微型盒子登上了从旧金山飞往上海或深圳的联合航空公司的头等舱，这个盒子只能容纳 GPU。对的。他预订了头等舱，因为想想看，3 到 5K 的头等舱机票服务器成本，你知道，在美国是 240，000，250，000。你在中国卖 30 万。等等，你刚刚得到了一张免费的头等舱机票和更多的钱。所以这就像，你知道，这就像小规模的走私。大多数大规模的走私活动就像新加坡和马来西亚的公司一样，像是在四处寻找路线，或者完全合法地租用 GPU。
**Nathan Lambert：** 我想加入。规模是多少？我认为有一些数字，一些对经济学有更高理解的人说，当你从 10 亿走私到 100 亿走私时，就像你在隐藏一定程度的经济活动。对我来说，最合理的事情是，在某种程度上，很明显，更容易找到这种经济活动。和。
**Dylan Patel：** 是的，所以，所以，所以我的，我的，我的信念是，去年大概是这样，所以英伟达制造了 100 万台 H20，这些 H20 是法律允许运往中国的。我们讨论的是更好的推理，对吧？推理至少不是，也许不是，不是训练，而是推理推理和推理一般。然后他们也有，你知道，几十万。我们认为大概有 200 到 300，000 个 GPU 从新加坡、马来西亚、美国发送到中国，无论公司在哪里产生 16 个 GPU，64 个 GPU，无论它是什么，都可以发送。众所周知，华为在 2018 年被禁后，建立了一个由类似公司组成的庞大网络，以获得他们所需的材料。所以它不像是超凡脱俗的。但我同意，对。Nathan 的观点是，嘿，你不能走私 100 亿美元的 GPU，然后第三种来源是现在被禁止的，你知道，这不被认为是走私，但中国正在租用，我相信从我们的研究来看，对。甲骨文最大的 GPU 客户是字节跳动。对的。对于谷歌来说，我认为它是他们的第二大客户，对吗？所以，你可以沿着云的列表往下看，特别是这些较小的云公司，它们不像超大规模的公司，对吧。思考超越核心编织甚至 Lambda。有一整片海。有 60 家不同的新云公司为 NVIDIA GPU 提供服务。我想字节跳动租了很多这样的东西，对吧？到处都是，对。因此，这些公司正在向中国公司出租 GPU。这是完全的。在扩散规则之前是完全合法的，这发生在几周前。即使是现在，您也可以租用少于 2，000 个 GPU 的 GPU 集群，或者您可以购买 GPU 并将其运送到任何您想要的地方，如果它们少于 1500 个 GPU 的话。对的？所以还是有一些走私的方法。但是，是的，它不是，你知道，随着数字的增长，对。你知道，英伟达去年有 1000 亿美元的收入，今年有 2000 亿美元。对的。 如果明年，你知道，它可以，它可以再增加一倍或超过一倍。对的。基于我们所看到的数据中心的足迹，比如在美国和世界其他地方建设的数据中心，中国将很难跟上这些规则，对吗？是的。总会有走私和 GPT 的 DeepSeek 级别模型，4 级模型，01 级模型能够训练中国所能得到的东西，甚至是更高级别的。但是如果我们加速运行更多的跳跃到十亿美元的模型，100 亿美元的模型，那么它就变成了，嘿，中国在训练模型和为它们服务方面有一个计算机劣势。上菜很关键，对吧？DeepSeek 今天不能为他们的模型服务，对吧？它完全没有库存了。它已经开始在应用程序商店的实际下载中下降，因为你下载它，你尝试注册，他们说我们不接受注册，因为他们没有能力。对的？你打开它，如果你的请求得到批准，你每秒得到不到 5 个令牌，对吗？因为没有容量，因为他们没有足够的 GPU 来为模型服务，尽管它的效率令人难以置信。
**Lex Frdiman：** 观察走私会很有趣，因为我的意思是，有毒品走私，对吗？这就是市场，武器走私和 GPU 将在某个时候超过它。

## **18.** 蒸馏：DeepSeek 基于 OpenAI 的数据进行训练

**Nathan Lambert：** 到目前为止，可能是我们每公斤的最高价值。我还有一个问题要问你，迪伦。您是否在国际上跟踪模型 API 访问？中国公司使用托管模型 API 的难易程度如何？
**Dylan Patel：** 美国。是的，我的意思是，这非常容易，对吗？就像 OpenAI 公开声明的那样，DeepSeek 使用他们的 API，正如他们所说，他们有证据，对吧？这是训练制度的另一个元素，OpenAI 的人声称这是一个提炼的模型。也就是说，你正在使用 OpenAI 的模型，你正在生成大量的输出，然后你在他们的模型中对输出进行训练。即使是这样，他们所做的仍然是惊人的。顺便说一下，DeepSeek 做了什么。
**Nathan Lambert：** 蒸馏的效率是工业上的标准做法。无论你是否在一个封闭的实验室里，在那里你密切关注服务条款和知识产权，你都可以从自己的模型中提取。如果你是一名研究人员，你没有构建任何产品，你可以从 OpenAI 模型中提取。
**Lex Frdiman：** 这是一个很好的机会。你能解释一下作为一个过程的大画面蒸馏吗？什么是蒸馏？过程是什么？
**Nathan Lambert：** 谈了很多关于训练语言模型的问题。他们受过文字训练。在后期训练中，你试图在非常高质量的文本上进行训练，你希望模型与其特征相匹配。或者如果你使用 RL，你让模型找到它自己的东西。但是对于偏好数据的监督微调，您需要完成一些工作。模型试图学习模仿什么，你在那里做什么，而不是人类数据，或者不是你目前正在训练的模型，你从一个不同的，通常更强大的模型中完成。我认为有传言说，人们正在等待的这些大模型，这些世界上的 GPT5，世界上的克劳德 3 号作品，在内部被用来做这个蒸馏过程。在一天结束的时候。
**Dylan Patel：** 也是公开的例子，对吗？像 META 一样，明确说明，不一定是蒸馏，但他们在他们的 Llama 3.2 或 3.3 中使用 405B 作为 70B 的奖励模型。
**Nathan Lambert：** 是的，这都是同一个话题。
**Lex Frdiman：** 那么这是，这是道德的吗？这合法吗？比如为什么，为什么《金融时报》文章标题说 OpenAI 说有证据表明中国的 DeepSeek 使用其模型来训练竞争对手。
**Dylan Patel：** 这是一个很长的，至少在学术和研究方面有很长的历史，因为你试图解释 OpenAI 的规则。OpenAI 的服务条款说，你不能用他们模型的输出来构建一个竞争对手。服务条款与许可证不同，许可证本质上是组织之间的合同。因此，如果你在 OpenAI 的帐户上有服务条款，如果我违反了它，OpenAI 可以取消我的帐户。这与说明如何使用下游工件的许可证有很大不同。所以这很大程度上取决于一个在人工智能领域非常不清楚的词，那就是什么是竞争对手？
**Dylan Patel：** 所以，道德方面的问题是，为什么我在你的模型上训练是不道德的，而你可以在互联网的文本上训练。
**Lex Frdiman：** 是的。
**Dylan Patel：** 对。
**Lex Frdiman：** 所以这有点虚伪，因为 OpenAI 和潜在的大多数公司都在未经许可的情况下接受了互联网文本的训练。
**Nathan Lambert：** 还有一个明显的漏洞，那就是我从 OpenAI 生成数据，然后我把它上传到某个地方，然后别人在上面训练，链接就断了。就像他们不在相同的服务合同条款下一样。
**Dylan Patel：** 这是，这就是为什么很多。
**Nathan Lambert：** 在嘻哈音乐中，有很多细节需要被发现，这些细节没有太多意义。
**Dylan Patel：** 这就是为什么今天的很多模型，即使他们在零 OpenAI 数据上训练，你问谁训练你的模型，它会说我是由 OpenAI 训练的 ChatGPT，因为互联网上有太多类似 OpenAI 输出的复制粘贴，你无法将其过滤掉。在 RL 中没有任何他们实施的东西，比如 “嘿”，比如 “后训练” 或 “ SFT ”，不管它说什么，嘿，我实际上是艾伦研究所的模型，而不是我们必须这样做。
**Nathan Lambert：** 如果我们提供演示，我们会做研究，我们会使用 OpenAI API，因为它很有用，我们想了解训练后的情况。我们的研究模型，他们会说它们是由 OpenAI 编写的，除非我们把我们谈到的系统道具放进去，我是 Tulu，我是艾伦人工智能研究所训练的语言模型。如果你问行业中更多的人，特别是有岗位训练的人，这是一个非常可行的任务，让模型说出它是谁，或者抑制 OpenAI 的东西。所以在某些层面上，Deepseak 可能并不在意它说它是由 OpenAI 开发的。就像如果你要上传模型重量，这并不重要，因为任何在应用程序中提供服务并非常关心服务的人都会这样做。当服务它时，如果他们使用它来完成特定的任务，他们会根据这一点来调整它，它说它是 ChatGPT 并不重要。
**Lex Frdiman：** 哦，我明白了。我想其中一种方法是系统提示或类似的东西。就像如果你在服务它，就说你在。
**Nathan Lambert：** 这就是我们所做的。就像如果我们主持一个演示，你说你是 Tulu。3 由艾伦人工智能研究所训练的语言模型。我们也从 OpenAI 数据中受益，因为它是一个很好的研究工具。
**Lex Frdiman：** 我的意思是，你认为 OpenAI 声称有证据表明中国的 DeepSeek 使用这种模型进行训练的说法有任何真实性和价值吗？
**Dylan Patel：** 我认为每个人都从中受益，因为数据在互联网上，因此它现在在你的前期训练中。对的？就像 SubReddit 一样，人们分享最好的 ChatGPT 输出，这些是，这些是在你的，我认为。
**Nathan Lambert：** 他们试图改变叙述，就像他们试图保护自己一样。我们在几年前就看到了这一点，当时字节跳动实际上被禁止在一些 OpenAI API 中进行输出训练。还有其他的人工智能创业公司，如果你在人工智能文化中，大多数人都会喜欢，他们只是告诉我们他们在 OpenAI 输出上进行了训练，他们从来没有被禁止。这就是他们如何引导他们的早期模型。因此，使用它比建立人力管道和建立一个强大的模型更容易起步。所以这里有很长的历史，很多交流看起来像是。
**Dylan Patel：** 实际上，在过去的几天里，我们看到很多人将 DeepSeek 的模型提取到 Llama 模型中，因为 DeepSeek 模型进行推理有点复杂，因为它们是专家的混合体，你知道，它们是 6000 多亿个参数，人们将它们提取到 Llama 模型中。然后因为 Llama 模型很容易服务，每个人都建立了管道和工具来推断 Llama 模型，对吗？因为它是开放标准。所以你知道，我们已经看到了，我们已经看到了一种迂回，对吧，就像它，它是坏的吗？它是非法的吗？也许这是非法的，随便吧。我不知道那件事，但就像。
**Nathan Lambert：** 这可能会破坏合同。我不认为这是非法的，就像在任何法律中一样，没有人会因此而坐牢。
**Lex Frdiman：** 我认为，从根本上说，我认为这是道德的，或者我希望这是道德的，因为一旦我们禁止这种事情，它会让每个人的情况都变得更糟。我也，实际上，这很难，但我认为你应该被允许在互联网上训练。我知道很多作者和创作者对此非常敏感。这是，这是一个很难的问题。但就像 Mo 一样，当你不被允许在互联网上训练时。
**Nathan Lambert：** 我同意。
**Dylan Patel：** 我，我，我对如何解决这个问题有一种精神分裂的看法，因为它已经起作用了。
**Nathan Lambert：** 我对此有合理的看法。
**Lex Frdiman：** 好的，好的。
**Dylan Patel：** 所以。所以你知道日本有一项法律，允许你在任何训练数据上进行训练，如果你想训练一个模型 A，B，日本有 9 千兆瓦的削减核能。根据人工智能扩散规则，日本被允许进口尽可能多的 GPU。所以我们所要做的就是，我们在这里有一个市场。我们建立大规模的数据中心，我们把它们租给实验室，然后我们以法律允许的方式训练模型。没有 “如果”、“和” 或 “但是”。现在模特们没有来自《纽约时报》的潜在版权诉讼或类似的东西。没有，没有。这是完全合法的。
**Nathan Lambert：** 没有。
**Dylan Patel：** 太天才了。
**Nathan Lambert：** 早期的版权诉讼对人工智能训练有利。我想说的是，使用的长尾将进入人工智能领域，也就是说，如果你搜集了数万亿的数据，你不会看到数万亿的数据标记，你不会看到并说《纽约时报》的这篇文章对我来说非常重要。但是，如果你正在为音乐或图像生成做音频生成，并且你说让它成为 X Person 的风格，这是一个合理的情况，你可以计算出他们在推理上的利润率。我不知道它是否会是 YouTube 创作者计划的 5050 或什么的，但我会选择以作家的身份加入该计划。就像，就像，就像那样。只是，这将是一段艰难的旅程，但会有一些有意义的解决方案。但在互联网上有一条长长的尾巴。
**Lex Frdiman：** 我认为《金融时报》那篇文章还暗示了另一个方面。这就引出了一个更普遍的问题。你认为有。从公司内部刺探、刺探和窃取实际密码和数据有多难？有多少人在尝试这样做？
**Nathan Lambert：** 代码和数据很难，但想法很容易。硅谷运作于。在这种情况下，顶级员工被其他公司收购以获得加薪。这些公司这样做的一个很大的原因是他们带来了想法。有，没有，我的意思是在加州有一些规定，比如竞业禁止协议或任何在加州非法的规定。不管有没有保密协议之类的东西，很多事情都是这样发生的。最近有一个来自双子座的人，他帮助制作了这个 100 万的上下文长度。每个人都在说下一个 Llama ，我的意思是他去了 Meta 团队，将会有 100 万个上下文长度。这就是这个世界。
**Dylan Patel：** 工作，你知道，就像工业间谍和过去非常成功的事情一样，对。你知道，美国人这样对待英国人，中国人也这样对待美国人。对的。你知道，等等等等。只是，这是生活的现实。因此，我认为工业间谍活动不太可能被阻止。你可以让它变得困难。但即便如此，所有这些故事都是关于，嘿，F35 和 F22 已经在设计，游戏计划和材料，代码和材料方面给了中国。就像之间。我说公司，而不是民族国家可能是非常困难的。但是想法被讨论了很多，对吧。无论是旧金山的家庭聚会，还是公司更换员工，或者总是被谈论的神秘蜜罐。有人得到了蜜罐，因为每个从事人工智能的人都是 20 多岁和 30 多岁的单身汉。不是每个人，但疯狂的百分比的疯狂数量。所以总是有这样的，你知道，很明显。
**Lex Frdiman：** 所以蜂蜜盆栽就像一个间谍，一个女间谍接近你一样。
**Nathan Lambert：** 是的，是的，或者，或者男性。
**Dylan Patel：** 对。你知道，这是旧金山。对的。但你知道，作为一个单身男人，我会说他快 30 岁了，对吧。就像我们很容易堕落。对的。就像，你知道，就像不，不是堕落的我自己，但你知道，就像我们，我们是，对。
**Lex Frdiman：** 其他人，不是我。
**Dylan Patel：** 是的，没错。
**Nathan Lambert：** 我太健忘了，而且我不是单身，所以我不会被间谍访问。

## **19.** AI Megaclusters（巨型集群）

**Lex Frdiman：** 是的，你必须确保关闭所有的安全漏洞。所以你，迪伦，为每个主要的人工智能公司收集了大量关于每个大型集群的信息。你能谈谈每一个突出的建筑吗？
**Dylan Patel：** 是的。所以我认为这些大型集群建设的真正重要之处在于它们的复杂性，其规模是前所未有的。对的。我们，你知道，有点像数据中心的电力消耗一直在缓慢上升，即使通过云计算革命，它也上升到了 2.3%。对的。数据中心消费占美国总消费的百分比，这已经有几十年了，对吧。数据中心等等。它一直在爬，慢慢地爬。但现在是 2% 到 3%，到这个十年末，就像甚至，甚至低于，你知道，当我说 10% 的时候，很多人传统上是 20，28，20，30 人，传统上是非传统的数据中心人员。就像坚果一样。但是，就像那些在人工智能中的人一样，他们真的看着这个，就像人类和开放的人工智能一样，这是不够的。我想，好吧，但是，你知道，这是，这既是通过全球分布或分布在美国各地，也像是集中的集群，对吧？分布在美国各地的是，是令人兴奋的，这是它的大部分，对不对？就像，嘿，你知道，OpenAI 或者你知道，说 Meta 正在增加千兆瓦，对吗？但其中大部分是通过美国分发的，用于推理和所有其他事情，对吗？
**Lex Frdiman：** 所以也许我们应该列出什么是集群。所以你知道，这包括 AWS 吗？也许谈论不同类型的集群是好的，你所说的超级集群是什么意思，什么是 GPU，什么是计算机。是的，不是很久以前，但是是的。那么我们所说的集群构建是什么意思呢？
**Dylan Patel：** 我以为我要做苹果的广告，对吗？电脑是什么？所以传统上，数据中心和数据中心任务一直是一个分布式系统问题，能够分布得非常广泛，对吧？例如，我向谷歌发送一个请求，它会被路由到离我比较近的数据中心。它做任何搜索，排名推荐，返回结果，对吗？任务的性质正在迅速变化，因为现在人们真正关注的是两个任务，对吗？它不是数据库访问，它不是为我提供正确的页面，为我提供正确的广告。现在这是一种推论。推理与传统的分布式系统有很大的不同，但它看起来更简单，更相似。然后还有训练，对吧？推论方面仍然是，嘿，我要在这些数据中心周围的块中放置数千个 GPU。我要在上面运行模型。你知道，用户提交请求，被踢出，或者，嘿，我的服务，你知道，他们向我的服务提交请求，对吗？他们在说话，他们就像，哦，是的，帮我副驾驶。它把它踢开了。我在我的 Windows Copilot 上，不管是什么苹果智能，不管它是什么，它都会被踢到一个数据中心，对吗？那个数据中心做了一些工作，然后把它送回来。这就是推论。这将是计算的主体。但你知道，这就像，你知道，有成千上万的数据中心，我们正在跟踪像卫星和所有这些其他的东西，这些是正在建设的大部分。但规模。所以这就像是真正的重塑，这就是获得数百万 GPU 的原因。但最大集群的规模也非常重要。对的。当我们回顾历史，对，就像你知道的，或者通过，通过人工智能时代，对。当他们做 AlexNet 的时候，这真的是一件大事，我想是两个 GPU 还是四个 GPU，我不记得了。这真的是件大事。
**Nathan Lambert：** 这是一件大事，因为你使用 GPU。
**Dylan Patel：** 这是一件大事。他们使用 GPU，他们使用多个。对的。但随着时间的推移，它的规模一直在扩大，对吧？所以当你跳到 GPT3，然后 GPT4，GPT4，20，000，100 个 GPU 前所未有的运行，对。就尺寸和成本而言，对。在 YOLO 上花了几亿美元。对的？为 GPT4 运行 YOLO。它产生了这种神奇的改进，与实验结果完全一致。就像对数刻度一样。
**Nathan Lambert：** 哦，是的，从报纸上看，技术是其中的一部分。
**Dylan Patel：** 比例定律是完美的。对的。但这并不是一个疯狂的数字，对吧？大约 20，000。每个 GPU 的功耗为 400 瓦。然后当你加入整个服务器，对，所有的东西，它就像 15 到 20 兆瓦的电力，对。你知道，你知道，也许你可以查一下人类的消费能力是什么，因为数字会变得很愚蠢。但是，15 到 20 兆瓦是标准的数据中心规模。这是前所未有的。所有的 GPU 都在运行一个任务。
**Nathan Lambert：** 烤面包机的功率是多少瓦？
**Dylan Patel：** 烤面包机就是一个很好的例子。与 A100 的功耗相似，对。H100 来了，他们把功率从 400 瓦增加到 700 瓦。这只是每个 GPU。然后围绕着它有所有相关的东西。所以一旦你把所有这些都算上，所有的东西大概都是 1200 到 1400 瓦。网络，CPU，内存，等等等等。
**Lex Frdiman：** 所以我们也应该说，那么需要什么？你说的是权力。所以需要很大的能量。产生大量热量。所以需要冷却。因为有很多 GPU 必须。或者 CPU 之类的，它们必须连接在一起。所以有很多网络。
**Dylan Patel：** 是的，是的。所以我想，是的，很抱歉跳过了这个。然后数据中心本身就很复杂，对吧？但这些仍然是 GPT4 规模的标准化数据中心。对的？现在我们来看看人们去年建立的集群的规模是多少，对吧？它的范围很广，对吧？它的范围从，嘿，这些是标准的数据中心，我们只是使用多个数据中心并将它们连接在一起，它们之间有大量的光纤，大量的网络，等等。这就是 OpenAI 和微软在亚利桑那州所做的，对吧？所以他们有 100，000 个 GPU，对吗？元相似的东西。他们采用了标准的现有数据中心设计，它看起来像一个氢，他们将多个数据中心连接在一起，他们首先做了 16，000 个 GPU，总共 24，000 个 GPU。只有 16 个。他们中的数千人在训练中运行，因为 GPU 非常不可靠。因此，他们需要有备件，以交换进出所有的方式。现在有 10 万个 GPU，他们目前正在 Llama 4 上训练，对吗？大概 12.8 万左右吧？想想 100，000 个 GPU，每个大约 1400 瓦。那是，那是，那是 140 兆瓦。150 兆瓦，对。为了 128，对。所以你说的是你在两年内从 15 到 20 兆瓦跳到了 10 倍，你知道，几乎是这个数字的 10 倍，9 倍到 150 兆瓦，对。从 2022 年到 2024 年。对。有些人喜欢埃隆，他承认，对。他说他自己进入这个游戏的时间有点晚，因为他对大型语言模型进行了预训练，对吧？赛赛是后来开始的，对吧？但后来他，他赌上了天堂和地狱，把他的数据中心建起来，得到了世界上最大的集群，对，那就是 20 万个 GPU。他做到了。他在孟菲斯买了一家工厂。他正在升级变电站，但同时他得到了一堆移动发电，一堆单循环联合。 他接通了工厂旁边的天然气管道，拉出了一吨天然气，燃烧天然气，他产生了所有这些能量。他在一家工厂里，在一家很久以前关闭并搬到中国的旧家电工厂里，你知道，他有 20 万个 GPU。现在下一个尺度是什么？对。就像所有的超大规模公司都这样做了一样。现在下一个规模是更大的，对吧？所以你知道，埃隆，只是为了坚持这个话题，他正在建造自己的天然气工厂，就像隔壁的一个合适的工厂。他正在部署数吨的特斯拉 Megapack 电池，以使电力更加平稳，以及其他各种事情。他用工业冷却器来冷却水，因为他用水来冷却芯片。所以所有这些疯狂的事情都是为了让集群变得越来越大。但当你看到 OpenAI 对星际之门所做的事情时，那就是在亚利桑那州。在德克萨斯的阿比林，对吗？他们至少宣布了什么，对吧？它还没建成，对吧？埃隆说他们没有钱。你知道，关于这一点有一些争论，但至少在整个范围内，第一部分是这样的，肯定是钱占了，但有多个部分。但全面的数据中心将达到 2.2 千兆瓦，对吗？2200 兆瓦的电力，大约 1.8 千兆瓦或 1800 兆瓦的电力输送到芯片，对吗？现在。这是一个荒谬的尺度。2.2 千兆瓦比大多数城市都要多，对吧？明确地说。交付给连接到进行训练的单个集群，对吗？去训练这些模型，去做前期训练，后期训练，所有这些东西，对吧？
**Lex Frdiman：** 这太疯狂了。
**Nathan Lambert：** 什么是核电站？
**Dylan Patel：** 每个人都在这样做，对吗？路易斯安那州的每个人都在做 Meta Metta，对吧？他们正在建造两个大型天然气工厂，然后他们正在建造这个大型数据中心。亚马逊对这种规模有类似的计划，谷歌也有这种规模的计划。赛伊对这些规模有计划，对吧？就像所有这些人一样，那些正在比赛的人，那些正在比赛的公司正在努力比赛，他们正在做多个千兆瓦的数据中心，对吗？来建造这个。因为他们认为，是的，如果我现在有明显的预先训练，缩放将会继续，但在某种程度上。但是还有所有这些训练后的东西，你有计算机使用的 RL 沙盒或其他东西，对吗？这是他们要去的地方。所有这些都验证了可行的领域，他们只是不断学习，学习，学习。自我游戏，不管它是什么，都会让 AI 变得更有能力，因为线确实上升了，对吧？投入越多计算，性能就越高。这件衬衫是关于比例定律的，你知道，在某种程度上它是收益递减的，对吧？你 10 倍的计算，你不会得到 10 倍更好的模型，对不对。你得到了递减的回报，但你也得到了效率的提高。所以你弯曲了曲线。对的。这些规模的数据中心正在做，你知道，散发着恶臭，你知道，对网络造成了很大的破坏。对的。你知道，内特 · Nathan 提到亚马逊试图收购塔龙核电站，如果你看看塔龙的股票，它就像，就像飞涨，你知道，就像他们在那里建造一个巨大的千兆瓦数据中心。你知道，你只要沿着名单往下走。有这么多的分歧。有趣的是，在美国的某些地区，传输电力的成本比实际发电的成本要高，对吧？因为电网的建设非常缓慢，对电力的需求和建设电力的能力，就像重新增加天然气发电厂，甚至是燃煤发电厂一样，很容易做到。 但就像传输能量真的很难。所以在美国的一些地方，比如弗吉尼亚州，传输电力的成本比发电的成本更高，这就像，你知道，这里有各种各样的二阶效应，这是疯狂的。
**Lex Frdiman：** 电网能支持这种增长吗？
**Dylan Patel：** 你知道，特朗普的行政命令，在年底之前有一个拜登的行政命令，但后来特朗普有更多的行政命令，希望将法规减少到，是的，可以建立的地方。但是，是的，这是一个很大很大的挑战，对吧？建设足够的电力速度够快吗？
**Lex Frdiman：** 你是不是打算在每一个数据中心旁边都建一个核电站？
**Dylan Patel：** 所以，有趣的是，建造发电厂的速度太慢了。建造发电厂或重新配置现有发电厂的速度太慢。因此你必须使用自然数据中心。电力消耗持平。对的。你知道，我的意思是喜欢它，它。
**Nathan Lambert：** 这就是为什么核能对它也有好处。就像长期核能是一个非常自然的选择。但是，是的，你不能在短期内做太阳能或任何类似的事情。
**Dylan Patel：** 因为数据中心的电力是这样的，对吗？就像你告诉我的那样，你知道，我打算购买数百亿美元的 GPU，然后闲置它们，因为电力没有产生。就像电力很便宜，对吧？如果你看看集群的成本，不到 20% 是电力，对吧。其中大部分是 GPU 的资本成本和折旧。对的。所以这就像，好吧，管他呢，我就像，你知道，我就会建造天然气工厂。这就是梅塔在路易斯安那州所做的。这就是 OpenAI 在德克萨斯州所做的事情。就像所有这些不同的地方一样，他们可能不会直接做这件事，但他们会与某人合作。所以。所以有几个希望，对吧？就像，一个是，你知道，埃隆，他在孟菲斯所做的就像，你知道，到了极端。他们不只是使用双联合循环气体，这是超级高效的。他也只是使用单循环和移动发电机之类的东西，效率较低。但另一方面，太阳能发电是这样的，风能是另一个这样的。不同的关联，你知道，不同的。所以如果你把这两个都堆起来，再加上你有一大块电池，再加上你有一点点汽油，就有可能让它运行得更环保。只是时间进度太慢了，对吧？所以人们在努力。但梅塔基本上是说，无论如何，不要在乎我的可持续发展承诺。或者他们会像每个电力一样购买，这被称为 PPA 电力购买协议，那里会有一个大型的风力发电场或太阳能发电场，就像任何地方一样，然后他们会假装这些电子被数据中心消耗，但实际上他们在这里为电力付费，并将其出售给电网，他们在这里购买电力。然后另一件事是微软放弃了他们的一些可持续发展承诺，对吗？埃隆，他对孟菲斯的所作所为客观上有些肮脏， 但他也在一个地区做这件事，比如隔壁有一个更大的天然气厂，旁边有一个下水道，或者不是下水道，而是废水处理和附近的垃圾场。对的？他显然让世界变得比一个数据中心要做的更干净，对吧？所以我认为，这在某种程度上是好的，也许 AGI 解决了全球变暖之类的问题，对吧？不管它是什么，你知道，这是，这是实验室里的人的一种态度，对吗？这就像，是的，这是伟大的，我们将只使用天然气，对不对？因为比赛很重要，如果我们输了，那就更糟了，对吧？
**Lex Frdiman：** 我应该说，我有机会参观了孟菲斯数据中心，这真是太不可思议了。我是说，我和埃隆一起去的。仅仅是团队和创新的速度是疯狂的，因为我的感觉是，你知道，从来没有人做过这种规模的事情，当然也没有人以 XAI 正在做的速度做过这种规模的事情。所以他们想弄清楚，我的意思是，我参加了所有这些他们头脑风暴的会议。这就像，这是疯狂的。这是令人兴奋的，因为他们正在试图找出瓶颈是什么，如何消除瓶颈，如何确保，你知道，有这么多很酷的事情要把数据中心放在一起，因为，你知道，一切都必须正常工作。这是人们喜欢的系统管理员，机器学习，所有这些都是令人兴奋的事情，等等。但实际上运行一切的人是那些知道运行一切的低级软件和硬件的人，网络，所有这些。所以你必须确保你有测试一切的程序。我想他们用的是以太网。我不知道他们是怎么做的。
**Dylan Patel：** 网络，但他们使用的是 NVIDIA Spectrum Xe 以太网。实际上，我认为，是的，无名英雄是冷却和电力系统，就像被掩盖了一样。
**Lex Frdiman：** 是的。
**Dylan Patel：** 但我认为，有一个故事可能说明了这件事有多疯狂，那就是当你在训练的时候，你总是在做，你在运行一堆模型，对吗？用最简单的话来说，在模型中运行一串。然后你要交换所有的东西并同步重量，对吗？所以你要做一个步骤。这就像是模特训练的一个步骤，对吧？每走一步，你的损失都会减少，希望如此，但并不总是如此。但用最简单的话来说，你会计算很多，然后你会交换，对吧？有趣的是，GPU 的能力是最重要的。网络的力量是一些，但它是少得多。但是当你在计算的时候，你的 GPU 的能力就在这里。但是当你交换权重时，如果你不能完美地重叠通信和计算，可能会有一段时间你的 GPU 处于空闲状态，你在交换权重，你会想，嘿，模型在更新，所以你在交换梯度，你在做模型更新，然后你又开始训练。所以电力正常，它是超级尖峰。有趣的是，当你谈论数据中心的电力规模时，你可以很容易地把东西炸掉。所以 Meta 实际上意外地将一些内容上传到了 PyTorch 中的代码中，他们在其中添加了一个操作符。我不骗你，不管是谁做的，就像，我想拥抱这个人，因为它说，PyTorch 说。就像 PyTorch Dot 发电厂一样。没有爆炸等于 0 或等于 1。它所做的，它所做的是惊人的，对吗？要么，你知道，一年当你交换权重时，GPU 只会计算假数字，所以功率不会太大。所以发电厂不会爆炸，因为短暂的尖峰就像把事情搞砸了一样。
**Lex Frdiman：** 嗯，有道理。我的意思是你必须做那种事。你必须确保他们没有闲着。是啊。
**Dylan Patel：** 埃隆的解决方案就像是，让我扔一堆特斯拉大包和其他一些东西，对吗？就像每个人都有不同的解决方案。但就像梅塔一样，至少是公开的，公开的，众所周知的，就像设置这个操作符一样。这个操作符所做的就是让 GPU 什么都不计算，这样功率就不会达到峰值。
**Lex Frdiman：** 但这只是告诉你你工作的力量有多大。我是说，这太疯狂了。这太疯狂了。
**Nathan Lambert：** 人们应该去谷歌，比如规模，比如 X 瓦特是做什么的？从 1 瓦到 1 千瓦再到 1 兆瓦，你看着它，盯着它，你知道千兆瓦在名单上有多高吗？令人兴奋不已。
**Lex Frdiman：** 你能谈谈冷却吗？所以我，我知道埃隆使用液体冷却。我相信，在任何情况下，这都是一件新鲜事，对吗？它们中的大多数不使用液体冷却。关于冷却，有什么有趣的事情要说吗？
**Dylan Patel：** 是的，是的。因此，空气冷却已成为事实上的标准。扔一堆金属热管等和风扇，对不对？就像冷却了一样。这足以让它冷静下来。人们一直在涉足水冷却。谷歌的 TPU 是水冷的，对吧？所以他们已经这样做了几年了。但有了 GPU，没有人做过，也没有人做过埃隆刚刚做过的水冷规模。目前，下一代 NVIDIA 是针对最高端 GPU 的。这是强制性的水冷却。你必须用水冷却它。但埃隆在这一代人身上做到了这一点，这需要很多东西。对的。如果你看一些卫星照片和孟菲斯设施的东西，所有这些外部水冷却器基本上看起来像一个半卡车，豆荚的东西，它叫什么？那个容器。但实际上那些是冷水机组。他大概有 90 台水冷却器就在外面。90 个不同的容器，对吧？用水，你知道冷却水，把它带回数据中心，然后你把它分配给所有的芯片，把所有的热量排出，然后把它送回来。对的？这既是一种冷却芯片的方法，也是一种效率的方法。好吧。回到三个矢量的问题上，对吧？有，有，你知道，内存带宽，触发器和互连。芯片之间的距离越近，就越容易实现高速互连，对吗？这也是为什么你想用水冷却的原因，因为你可以把芯片放在一起，从而获得更高的连接速度。
**Lex Frdiman：** 我想问你，在你最近的一篇文章中，有一个叫做集群测量竞赛的部分。
**Dylan Patel：** 还有一个词，但我不会说出来。
**Lex Frdiman：** 现在谁的最大，谁的。
**Dylan Patel：** 今天会有最大的吗？个人最大的是埃隆，对吗？
**Lex Frdiman：** 埃隆的集群。
**Dylan Patel：** 埃隆在孟菲斯的集群，200，000 个 GPU，对吗？Meta 有 128，000 个，OpenAI 有 100，000 个。现在，需要明确的是，其他公司拥有比埃隆更多的 GPU。他们只是没有把它们放在一个地方，对吗？对于训练，你希望他们紧密相连。有一些技术，人们正在研究和工作，让你在多个地区训练，但在大多数情况下，你希望他们都在一个地区，对不对？因此，您可以通过高速网络将它们高度连接起来。所以，你知道，埃隆今天有 200，000 马力的 H1，100，000 马力的 H2，100，000 马力的 H2，对吗？Meta、OpenAI 和 Amazon 都在 10 万的规模上略少一些。但是明年，对，今年人们会建造更多，对吗？Anthropic 和亚马逊正在建立一个 40 万 Trainium 2 的集群，这是亚马逊专用的芯片，试图摆脱英伟达，对吗？你知道，Meta 和 OpenAI 的规模可达数十万，但到明年，你将拥有 50 万到 70 万个 GPU 集群。请注意，这些 GPU 的功耗比现有的要高得多，对吗？漏斗，700 瓦。布莱克威尔达到 1200 瓦。对的？所以，每个芯片的功率在增长，芯片的数量也在增长，对吧？
**Lex Frdiman：** 坚果。是的。你认为，你认为埃隆说他会得到一百万。你觉得这可行吗？
**Dylan Patel：** 我的意思是，我不怀疑埃隆，对吗？他的文件，比如，你知道，电力计划和特斯拉电池组，很明显他对孟菲斯有一些疯狂的计划，比如许可证之类的东西是公开记录的，对吧？但目前尚不清楚是什么以及时间尺度是什么。我从不怀疑埃隆，对吧？他会给我们惊喜的。
**Lex Frdiman：** 那么这些集群的想法是什么？如果你有一百万个 GPU，那么在两三年的时间里，用于训练的 GPU 占多大比例？以及训练前的百分比和实际使用的百分比。
**Dylan Patel：** 这些巨型集群对推理没有意义，对吗？你可以在那里进行推理，而不是训练。但大部分的推理能力是，你知道，嘿，我在这里有一个 30 兆瓦的数据中心，我在这里有 50 兆瓦，我在这里有 100 兆瓦，等等。我将在所有这些中加入推论。因为大型集群，对，多千兆瓦数据中心，我想在那里训练，因为我所有的 GPU 都位于那里，在那里我可以将它们以超高的网络速度连接在一起。对的？因为这是你训练所需要的。现在有了预训练，这是旧的规模，对吗？你可以增加参数，增加数据。模型变得更好。这已经不适用了，因为在训练前没有更多的数据，对吧？是的，视频、音频和图像还没有被充分利用。所以有更多的扩展。但是很多人都有 YouTube 视频的抄本，这让你得到了大量的数据，但并没有让你从视频和图像数据中获得所有的学习价值。但在训练前仍需进行缩放。但在这个训练后的世界里，所有的失败都会被花掉，对吧？模特要自己玩了。它会自己玩，它会做可验证的任务，它会在沙箱中使用计算机。它甚至可以做模拟机器人的事情，对吧？就像所有这些东西都将是计算花费在引用或引用，训练后的环境。但我想会很好的。我们要把这个岗位从岗位训练中撤下来。是的，这将是预先训练，这将是训练。我认为在某种程度上，因为在过去几年的大部分时间里，训练前的训练已经让训练后的训练相形见绌。但有了这些可验证的方法，特别是那些具有无限潜力的方法，比如计算机使用和机器人技术，而不仅仅是数学和编码，你可以验证发生了什么， 那些无限可验证的任务，似乎你可以花费和你一样多的计算。
**Dylan Patel：** 想要他们，尤其是在上下文长度增加的时候。因为当您增加这些模型的上下文长度时，预训练结束。我们在前面的对话中讨论过，当你有一个长的输入时，上下文长度比输出更容易管理。很多训练后和推理技术都依赖于大量的采样，而且它变得越来越长。所以实际上你的计算效率下降了。我认为翻牌是衡量它的标准。但在 RL 中，你必须做所有这些事情，以不同于训练前和生成时的方式移动你的重量，这将变得效率更低，FLOPS 将不再是一个有用的术语。然后，随着基础设施变得更好，它可能会回到失败。
**Lex Frdiman：** 所以我们一直在谈论的所有东西最有可能是英伟达，对吗？有竞争对手吗？
**Dylan Patel：** 谷歌。谷歌。我有点忽略了他们。
**Lex Frdiman：**TPU 的故事是什么？
**Dylan Patel：** 比如 TPU 很棒，对吗？太棒了。谷歌是。他们在建立数据中心方面有点不温不火。出于某种原因，他们正在建设大数据中心，不要误解我的意思。他们实际上拥有最大的集群。我说的是英伟达集群。它们实际上拥有最大的集群周期。但他们做的方式很有趣。对的。他们有两种数据中心超级区域，对吧？因为数据中心不是物理上的。就像所有的 GPU 都不在一个站点上一样，但它们彼此相距 30 英里，而不是 GPU。TPU，对吗？他们在爱荷华州和内布拉斯加州有四个数据中心，就像彼此相邻一样。
**Lex Frdiman：** 为什么谷歌不调整它的集群规模？
**Dylan Patel：** 去多数据中心训练？里面有很好的图像。所以我会告诉你我的意思。它只是半分析多数据中心。所以这就像，你知道，这是一个标准的谷歌数据中心的图像。顺便说一下，他们的数据中心看起来与其他公司的数据中心非常不同。
**Lex Frdiman：** 我们在这里看到的是什么？
**Dylan Patel：** 所以这些是。是啊。所以如果你，如果你看到这个图像在中间，有这些大的矩形盒子。盒子，对。这些是真正的筹码存放的地方。然后如果你再往下滚动一点，你可以看到这些水管，顶部有这些冷却器冷却塔和一堆柴油发电机。柴油发电机是备用电源。数据中心本身看起来比冷水机组小。对的。所以薯片实际上更容易放在一起。但是然后冷却所有的水，对于水冷却来说是非常困难的。所以谷歌有一个非常先进的基础设施，这是其他人没有的。他们所做的就是建立这些数据中心，他们在几个地区建立了一批这样的数据中心，对吧？所以如果你再往下走一点，这是微软，这是在亚利桑那州。这是训练 GPT5 报价的地方。
**Nathan Lambert：** 如果它还不存在的话。
**Dylan Patel：** 是的，如果它还不存在的话。但是这些数据中心中的每一个，我都展示了它们的几张图片。他们在同一个地区非常接近，对吗？内布拉斯加州，爱荷华州。然后他们在俄亥俄州也有一个类似的。复杂的。对的。所以这些数据中心彼此非常接近，他们所做的就是用光纤将它们连接起来，带宽非常高。所以这些只是一堆数据中心。这里的重点是，谷歌拥有非常先进的基础设施，在一个小区域内非常紧密地连接。因此，Elon 将始终拥有完全连接的最大集群。对的。因为这一切都在一栋楼里。对的。他在这一点上完全正确。对的。谷歌拥有最大的集群，但你必须分布在三个站点上，而且要有很大的差距，但你必须跨越多个站点。
**Lex Frdiman：** 为什么谷歌不与英伟达竞争？他们为什么不卖 TPU？
**Dylan Patel：** 我认为，我认为它有几个问题。这就像一个 TPU 已经成为一种让搜索变得非常便宜的形式，并为此建立模型。对的？因此，就像搜索的一大块，GPU 购买或 TPU 购买或谷歌的购买和使用的一大块，所有这些都是用于内部工作负载，无论是 Search Now，Gemini，YouTube，他们拥有的所有这些不同的应用程序，广告，这些都是他们所有的 TPU 花费的地方，这是他们高度关注的。对的。因此，体系结构的某些方面针对其使用情形进行了优化，而在其他地方没有进行优化。对的。一个简单的例子是他们开源了一个杰玛模型，他们称之为杰玛 7B，对吗？但实际上是 80 亿个参数，因为词汇量太大了。他们之所以把词汇量做得这么大，是因为 TPU 的矩阵乘法单元很大，因为这是他们优化的原因。所以他们决定，好吧，我也要扩大词汇量。尽管在这么小的模型上这样做是没有意义的，因为这适合他们的硬件。因此，杰玛在 GPU 上的运行效率不如 Llama 。对的。但反之亦然，Llama 在 TPU 上的运行效率不如杰玛。对的。这就像硬件、软件共同设计的某些方面一样。所以他们所有的搜索模型都是他们的排名和推荐模型，所有这些不同的模型都是人工智能，但不像人工智能。已经与 TPU 的超级优化永远。软件堆栈是超级优化的。但是所有这些软件栈都没有公开发布。对的。它的一小部分，JAX 和 XLA 已经。但就像你在谷歌内部的经历一样，作为一名研究人员，你在 TPU 上接受训练，在很多情况下，你不需要了解任何关于硬件的知识，对吧？就像它很漂亮，但就像。
**Nathan Lambert：** 你一走出去，他们就都走了。很多人都回去了。他们离开谷歌，然后又回去。
**Dylan Patel：** 是的，是的，他们离开了，他们开了一家公司，因为他们有所有这些惊人的研究想法，他们想，等等，基础设施很难，软件很难，这是在 GPU 上，或者如果他们试图使用 TP，同样的事情，因为他们无法访问所有这些代码。所以这就像你如何说服一家以搜索为摇钱树的公司，他们从那里赚了数千亿美元，开始销售 GPU 或 TPU，他们过去只买了几十亿。你知道，我认为在 2023 年他们购买了几十亿美元，现在他们购买了价值 100 亿到 150 亿美元的东西。但你如何说服他们，他们应该购买两倍的数量，然后想办法卖掉它们，赚到 300 亿美元。比如谁在乎赚 300 亿美元？
**Lex Frdiman：** 那 300 亿美元最终不会超过实际的搜索利润吗？
**Dylan Patel：** 我的意思是，你总是会在服务上赚更多的钱。我的意思是，是的，需要明确的是，今天人们在硬件上的花费比在服务上的花费多得多。对的？因为硬件前端运行服务开销。但就像你在投资一样。如果人工智能的东西没有收入或者没有足够的收入，那么很明显它会爆炸。对的。你知道，人们不会永远把钱花在 GPU 上。英伟达正试图通过他们试图销售和授权的软件和其他东西来提升堆栈。对的。但是，但是谷歌从来没有像这样的 DNA，这是我们应该销售的产品。对的。他们不行动。谷歌云做到了。它是一个独立于 TPU 团队的组织，TPU 团队是一个独立于 DeepMind 团队的组织，DeepMind 团队是一个独立于搜索团队的组织。对的。有很多官僚作风。
**Lex Frdiman：** 等等，谷歌云是一个独立的团队，而不是 TPU 团队？
**Dylan Patel：** 从技术上讲，TPU 位于谷歌云下的基础设施之下。但就像谷歌云一样，租用东西和 TPU 架构是非常不同的目标。硬件和软件，就像所有这些。对的。与 JAX 一样，XLA 团队不为 Google 的外部客户提供服务，而 NVIDIA 的各种 CUDA 团队则为 Nickel 等公司提供外部客户服务。对的。像 Jackson XLA 这样的内部团队，他们更多地服务于 DeepMind 和搜索。对的。所以他们的客户是不同的。他们不是在为他们制造产品。
**Lex Frdiman：** 你明白为什么 AWS 一直在与 Azure for Cloud 和 Google Cloud 的竞争中获胜吗？是的，谷歌云很小，不是吗？
**Dylan Patel：** 相对于 AWS，谷歌云排名第三。是的，是的。微软是第二大，但亚马逊是最大的。对的。微软欺骗性地包括了微软 Office 365 之类的东西，比如一些企业范围的许可证。所以在现实中，鸿沟甚至更大。微软仍然是第二，对不对？亚马逊要大得多。为什么？因为使用 AWS 更好、更容易，而且在许多情况下，它更便宜，而且是第一个。
**Nathan Lambert：** 是第一次。
**Lex Frdiman：** 是的，但是有很多事情是第一位的。
**Dylan Patel：** 嗯，这很容易，转换比转换更难。转换的费用也很高。
**Dylan Patel：** 亚马逊网络服务为亚马逊创造了超过 80% 的利润。我认为超过 90%。
**Lex Frdiman：** 这太疯狂了。
**Dylan Patel：** 配送中心就像有一天我们会决定从中赚钱。但他们还没有。对的。就像他们从中赚取了一点点利润。
**Nathan Lambert：** 是的。总有一天亚马逊 Prime 的价格会翻三倍。
**Lex Frdiman：** 你会认为他们会改进 AWS 界面，因为它很糟糕，很笨重。但每个人都是。
**Nathan Lambert：** 我，我，是的，你会认为。
**Dylan Patel：** 我，我认为实际上谷歌的界面有时很好，但也好像他们不关心除了他们的顶级客户之外的任何人。
**Lex Frdiman：** 没错。
**Dylan Patel：** 就像他们的客户服务一样糟糕。就像他们拥有的少了很多。
**Lex Frdiman：** 就像，我的意思是所有这些公司，他们为大客户进行优化。是啊，应该是为了生意。
**Dylan Patel：** 亚马逊也一直在为小客户进行优化。对的。很明显，他们为大客户优化了很多。但是，就像，就像当他们开始的时候，他们只是去喜欢随机的海湾地区的东西，并给出学分。对的。然后他们喜欢。或者把你的信用卡放进去，用我们的。对的。就像早期一样。所以他们总是，业务与他们一起成长。对的。还有处女。就像为什么亚马逊，为什么雪花遍布亚马逊？因为雪花在一开始亚马逊不关心他们的时候，还在使用亚马逊。对的。当然，有一天雪花和亚马逊有了一个超级庞大的合作伙伴关系。但像这样的情况，像亚马逊的用户体验和质量是比较好的。此外，他们设计的许多芯片使他们在传统的云存储、CPU 网络以及数据库等方面具有更低的成本结构。对的。我认为亚马逊的五大收入产品中的四个，毛利产品，对不起，毛利产品都是与数据库相关的产品，比如红移和所有这些东西。对的。所以亚马逊有一个非常好的硅用户体验，与 AWS 的整个管道。我认为谷歌，他们的硅团队，是的，他们内部有很棒的硅，TPU，YouTube 芯片，他们制造的其他一些芯片。问题是他们不是在为外部客户服务，而是在为内部客户服务。对的。
**Nathan Lambert：** 我的意思是，英伟达的整个文化都是自下而上设计的。最近有一本名为《The Nvidia Way by Take Him》的书，详细介绍了这一点，以及他们如何寻找未来的机会，并准备好他们的 CUDA 软件库，以便高性能计算的新应用可以在 CUDA 和 NVIDIA 芯片上快速发展。这与谷歌的服务业务完全不同。
**Lex Frdiman：** 是的，我的意思是英伟达，应该说，是一家真正特别的公司。就像我的意思是他们，整体，文化，一切，他们真的优化了这种事情。说到这里，有没有人甚至可以挑战英伟达的硬件智慧？英特尔？AMD？
**Dylan Patel：** 我真的不这么认为。我们经历了一个非常漫长的过程，与 AMD 合作，对他们的 GPU、推理和其他东西进行训练，他们做得很好。他们的硬件在很多方面都比英伟达的好，问题是他们的软件真的很糟糕，我认为他们正在变得更好，对吗？他们好得更快了。但海湾太大了，他们没有在这方面投入足够的资源，历史上也没有。也许他们现在改变了调子。但是你知道，因为，因为，几个月来我们提交了最多的错误，对吗？就像我们半分析，对吧？比如他妈的什么？比如为什么我们提交的 bug 最多，对吧？因为他们只关心他们最大的客户，所以他们会给他们一个私人形象，等等等等。这就像，好吧，但就像我只是在使用 PyTorch，我想使用公开可用的库，就像你不关心这个，对吗？所以他们，他们正在变得更好。但我认为 AMD 是不可能的。英特尔现在显然处于水深火热之中，需要以某种方式加以拯救。对美国的国家安全非常重要。
**Lex Frdiman：** 你能解释一下吗？很明显。那么，他们为什么处于水深火热之中呢？
**Dylan Patel：** 回到早期，只有三家公司可以研发，对吗？台湾、新竹、三星、平壤，然后是英特尔希尔斯堡。三星的表现很糟糕，英特尔的表现也很糟糕。我们可能处在这样一个世界，只有一家公司可以进行研发，而这家公司已经制造了大部分芯片。不管怎样，他们的市场份额一直在增加。但就像，就像这是一个关键的事情，对不对？因此，台湾发生的事情意味着世界其他地区的半导体产业，因此科技依赖于台湾，对吗？这显然是不稳定的。就像英特尔一样，他们一直在缓慢而稳步地下降。他们是在服务器和个人电脑之上，但现在苹果完成了 M1，英伟达发布了个人电脑芯片，高通发布了个人电脑芯片。在服务器方面，超大规模厂商都在制造自己的基于 ARM 的服务器芯片。英特尔没有人工智能芯片的胜利，对吧？他们有非常小的胜利，他们从来没有进入移动领域，因为他们对 iPhone 说不，所有这些事情都加剧了，他们已经失去了他们的流程技术领导地位，对吗？他们领先了 20 年，现在至少落后了几年，对吧？他们正在努力追赶，我们将看到他们的 18A 和 14A 战略是否奏效，他们试图超越台积电和英特尔，这就像是损失了大量的钱，对吗？他们刚刚解雇了他们的首席执行官，尽管首席执行官是唯一了解公司的人。是的，我们会看到，他不是最好的，但他是相当不错的，相对技术的家伙。
**Lex Frdiman：** 英特尔在哪里赚钱最多？CPU。
**Dylan Patel：** 仍然是个人电脑和数据中心 CPU。是的，但数据中心 CPU 都在云计算中，亚马逊，微软，谷歌正在制造基于 ARM 的 CPU，然后在个人电脑方面，AMD 的市场份额增加了。英伟达正在推出一款不会成功的芯片。对的。联发科，高通曾经推出过芯片。苹果做得很好。对的。就像，就像他们。他们在个人电脑上可能会受到一些挤压。虽然个人电脑一般我想只会坚持。英特尔主要用于 Windows 端。

## **20.** 谁是 AGI 的最后赢家


**Lex Frdiman：** 让我们来谈谈广泛的人工智能竞赛。你觉得谁会赢？谁谈到了谷歌？
**Nathan Lambert：** 默认的领导者是谷歌，因为他们的基础设施优势。
**Lex Frdiman：** 嗯，就像新闻中一样，OpenAI 是领导者。
**Nathan Lambert：** 他们是这方面的领先者。
**Dylan Patel：** 他们有最好的模型。
**Nathan Lambert：** 他们有人们可以使用的最好的模型，而且他们是。
**Dylan Patel：** 他们拥有最多的人工智能收入。
**Nathan Lambert：** 是的。OpenAI 正在取得胜利。
**Lex Frdiman：** 那么现在谁在人工智能上赚钱？有人赚钱吗？
**Dylan Patel：** 所以从利润的角度来看，微软是在赚钱，但他们花了很多资本支出。对的。你知道，那会随着时间的推移而贬值。梅塔赚了很多钱。但是有了推荐系统，也就是人工智能。但不是 Llama 。对的。拉玛肯定在赔钱。对的。我认为 Anthropic 和 OpenAI 显然没有赚钱，因为否则他们就不会筹集资金。对的。他们必须筹集资金来建造更多。对的。虽然理论上他们是在赚钱。对的。就像你知道的，你在 GPT4 上花了几亿美元，而它却创造了数十亿美元的收入。所以很明显，这就像赚钱一样。尽管他们必须继续研究才能获得计算效率的胜利。对的。然后沿着曲线向下移动，就像你知道的那样，12 得到了 GPT3 已经达到的 1200 倍。你知道，也许我们现在只有几百倍，但你知道，有了 GPT4、Turbo 和 4.0，即使在某个时候推出，也会有另一种可能比 GPT4O 更便宜的产品。
**Lex Frdiman：** 这项研究花费了很多钱。
**Dylan Patel：** 是的，没错。
**Lex Frdiman：** 我想这是与成本无关的事情，当你提到模型的成本时，它不仅仅是训练或测试运行，而是实际的研究。人力。
**Dylan Patel：** 是的。去做一些事情，比如现在就去推理它的存在。他们会扩大规模，他们还会做大量的研究。我认为人们关注的是回报问题，但很容易就像这样，好吧，GDP 是人类和工业资本。对的。如果你能让智能变得便宜，那么你就能增长很多。对的。这是一种愚蠢的解释方式，但这基本上就是投资的主题。我认为只有英伟达和其他硬件供应商实际上赚了很多钱，超大规模厂商都在纸面上赚钱，但实际上他们在购买 GPU 上花了很多钱，你不知道两年后他们是否还能在每个 GPU 上赚这么多钱。对的。你不知道 OpenAI 会不会突然变得很糟糕，现在微软有成千上万的 GPU，他们租给了 OpenAI，他们用自己的投资购买了这些 GPU，但这些 GPU 不再有客户了。对的。这始终是一种可能性。我不相信。我认为 OpenAI 将继续筹集资金。我认为其他人会继续筹集资金，因为一旦我们有了 AGI，投资和回报最终将是巨大的。
**Lex Frdiman：** 那么你认为多家公司会得到。让我们假设。
**Dylan Patel：** 我不认为这是赢家通吃。
**Lex Frdiman：** 好的，所以它不是，让我们不要叫它 AGI，无论什么。就像一天一样。它是，它是一个渐进的人工智能，超级强大的人工智能，但它是，它是一组逐渐增加的有用的功能，并快速增加快速，快速增加的功能集。所以你是说很多公司都会。所有这些公司都在建造巨大的数据中心，这似乎很荒谬。
**Nathan Lambert：** 有些公司会从人工智能中受益，但不是因为他们进行了训练。像 Meta 这样最好的模式有很多途径可以从人工智能和他们所有的服务中受益，人们在那里，人们花时间在 Meta 的平台上，这是一种从每个用户每小时赚更多钱的方式。
**Lex Frdiman：** 是的，看起来就像谷歌 X 斜线 Xai 斜线特斯拉。重要的是要说。然后，Meta 将不会像 LLM 一样直接受益于人工智能，而是受益于智能，比如对他们已经销售的产品的额外智能提升。所以无论是推荐系统还是埃隆，他一直在谈论擎天柱，机器人，潜在的机器人智能，然后你在家里有个性化的机器人，诸如此类的事情。他认为这是一个十多万亿美元的生意，在某种程度上，也许。
**Nathan Lambert：** 我不知道，不会很快，但谁知道呢。
**Dylan Patel：** 什么机器人技术？让我们做一个 TAM 分析。对的？80 亿人类，让我们得到 80 亿机器人。对的？让我们支付他们的平均工资，是的，就这样。10 万亿。超过 10 万亿。
**Lex Frdiman：** 是的。我的意思是，如果到处都有机器人，为什么只有 80 亿个机器人？
**Dylan Patel：** 是的，当然，当然我会得到，我会有一个机器人，你会有 20 个。
**Lex Frdiman：** 是的，我的意思是我看到了一个使用案例。所以，是的，所以我想好处是他们销售的产品，这就是为什么 OpenAI 处于一个棘手的位置，因为他们。
**Nathan Lambert：**OpenAI 现在作为一个品牌的所有价值都在 ChatGPT 中，实际上对于大多数用户来说，他们没有太多的理由需要 OpenAI 在下一个最好的模型上花费数十亿美元，因为他们可以授权 Llama 5，而且价格更便宜。所以这有点像 ChatGPT 对他们来说是一个非常有价值的实体，但他们可以从中赚更多的钱。
**Dylan Patel：** 聊天应用程序显然是这样的，没有足够的空间继续。对，就像标准的聊天，对，你只是用它来回答随机问题之类的。对的。成本继续崩溃。V3 是最新的，最大的，但它会得到广告的支持，对吗？就像，你知道，Llama 梅塔已经服务了 405B，可能会赔钱。但在某种程度上，你知道，他们会得到，模型会变得如此便宜，以至于他们可以在广告支持下免费提供服务。对的？这就是谷歌将能够做到的，而且很明显，他们已经有了更大的影响力，对吧？因此，聊天不会是唯一的用例。就像这些推理代码，代理，计算机使用，所有这些东西都是 OpenAI 在未来赚钱的地方，否则它们就完蛋了。
**Lex Frdiman：** 但是 X，Google 和 Meta 有这些其他产品，所以它没有。OpenAI 和 Anthropic 最终不会消失吗？
**Dylan Patel：** 除非他们很擅长做模特，他们确实很擅长。
**Lex Frdiman：** 但它是如此的先进。
**Nathan Lambert：** 我的意思是，这取决于你认为人工智能的发展方向。
**Lex Frdiman：** 你必须保持胜利。是的，你必须在攀登的过程中保持胜利。即使人工智能的能力发展得超级快，也是令人敬畏的。进入 AGI 的方向，就像 X 在数据方面，谷歌在数据方面，Meta 在数据方面，在其他产品方面，仍然有一个提升。还有钱。还有一大笔钱。
**Dylan Patel：** 整个想法是人类数据被挖掘出来。我们不在乎。我们都关心自我游戏。可验证的。是的。
**Lex Frdiman：** 自我发挥，这是一个研发问题。
**Nathan Lambert：**AWS 并没有在每台机器上赚很多钱。最强大的人工智能平台也是如此，尽管对 API 的调用非常便宜，但拥有该平台仍然可以赚很多钱。因为它是下一个计算层，所以有很多讨论。
**Dylan Patel：** 你必须相信这一点，你知道，有很多讨论认为代币、代币经济学和 LLM API 是下一个计算层或下一个经济范式。就像能源和石油一样。但你也必须相信，API 和聊天并不是人工智能陷入困境的地方。对的。它实际上只是任务、代理、机器人和计算机使用，这些都是所有价值将被交付的领域。不是 API，不是聊天应用程序。对的。
**Lex Frdiman：** 你有没有可能。我的意思是，这一切都变成了一种商品，你有一个非常薄的包装，就像困惑一样。开个玩笑。
**Nathan Lambert：** 有很多包装工赚了很多钱。
**Lex Frdiman：** 是的。但你认为人们有可能忘记 OpenAI 和 Anthropic 是什么吗？因为 API 周围有包装器，而且是动态的。
**Dylan Patel：** 如果模型进展不快。是的，它. 它正在成为一种商品。对的。DeepSeek V3 显示了这一点，但 GPT3 图表也显示了这一点。对的。Llama 3B 比 GPT3 便宜 1200 倍。任何 GPT3，就像任何商业模式是 GPT3 级别功能的人一样，都是死的。任何商业模式是 GPT4 级别功能的人都是死的。
**Nathan Lambert：** 人们常说，现在最好的企业是那些建立在模式变得更好基础上的企业。
**Lex Frdiman：** 对。这就像包装纸一样，在模型的浪潮中乘风破浪。
**Nathan Lambert：** 从短期来看，能赚最多钱的公司是那些能找出适合语言模型生成的广告定位方法的公司。我们有元广告，这些广告在 Feed 中非常有针对性，而不是在特定的内容中。我们有谷歌和亚马逊使用的搜索广告，在搜索上上升了很多。但在 ChatGPT 的回报中，并不清楚如何在输出中获得高质量的投放广告。如果你能在模型成本下降的情况下做到这一点，你就能获得超高的收入，就像收入完全没有被利用一样，技术上也不清楚它是如何做到的。
**Lex Frdiman：** 是的，我指的是谷歌所做的那种 AdSense 创新。有一天你会在 GPT 中输出一个广告，那将会赚到数十亿美元。
**Nathan Lambert：** 它可能非常微妙，它可能在对话中，就像我们现在有语音模式一样。这可能是某种制造它的方法。所以声音介绍了某些事情。这很难衡量，而且需要想象力。但是是的。
**Lex Frdiman：** 也不会这么糟糕。它不会变得可疑。所以你会受到公众的反对，诸如此类的事情。所以你必须把声音调得足够大，才能清楚这是一个广告，并平衡这一切。所以这是他们试图解决的开放问题，人类和 OpenAI。
**Nathan Lambert：** 他们需要，他们可能不会说。
**Dylan Patel：** 我认为他们根本不关心这个。
**Nathan Lambert：** 他们现在不关心这个。我想那是个地方。我认为他们的困惑更多地是在做实验。
**Lex Frdiman：** 哦，有意思。是啊，当然。
**Dylan Patel：** 比如困惑。谷歌元关心这一点。我认为 OpenAI 和 Anthropic 纯粹是专注于 AGI 的激光。代理和 AGI。如果我建立了 AGI，我可以赚很多钱。对的。或者我可以花钱，支付一切。对的。这是，这是，它只是基于类似出口管制的事情。对的。如果你认为 AGI 是 5 年、10 年或更短的时间。对的。这些实验室认为还有两三年的时间。很明显，你的行动是，你知道，如果你假设他们是理性的行动者，他们大多是你在两年内做的事情，而不是五年或十年。非常，非常，非常不同。对的。

## **21.** AI Agents

**Lex Frdiman：** 你认为代理商有前途吗？我们得谈谈这个。这是，这就像今年的兴奋点，代理商将会修订。这是许多商业人士正在使用的通用炒作术语。AI 将彻底改变一切。
**Nathan Lambert：** 好的。所以大多数情况下，“代理人” 这个词显然被夸大了。我们已经讨论了很多关于强化学习作为一种训练可验证结果的方法。智能体应该是开放式的，能够独立解决任务，并能够适应不确定性。有很多术语代理应用于像苹果智能这样的东西，在上一次全球开发者大会之后，我们仍然没有，这是在应用程序之间进行协调，这种类型的工具使用是语言模型可以做得很好的事情。我猜想，苹果智能最终会到来。这是一个封闭的领域，它是你的消息应用程序与你的照片集成，在后台有人工智能，这将是可行的。这被很多软件公司描述为代理。为了进入叙述，问题是我们有什么方法可以让语言模型推广到新的领域，并实时解决它们自己的问题？也许是一些少量的训练，当他们这样做的时候，通过微调自己或在情境学习中，这是在提示中存储信息的想法，你可以使用学习算法来更新它。无论你是否相信这实际上会推广到像我说的，预订两天后去奥斯汀的旅行，我有 XYZ 约束，并且实际上相信它。我认为有一个 HCI 的问题，回来获取信息。
**Lex Frdiman：** 好的，你的预测是什么？因为我的直觉告诉我，我们离那个目标还很远。
**Dylan Patel：** 我认为 OpenAI 的声明，我不知道你是否见过五个级别，其中聊天是第一级，推理是第二级，然后代理是第三级。我认为还有几个层次，但重要的是要注意，对吧，我们聊了几年，对吧？我们只是从理论上进行推理。我们会在这里呆上一两年，对吗？然后是特工。但同时，人们可以尝试并喜欢下一个级别的近似能力，但代理正在自主地做事情，一次做几分钟，一次做几个小时，等等。对的。推理就是一次做几十秒的事情，对吧？然后返回一个输出，我仍然需要验证和使用，并尝试，检查。对的？当然，最大的问题是，制造业也是如此，对吧？这是整个六西格玛的事情。你得了多少个 9？然后你把 9 加在一起，就像如果你乘以六西格玛的步骤数，你就会得到一个产量之类的东西。所以在半导体制造中，几万个步骤，99-9999 是不够的，对吧？因为你乘以这么多倍，你实际上得到了 60% 的收益。
**Nathan Lambert：** 对吗？
**Dylan Patel：** 产量很低。是啊。或者零。这和特工是一样的，对吧？就像每次都把任务链接在一起。LLM，即使是最好的 LLM，在特别好的基准测试中也不能达到 100%，对吧？因为有很多噪音，所以它们会稍微低一点。所以你如何得到足够的 9。对的。这和自动驾驶是一样的。我们不能有自动驾驶，因为没有像谷歌那样的超级地理围栏。对的。即使这样，他们也有一群电话接线员来确保它不会卡住。对的。但你不能这样做，因为它没有足够的 9。
**Lex Frdiman：** 自动驾驶有很多结构，因为道路有规则，有明确的定义，有监管。例如，当你谈论开放网络或开放操作系统的计算机使用时，就像没有。一团糟。所以喜欢这种可能性。我总是对任何负责与人类世界，与开放、混乱的人类世界互动的系统持怀疑态度。
**Nathan Lambert：** 这就是问题所在。如果我们不能获得足够的智能来解决人类世界的问题，我们可以多年来为 Waymo 创建类似于人类操作员的基础设施，以实现某些工作流程。
**Dylan Patel：** 有一家公司，我不记得了，但确实有。但从字面上看，他们的说辞是，是的，当代理失败时，我们只是人类操作员，你只要打电话给我们，我们就能解决问题。它。是的，它就像一个 API 调用，它很搞笑。
**Nathan Lambert：** 当我们有了人类机器人开关时，将会有远程操作市场。当我对我的洗碗机不满意时，世界上会有人很乐意解决这个问题。但这只是特斯拉服务包的一部分。
**Lex Frdiman：** 我只是想象一个 AI Agents 与另一个 AI Agents 交谈。一家公司有一个 AI Agents，专门帮助其他 AI Agents。
**Nathan Lambert：** 但是如果你能在一个步骤中做出好的东西，你可以，是的，你可以把它们堆在一起。所以这就是为什么我。如果它需要很长时间，我们将构建支持它的基础架构。你看到运营商推出，他们与某些网站有合作关系，与 DoorDash，与 OpenTable，与这样的东西。这些伙伴关系将让他们爬得非常快。他们的模型将在这些事情上变得非常出色。这将是一个概念验证，这可能是一个网络效应，更多的公司想让人工智能变得更容易。有些公司会说，不，让我们把拦截器放在适当的位置。
**Lex Frdiman：** 是的。
**Nathan Lambert：** 这就是互联网的故事。我们已经看到了，我们现在看到了语言模型的训练数据，公司就像，不，你必须像企业一样付钱。
**Lex Frdiman：** 也就是说，我认为航空公司有一个非常。酒店有很高的动机让他们的网站工作得很好，他们通常不喜欢你看多少点击才能订购机票，这是疯狂的。我不要。
**Nathan Lambert：** 实际上你再也不能给美国航空公司的代理人打电话了。他们没有电话号码。
**Lex Frdiman：** 我的意思是，对于许多界面前端的人来说，想象代理商能够处理该网站是很可怕的。当我作为一个人类挣扎时，就像我每次试图订机票时都会有生存危机一样，我认为建立一个强大的 AI Agents 将是极其困难的。
**Nathan Lambert：** 但是想想看。曼联已经接受了星联条款，即他们必须免费提供星联。用户会喜欢它的。如果一家航空公司是这样的，我们要花一年的时间，我们要让我们的网站有白色的文字，完美地为 AIS 工作。每当有人询问人工智能航班时，他们就会购买任何航空公司的航班。
**Dylan Patel：** 或者就像他们一样，这里有一个 API，它只暴露给 AI Agents，如果有人质疑它，价格会高出 10%。以及任何航班。但是我们会让你看到我们的任何航班，你可以预订其中的任何一个。给你，马特探员。然后它就像自己的一样，我把价格提高了 10%。极好的。
**Lex Frdiman：** 是的。
**Dylan Patel：** 就像，我愿意说，嘿，给我订一张去看莱克斯的机票。对的？这就像，是的，随便吧。
**Lex Frdiman：** 是的，是的。
**Dylan Patel：** 我认为，我认为，你知道，计算机和现实世界以及开放世界是非常非常混乱的。但如果你开始在狭窄的区域内定义问题，人们将能够创造出非常非常有生产力的东西，并大幅降低成本。对的。就像现在疯狂的事情，比如家庭中的机器人，这些将会变得更加困难。就像自动驾驶一样，因为只有十亿种不同的故障模式。但是代理商可以浏览特定的网站，完成特定的任务，或者给你的冰箱拍照，或者上传你的食谱，然后它会计算出从亚马逊订购什么，全食超市，食品配送，我认为这将是非常快速和容易的。因此，这将是一系列的业务成果，将会有大量的乐观情绪。人们可以找到赚钱的方法。
**Nathan Lambert：** 需要明确的是，这些沙盒已经存在于研究中。有些人克隆了谷歌、亚马逊等所有最受欢迎的网站，这样就有了。我的意思是，OpenAI 可能在内部有他们来训练这些东西。就像 DeepMind 的机器人团队多年来一直拥有机器人集群一样，你可以完全远程地与机器人互动。他们只是在伦敦有一个实验室，你给它发送任务，安排积木，然后你做这个研究。很明显，有文本可以修复这些东西，但我们以前已经改变了这些自动化的曲柄。你从沙盒到进步，然后你一次添加一个更多的领域并推广。我认为在自然语言处理和语言处理的历史上，每个语言模型的任务中的指令调整曾经就像一个语言模型，做一个任务。然后在指令调优文献中，你开始添加越来越多的任务，任务在一起，它开始推广到每个任务，我们不知道我们在这条曲线上的位置。我认为对于这个 RL 和可验证领域的推理，我们是早期的，但我们不知道点在哪里，你只是开始在足够的领域上训练，然后噗，更多的领域开始工作，你已经跨越了泛化障碍。

## **22.** AI 与编程


**Lex Frdiman：** 那么，你对编程环境有什么看法？所以软件工程，这是我个人和我知道很多人与人工智能互动最多的地方。
**Dylan Patel：** 现在的计算机科学学生也有很多恐惧和焦虑。但也有。那是哪里。这可能是人工智能收入和生产力增长最多的领域。对的。不管是副驾驶还是光标等等。对的。这是。或者只是标准的聊天 GPT。对的。就像很多。我不知道，我知道很少有程序员没有聊天 GPT，实际上他们中的许多人都有 200 层，因为这就是它的好处。对的。我认为在那个世界里，我们已经看到了斯韦本奇。我不知道你是否看过一些斯坦福学生所做的基准测试。我不会说这真的很难，但我也不会说这很容易。我认为这需要一个至少经历过几年计算机科学或几年编程的人来做好 Swebench。这些模型在一年内从 4% 上升到 60%。对的。他们明年要去哪里？它会更高，可能不会是 100%，因为 9 是很难做到的。但我们总有一天会到达。然后我们需要更难的软件工程基准测试等等。但人们现在认为它可以很容易地完成代码，它可以生成一些函数，我必须回顾一下。棒极了。但实际上，我认为软件工程代理可以比任何其他代理更快、更快地完成，因为它是一个可验证的领域。你总是可以单元，测试或编译，它有许多不同的区域，可以一次检查整个代码库，这是没有工程师可以做到的。只有架构师才能真正考虑这些东西，真正资深的人，他们可以定义东西，然后代理可以执行它。所以我认为软件工程成本将会疯狂下降。一个有趣的方面是，当软件工程成本非常低时，你会得到非常不同的市场，对吧？所以在美国，你有所有这些平台 SAAS 公司，对吗？ Salesforce 等等，对吧？在中国，没有人使用平台 SAAS。每个人都建立自己的堆栈。因为软件工程在中国要便宜得多，部分原因是 STEM 毕业生的数量，等等。所以这样做通常更便宜。同时，中国很少采用代码 LLM，因为那里的工程师成本要低得多。但是，当每个公司都可以合理地、真正廉价而快速地发明自己的业务时，会发生什么呢？你停止使用平台 SAAS，你开始构建定制的解决方案，你很快就会改变它们。现在，突然之间，你的业务也可能变得更有效率了，因为你不需要处理像一些随机的平台 SAAS 公司的东西不能完美地工作，必须调整工作流程或随机的业务自动化案例，而这些并不一定需要人工智能。这只是需要建立的逻辑，没有人建立过。对的？所有这些事情都可以发生得更快。所以我认为软件，然后，然后另一个领域就像工业化学。机械工程师不擅长编程，对吧？只是一般。他们的工具，比如半导体工程师，他们的工具都是 20 年前的。所有的工具都在 XP 上运行，包括 ASML。平版印刷工具在 Windows XP 上运行。对的？就像。很多分析都是在 Excel 中进行的，对吧？这就像伙计们一样，你们可以用你们收集的所有数据向前移动 20 年，并做得更好。您只需要将软件工程的工程技能交付给实际的领域专家工程师。所以我认为，我认为这是我超级看好人工智能创造价值的领域。
**Nathan Lambert：** 总体情况是，我不认为它会是一个悬崖。我们谈到了一个很好的例子，当 Meta 添加故事时，增长是如何变化的。所以 Snapchat 呈指数增长。他们增加了故事。它停了。软件工程师一直在上面和右边。人工智能要进来了。它可能只会是平的。并不是每个人都会失业。这很难，因为供给修正得更慢。所以学生的数量仍在增长，这将在多年后得到纠正，就像一年的延迟。但工作岗位的数量会发生变化，也许在 20 年、40 年后，工作岗位的数量会大幅下降，但在这几年内，永远不会出现软件工程师没有用处的短暂时刻。
**Lex Frdiman：** 我认为程序员的本质以及程序员所做的工作也发生了变化，因为我认为你所说的一切都需要有一个人参与。在这张图片中有一个非常重要的人，比如纠正代码。
**Dylan Patel：** 上下文长度。
**Lex Frdiman：** 是的。调试也像是通过阅读代码来调试，理解操纵系统，就像不，不，不，你没抓住重点。添加更多的提示，就像是，添加人类设计完美的谷歌按钮。
**Nathan Lambert：** 谷歌以让人们设计完美的按钮而闻名，这就像人工智能如何做到这一点？就像他们可以给你所有的想法。完美的按钮。
**Lex Frdiman：** 我的意思是，这就是你可以称之为品味的东西。人类有。人类可以做的一件事是找出其他人类比人工智能系统更喜欢什么。这就是你加载的首选项。但最终人类是最大的偏好生成器。这就是偏好的来源。
**Nathan Lambert：** 人类实际上非常擅长在两件事之间进行阅读或判断，而这又回到了 RLHF 和偏好调整的核心，即很难为很多问题提供一个好的答案，但很容易看出哪一个更好。这就是我们现在使用人类进行人工智能的方式，判断哪一个更好。这就是软件工程的样子。公关审查这里有几个选项。这里有一些潜在的优点和缺点，他们将成为法官。
**Lex Frdiman：** 我认为我非常推荐的一件事是，人们从程序员开始，开始使用人工智能，并接受人工智能系统的监督者的角色，就像人工智能系统的合作伙伴，而不是从零开始编写，或者根本不学习编码，只是生成东西。因为我认为，作为一名程序员，实际上必须具备相当高的专业知识，才能管理日益智能的系统。
**Dylan Patel：** 我认为是这样的，然后成为某领域的专家。
**Lex Frdiman：** 当然，是的。
**Dylan Patel：** 因为说真的，如果你去看看航空航天或半导体或化学工程，每个人都在使用非常蹩脚的平台，非常旧的软件。数据科学的工作就是个笑话，对吧？在许多情况下，在许多情况下，它是非常真实的。但这就像把人类能力的前沿带到你的领域。即使最前线是来自你的领域的人工智能，你就像在最前线。对的？所以这就像，这就像你必须站在某件事的最前沿，然后利用人工智能的涨潮来做其他事情。
**Lex Frdiman：** 哦，是的。在法律系统中，软件可以帮助实现自动化或数字化，在这方面，到处都有很多低垂的果实。我是说，这就是为什么 Doge 令人兴奋的原因。我和一群执政官一起出去玩，他们，我的意思是，政府就像是守旧派。这就像乞求软件的现代化，组织数据的现代化，所有这些东西。我的意思是，在这种情况下，这是故意的，因为官僚机构保护权力中心等等。但软件打破了这些障碍。因此，它伤害了那些紧握权力的人，但最终造福了人类。所以有很多这样的领域。有一件事我们还没有完全讲完，那就是开源。首先，恭喜你发布了一款新模型。

## **23.** 开源

![alt text](https://poketto.oss-cn-hangzhou.aliyuncs.com/202502031641887.png?x-oss-process=image/resize,w_800)

**Nathan Lambert：** 是的，Tulu。我会解释什么是 Tulu。Tulu 是一种杂交骆驼。当你用巴克里亚骆驼繁殖单峰骆驼时。早在 ChatGPT 之后的早期，就出现了一大批模型，比如羊驼、小羊驼等等，它们都是以各种哺乳动物命名的。所以 Tulu 是一个有多年历史的品牌，这就是原因。我们一直在使用开源代码进行岗位训练的前沿。这个版本的第一部分是在秋季发布的，我们建立了 Llama 的开放模型，开放重量模型，然后我们添加了完全开放的代码或完全开放的数据。有一个流行的基准是聊天机器人竞技场，这通常是评估这些聊天模型的指标，它是人类。比较来自不同组织的随机模型。如果你看一下 11 月或 12 月的排行榜，在前 60 个模型中，从 10 个到 20 个组织中，没有一个组织有开放代码或数据，仅用于后训练。其中，甚至更少或没有预训练数据和代码可用。但在这个时候，岗位训练要容易得多。它仍然很便宜，你可以这样做。问题是，在人们可以访问所有代码和数据的情况下，我们能把这个数字推到多高？这就是这个项目的动机。我们从 Llama 身上吸取教训。英伟达有一个 Nematron 模型，其中他们的后期训练配方是相当开放的，有一些数据和一篇论文。它把所有这些放在一起，试图创造一个配方，让人们可以根据自己的领域对 GPT4 等模型进行微调。
**Lex Frdiman：** 所以要明确的是，在 Tulu 的情况下，也许你也可以谈论阿尔玛。但在图尔的情况下，你要把 Llama 3，4，5B 带给卢。
**Nathan Lambert：** 这是一系列岗位训练的食谱。所以我们多年来做了很多模型。
**Lex Frdiman：** 所以你把所有东西都开源了？
**Nathan Lambert：** 是的，如果你从一个开放的基于重量的模型开始，整个模型在技术上是开源的，因为你不知道骆驼在里面放了什么，这就是为什么我们有一个单独的东西。但它只是获得了人们可以放大和定制的管道的一部分。我知道，我从创业公司和企业那里听说，好吧，我可以参加这个职位的训练，并尝试将它应用到我的领域。我们经常谈论验证器。我们使用这个想法，即带有可验证奖励的强化学习。RLVR 类似于 RLHF。我们今天把它应用到数学和模型中，就像我们把它应用到去年的 Llama 405B 基础模型中一样，我们还有其他的东西，我们有我们的指令调整和偏好调整。但是数学的东西很有趣，它更容易改进这个数学基准。所有的资本模型都有一个基准并购数学。当基准名称是您正在评估的区域时，使用严格的名称。我们是研究人员，不是品牌战略家。这也是 DeepSeek 论文中谈到的，就像在这个更大的模型中，通过这种 RL 训练更容易获得强大的能力。然后他们把它从大模型提取到小模型，我们今天发布的这个模型，我们看到了同样的事情。我们在 AI2，我们没有大量计算机。我们不能一直训练 405B 模型。所以我们只是做了几次跑步，它们往往会起作用。它只是表明人们在这些事情上有很大的发挥空间。
**Dylan Patel：** 他们粉碎了 Llama 的实际释放，对吗？他们比它好多了。
**Nathan Lambert：** 是的。所以我们的评估数字，我的意思是，我们有额外的月份，但我们的评估数字比他们发布的 Llama Instruct 模型要好得多。
**Lex Frdiman：** 然后你也说比 DeepSeek V3 更好。
**Nathan Lambert：** 是的。在我们的评估基准测试中，DeepSeek V3 非常相似。我们有一个安全基准来理解，如果它会说有害的事情和类似的事情，这是最大的方式，它仍然是。
**Dylan Patel：** 这就像是多个基准的合并。你什么意思？
**Nathan Lambert：** 是的。所以我们有 10 个评估。这是训练后的标准做法，你可以选择你关心的评估。在学术和较小的实验室，你会有较少的评估。在公司里，你会有一个你真正关心的领域。在前沿实验室，你会有几十到二十个甚至可能是 100 个特定事物的评估。所以我们选择了一套有代表性的东西，就像聊天，精确的指令跟随，也就是回应。只有在表情符号中，模型才会遵循奇怪的东西，比如数学代码。你创建了一个这样的套件。因此，安全将是这类套件中的 10 个之一，在这类套件中，更广泛的人工智能社区关心什么？例如，与 DeepSeek 相比，我们的模型的平均评估是 80，包括安全和类似的。如果没有安全性，DeepSeq 的平均得分将达到 79%。他们的安全评分会带来它。
**Dylan Patel：** 下降到喜欢，哦，所以你击败他们甚至忽视安全。
**Nathan Lambert：** 是的。所以这是内部的事情，我不想只通过你如何塑造评估基准来赢得胜利。所以如果有这样的东西，人们可能会也可能不会关心他们模型中的安全性。安全可以顺流而下。安全可以是当你为 API 托管模型时。比如，安全是在 AI 应用程序的一系列位置中解决的。所以这就像如果你想说你有最好的食谱，你不能只是把它关在这些东西上，有些人可能不想要。这就像是进步的时代。如果我们能晚些时候发布一个模型，我们就会受益，我们有更多的时间来学习新的技术，比如 RL 技术。我们在秋天就开始了。它现在在推理模型中非常流行。开源后训练的下一步是扩大验证器，扩大数据，复制 DeepSeq 的一些结果。很棒的是，我们有一张纸可以画，这让它变得容易多了。这就是在人工智能的学术和封闭前沿研究中正在发生的事情。
**Lex Frdiman：** 既然你在推动开源，你认为它的未来是什么？你认为 DeepSeek 实际上改变了一些事情，因为它是开源或开放的，或者正在推动开源运动向开放的方向发展？
**Nathan Lambert：** 这可以追溯到许可证的讨论。因此，使用友好许可证的 Deepseak R1 是一次重大重置。因此，这就像我们第一次有了一个真正清晰的边界模型，它是开放的权重，并且具有商业友好的许可证，对下游用例、合成数据提取等没有任何限制。在人工智能的历史上，从来没有出现过这种情况。在过去的几年里，自从 ChatGPT 以来，已经有一些模型脱离了前沿，或者有一些奇怪的许可证，你不能真正使用它们。
**Dylan Patel：** 那么，除了五家公司外，Meta 的许可证是不是几乎是允许的？
**Nathan Lambert：** 所以这就涉及到什么是开源人工智能。在 Llama 许可证中也有用例限制，即您不能将其用于特定的事情。所以如果你有开源软件的背景，你会说那不是开源。
**Dylan Patel：** 那些是什么样的东西？
**Dylan Patel：** 就像，他们就像在这一点上，我不能把他们从我的头上拉下来。
**Lex Frdiman：** 但它会像竞争对手一样。
**Nathan Lambert：** 它曾经是军事用途之一，他们为了规模而删除了它。它就像 CSAM，就像虐待儿童的材料。就像这是那里禁止的事情一样。但从开源背景来看，这足以说明它不是开源许可证。此外，骆驼许可证有一个可怕的东西，如果你把它和骆驼模型接触，你必须给你的模型命名为骆驼。所以这就像是品牌的事情。因此，如果一家公司在技术上使用 Llama ，许可证上说他们应该在应用程序的底部使用 Llama 构建。从营销的角度来看，这很伤人。作为一名研究员，我可以接受。我想，哦，这很好。这次发布的所有材料上都写着 “Llama 冲刺”。但这就是为什么我们需要真正开放的模型，即我们不知道 DeepSeek R1 的数据。
**Dylan Patel：** 所以你的意思是我不能做一个廉价的 Llama 复制品，假装它是我的，但我可以用中国的模式来做这个？
**Nathan Lambert：** 是的，当然，这就是我要说的。这就是为什么我们想要整个开放语言模型的原因。奥尔莫的事情是试图保持模型，其中一切都是开放的，数据尽可能接近边界。所以我们的计算受到限制，我们的人员受到限制，我们依赖于从像约翰 · 舒尔曼这样的人那里获得洞察力，他告诉我们要在输出上做 RL。我们可以实现这些大的飞跃，但需要很长时间才能推动开源的前沿。而且，从根本上说，我想说的是，这是因为开源人工智能没有与开源软件相同的反馈回路。我们还讨论了用于安全的开源软件。这只是因为你曾经建立了一些东西，如果你进入一家新公司，你可以重复使用它，有这么多的好处。但是如果你开源一个语言模型，你有这些数据，你有这些训练代码，要有人来构建和改进并不容易，因为你需要在计算上花很多钱，你需要有 X 专业知识。因此，在开源人工智能的反馈循环出现之前，它似乎主要是一种意识形态使命。像马克 · 扎克伯格这样的人就像这样，美国需要这个。我同意他的观点。但在意识形态动机很高的时候，我们需要利用并建立这个生态系统。查看语言模型数据有什么好处？关于这一点并没有太多。我们将很快推出一个演示，您可以在其中查看 OLMO 模型和查询，并查看与其相似的预训练数据，这在法律上是有风险和复杂的。但这就像，看到人工智能训练的数据意味着什么？它很难解析，它是兆兆字节的文件。
**Dylan Patel：** 这就像，我不知道我是什么。
**Dylan Patel：** 去那里找。但如果人们希望开源人工智能在经济上有用，这就是我们作为一个生态系统所需要做的。

## **24.** Stargate （OpenAI 星际之门）

**Lex Frdiman：** 我们并没有真正谈论星际之门。我很想听听你对新政府、特朗普政府、美国方面所做的一切以及支持人工智能基础设施和不同人工智能公司的努力的看法。你觉得星际之门怎么样？我们该怎么看待星际之门？萨姆拿到钱了吗？
**Dylan Patel：** 是的。所以我认为星际之门是一个不透明的东西。它肯定没有 5000 亿美元，甚至没有 1000 亿美元。对的。所以他们宣布的是这个 5000 亿美元的数字。拉里 · 埃里森、萨姆 · 奥特曼和特朗普说了这句话，他们感谢特朗普，这句话被使用了。特朗普确实采取了一些行政措施，比如，确实显著提高了更快建设的能力。你知道，他所做的行政行为之一是在联邦土地上，你基本上可以在电力中建立数据中心，你知道，就像，就像那样。然后许可程序基本上就没有了，或者你在事后申请。所以，就像，其中一个。又一次，就像，我之前有过精神分裂。又一次精神分裂。如果你曾经去过普雷西迪奥和旧金山，美丽的地区，如果你愿意，你可以在那里的数据中心建一个发电厂，因为那里是联邦土地。它曾经是一个军事基地。但是，你知道，很明显，这会让人们生气。你知道，这是一个很好的。不管怎样，特朗普。特朗普让这件事变得容易多了。对的。一般来说，德克萨斯州拥有唯一不受监管的电网。在国内也是如此。
**Lex Frdiman：** 我们走吧，德克萨斯。
**Dylan Patel：** 所以，你知道，因此，就像 ERCOT 也能让人们更快地建造。此外，联邦法规即将出台。所以星际之门被预言了。这就是整场演出发生的原因。现在，我不知道他们是如何得出 5000 亿美元这个数字的。他们是如何得出 1000 亿美元这个数字的，这在某种程度上是有道理的。对的。实际上这里有一张很好的桌子，我想在我的星际之门作品中展示一下。这是最近的一次。是啊。所以无论如何，星际之门，你知道，基本上是正确的。就像，有。那里有一张关于成本的桌子。你已经通过了。是那个。所以这张表解释了发生了什么。对的。星门在德克萨斯州的阿比林。第一个一千亿美元。在大约 1.8 千兆瓦的电力消耗中，该站点的电力为 2.2 千兆瓦。对的。每个 GPU。他们大概有。在星际之门出现之前，甲骨文已经在建造它的第一部分，需要说明的是，他们已经建造了一年。事实上，他们想把它租给埃隆。对的。但埃隆说，这太慢了。我需要快点。然后他去做了他在孟菲斯的事。所以 OpenAI 能够通过这个叫做星际之门的奇怪的合资企业来获得它。他们最初与 Just Oracle 就该集群的第一部分签署了协议。这个集群的第一部分大约是 50 亿到 60 亿美元的服务器支出，对吗？此外，还有 10 亿美元左右的数据中心支出。但是，然后，然后同样地，如果你用未来两代英伟达的芯片 GB200，GB300，VR200 填满整个 1.8 千兆瓦，并且你完全填满它，这最终大约是 500 亿美元的服务器成本，对吗？再加上数据中心成本，再加上维护成本，再加上运营成本，再加上所有这些东西。这就是 OpenAI 宣布获得 1000 亿美元的原因，对吧？因为他们谈到 1000 亿美元是第一阶段。这是阿比林，德克萨斯数据中心， 是吧？1000 亿美元的总拥有成本，报价，报价。对的？所以这不是资本支出，也不是投资，而是 1000 亿美元的总拥有成本。然后会有未来的阶段。顺便说一下，他们正在寻找其他比这个 2.2 千兆瓦更大的地方，在德克萨斯州和其他地方。所以你知道，他们并没有完全忽视这一点。但是，他们说第一阶段需要 1000 亿美元，我认为这是会发生的。他们甚至没有钱买那个。此外，这不是 1000 亿美元，而是 500 亿美元的支出。对的？然后像 500 亿美元的运营成本，电力，等等，租金定价，等等，因为他们租用它。OpenAI 从 Stargate 合资公司租用 GPU。对的？对的。他们到底有多少钱？对的。软银。软银要投资了。甲骨文要投资了。OpenAI 要投资了。OpenAI 即将获得 190 亿美元。每个人都知道他们在上一轮中只得到了 60 亿美元和 40 亿美元的债务。所以，有消息说软银可能会向 OpenAI 投资 250 亿美元。对的？所以那是，那是，那是它的一部分，对吗？所以 190 亿美元可以从那里来。所以 OpenAI 根本没有钱，对吧？需要说明的是，墨水在任何东西上都不会变干。OpenAI 在这 500 亿美元中有 0 美元，他们在法律上有义务将 190 亿美元的资本支出投入到合资企业中。剩下的钱他们将通过租用合资公司的 GPU 来支付。然后是，然后是甲骨文。甲骨文有很多钱。他们正在完全建造第一部分。他们在为自己花钱，对，这 60 亿美元的资本支出，100 亿美元的总拥有成本。但是他们，他们要做第一部分。他们为此付出了代价，对吧？至于剩下的部分，我不知道拉里想花多少钱。对的。在任何时候他都可以退出，对吧？又是这样， 完全是自愿的。所以在任何时候，这上面都没有签名。对的。但他可能会贡献数百亿美元。对的。需要说明的是，他拿到了钱，甲骨文拿到了钱。还有像 MGX，这是阿联酋基金，从技术上讲，它有 1.5 万亿美元用于投资人工智能。但是，我不知道那笔钱有多真实。就像，尽管没有公司签约，软银也没有 250 亿美元的现金。他们不得不出售他们在 ARM 的股份，你知道，这是 CPU 的领导者，他们，他们首次公开募股。这显然是他们一直想做的。他们只是不知道他们将在哪里重新部署资本。卖掉 ARM 的股份很有道理。所以他们可以把它卖掉，如果他们愿意的话，他们可以投资这个，如果他们愿意的话，他们可以投资 OpenAI。就像资金担保一样，第一个 10 万 GB200 集群就像，可以资助，被资助。在那之后的一切都悬而未决。钱来了。我相信钱会来的。我个人认为。
**Lex Frdiman：** 这是一种信念，好吧。
**Dylan Patel：** 这是一种信念，他们将发布更好的模型，并能够筹集更多资金。
**Lex Frdiman：** 是的，没错。
**Dylan Patel：** 但是，事实是，埃隆是对的。有钱是不存在的。对的。
**Lex Frdiman：** 这和美国政府有什么关系？特朗普和这一切有什么关系？他只是个炒作的人。
**Dylan Patel：** 特朗普是。他正在减少监管，这样他们就可以更快地建造它。对的。他允许他们这么做。对的。你知道，因为这方面的任何投资都会涉及到反垄断的问题。对的？就像，很明显他会，他会允许他们这么做。他将使法规能够真正允许它的建造。不过，我不相信有任何美国政府的美元被花在这上面。
**Lex Frdiman：** 是的。所以我认为他也在创造一种普遍的氛围，这是监管将会下降，这是建设的时代。所以，如果你是一个建设者，你想创造的东西，你想推出的东西，这是时间去做。
**Dylan Patel：** 你看，我们的数据中有这个 1.8 千兆瓦的数据中心已经有一年了，我们一直在把它发送给我们所有的客户，包括许多正在建设多个千兆瓦的公司。但这就像在一个水平上，也许高管们不太喜欢看到 5000 亿美元，1000 亿美元，然后每个人都在问他们，所以这可能会刺激另一场更快的军备竞赛。对的。因为军备竞赛已经开始了。但就像这样，这就像特朗普在电视上谈论的 1005-0000-00000 美元的数字，就像它可以刺激军备竞赛变得更快，更多的投资者涌入等等，等等。所以我认为，我认为你在这个意义上是正确的，OpenAI 或特朗普有点像拥护人们会建造更多，他的行动会让人们建造更多。

## **25.** AI 的未来

**Lex Frdiman：** 你对即将到来的这几年感到兴奋的是什么？在集群建设方面，在人工智能的突破方面，比如在未来几年，两年，三年，四年里你可以想象最好的未来。那看起来像什么？只是它可能是非常具体的技术问题，比如岗位训练的突破。也可能只是大号的。
**Dylan Patel：** 是的，我的意思是它令人印象深刻。
**Lex Frdiman：** 集群。
**Dylan Patel：** 我真的，我真的喜欢跟踪供应链，喜欢谁参与了什么。我真的知道。看到这些数字，成本，谁在建设什么能力，帮助他们计算出他们应该建设多少能力，赢得交易，战略性的东西，这真的很有趣。那真的很酷。我认为在技术上，网络方面有很多东西让我对光学和电子学感到兴奋。对的。就像越来越接近，无论是 CO 封装光学还是某种类似的新形式的开关。
**Lex Frdiman：** 这是集群内部的。
**Dylan Patel：** 集群，是的。还有多数据中心训练。对的。就像人们在这些数据中心之间铺设了这么多光纤，并用这么多不同的东西照亮它，你知道，有这么多的带宽，在这一端发生了很多有趣的事情。对的。自从 5G 以来，电信一直很无聊，现在它真的很令人兴奋。再说一次，你能教育我一下吗？
**Lex Frdiman：** 关于事物的速度？因此，数据中心之间的内存速度、互连速度和光纤速度是否存在数量级差异？我们能不能在某一点上汇聚到一个地方，让一切都感觉像是一台电脑？
**Dylan Patel：** 不，我认为这是不可能的。好吧。编程只会变得更难，而不是更容易。它只会变得更困难、更复杂、层次更多，对吗？人们喜欢拥有的普遍形象就像这种记忆的层次结构。所以在芯片上是非常接近的，在芯片内定位，对吧？你有登记簿，对吧？且它们在一些计算元件之间共享。然后，您将拥有在更多计算元素之间共享的缓存。然后你有内存，对，像 HBM 或 DRAM，像 DDR 内存或其他什么，这是在整个芯片之间共享的。然后你可以在许多芯片之间共享内存池，对吧。然后是存储。你一直在走神。对，对。跨数据中心的访问延迟、跨芯片内的数据中心的访问延迟是不同的。所以就像你显然总是，你总是会有不同的编程范例。这不是一件容易的编程工作。这东西会很难。也许人工智能能帮上忙，对吧？你知道，通过编程。但是，思考这个问题的方式是，就像你在任务中添加的元素越多，你就不会得到强大的扩展。如果我将芯片数量增加一倍，性能也不会提高一倍。这就是计算的现实，因为效率低下，而且有很多有趣的工作正在进行，以使其更加线性，无论是使芯片更紧密地连接在一起，还是很酷的编程模型，或者你可以在模型端做的很酷的算法。DeepSeq 做了一些非常酷的创新，因为它们在互连方面受到限制，但它们仍然需要并行化。对的。就像各种各样的，你知道，每个人都在做一些事情。谷歌有一大堆工作，每个人都有一大堆关于这个的工作。那东西超级刺激。在模型、工作量和创新方面，对吧？ 硬件固态 Transformer 很有趣，对吧？对于电源方面，电池上有各种各样的东西，还有各种各样的东西，你知道，我认为，我认为当你看，如果你看计算堆栈的每一层，对，无论是从光刻和蚀刻一直到制造，到光学，到网络，到电源，到 Transformer，到冷却，到网络，你只需要在堆栈中不断上升。即使是数据中心的空调也在不断创新。铜缆正在创新。你不会想到它，但铜电缆。在如何包装它们的密度方面有一些创新。它是堆栈的所有这些层，一直到模型。人类的进步速度是前所未有的。
**Lex Frdiman：** 我只是想象你坐在一个到处都是屏幕的地方，只是监控供应链，所有这些集群就像你收集的所有信息一样，我是说你，有一个大的。
**Dylan Patel：** 团队，有一个很大的团队。
**Lex Frdiman：** 你在半分析方面做了令人难以置信的工作。我的意思是，它只是让你在数字世界中把握人类文明的脉搏。这很酷，就像只是看着。感觉到了吗？
**Dylan Patel：** 是的，谢谢。
**Lex Frdiman：** 我想感觉，感觉我们所有人都喜欢做狗屎，史诗般的狗屎。
**Dylan Patel：** 感受 AGI。
**Lex Frdiman：** 我的意思是从模因到现实。Nathan，有没有你期待的潜在突破？
**Nathan Lambert：** 听着迪伦美妙的回应，我花了一段时间来思考这个问题。
**Dylan Patel：** 他不听我的话，他太笨了。
**Nathan Lambert：** 我知道，不，我知道这是迟早的事。这就像现实训练模型是非常有趣的，因为有这么多低垂的果实。让我的工作变得有趣的是，我训练模特，我写关于模特的分析，这很有趣，因为显然还有更多的进步。我在一个可以分享东西的地方做这件事的真正动机是。我不相信那些说 “相信我，兄弟，我们会让人工智能变得更好” 的人。就像是我们才是。这就像我们要做这件事，你可以信任我们，我们将拥有所有的人工智能，这就像我希望未来有更多的人对人工智能有发言权，并能理解它，这就不那么有趣了，这不是一件积极的事情，这真的很有趣。就像训练模型很有趣，把人带进来也很有趣，但这真的很像人工智能，如果它要成为我一生中最强大的技术，就像我们需要很多人参与制作。
**Lex Frdiman：** 让它变得开放，让它变得尽可能开放。是啊。
**Nathan Lambert：** 在我过去几年的阅读中，更多的开放将有助于人工智能生态系统，让更多的人了解正在发生的事情。而不是从非人工智能领域到政府到一切领域的研究人员。这并不意味着开放永远是答案。我认为到那时，它将重新评估人工智能面临的最大问题是什么，并从一个不同的角度来看待我们正在进行的疯狂之旅。
**Lex Frdiman：** 对我来说，即使是从用户体验来看，任何时候你都会有像 “啊哈” 这样的冷漠时刻，就像魔法一样，就像看到推理，思维的链条，就像有一些东西从根本上是美丽的。这是给我们自己放一面镜子，看起来就像哦，妈的。正如这些公司老生常谈的目标一样，它正在解决智能问题。你就会明白为什么我们人类是特别的。我们内心的智慧是特别的。现在也是为什么我们是特别的，因为我们似乎是有意识的，而人工智能系统现在不是。我们要解决，我们要探索这个谜团。所以，去探索这些问题真的很酷，我不认为我，我从来没有想象过会有可能回来，所以只是兴奋地看着深蓝大卡斯帕罗夫。就像我从来没有想过这种人工智能在我的有生之年是可能的。这就像是真的感觉像人工智能。这太不可思议了。
**Nathan Lambert：** 我从学习驾驶纤毛四旋翼飞机的人工智能开始。就像学习飞行一样。就像它学会了飞起来一样。它会撞到天花板，然后停下来抓住它。这就像，好吧，与现在发生的事情相比，这真的很愚蠢。
**Lex Frdiman：** 现在你可以用自然语言告诉它学习飞行。它将生成所需的控制算法。
**Nathan Lambert：** 可能有低水平的拦截者，就像我们不得不做一些奇怪的事情一样。
**Lex Frdiman：** 但你可以回到我们的机器人对话。是啊。当你必须在真实的物理世界中互动时，这很难。是什么让你对人类文明的未来充满希望？展望未来 10 年，100 年。千年。你认为我们能坚持多久？你认为我们有一千年？
**Nathan Lambert：** 人类肯定会在一千年内出现。我想可能会有非常糟糕的事情发生。人类会少很多，但人类很擅长生存。有很多事情都是真的。我不认为他们是必要的。我们擅长风险的长期信用分配，但当风险变得迫在眉睫时，我们倾向于解决问题。由于这个原因，像 AGI 这样的东西有物理限制，递归改进可以杀死我们所有人。我是因为身体的原因，也是因为人类以前是如何解决问题的。我不太担心人工智能的接管。国际上还有其他令人担忧的事情，但这只是人类基本的善良，并试图放大这一点。我认为我们正处于一个脆弱的时期，我的意思是，如果你把人类作为一个整体来看，有时事情会倒退，有时事情根本不会发生，而我们现在正处于一个非常积极的轨道上。
**Lex Frdiman：** 是的，似乎有进步，但就像权力一样，有人类痛苦的尖峰，我们想尽量减少尖峰的数量。
**Dylan Patel：** 一般来说，人类遭受的痛苦会少很多。对的。我对此非常乐观。随着人工智能变得越来越普遍和强大，那些控制它的人可以做越来越多的事情，我确实担心像技术法西斯主义之类的东西会出现。也许它不会杀死我们所有人，但在某一点上，每一个非常强大的人类都会想要一个脑机接口，这样他们就可以与 AGI 互动，并以更多的方式与其所有的优势融合，你知道，有点像。它的能力或那个人的能力可以比其他任何人更好地利用这些能力，因此，你知道，它不会是一个人统治所有人，但它会是，你知道，我担心的是，它会像少数人一样，你知道，你知道，数百，数千，数万，也许数百万人统治剩下的人。对的。以及它周围的经济。对的。我认为，这就像，可能更令人担忧的事情是人类和机器的融合。这使个人能够对世界产生更大的影响，这种影响既可以是积极的，也可以是消极的。对的。一般来说，人类对世界有积极的影响，至少在社会上是这样。但个人有可能产生这种负面影响，而 AGI，至少我认为实验室对它的定义是，它不是，不是一种失控的有意识的东西，而是一种可以完成很多任务的东西，真正有效地放大了造成极端破坏的人的能力。但在大多数情况下，我认为它将被用于追求利润的动机，这将减少，这将增加事物的丰富和供应，从而减少痛苦。对的？
**Nathan Lambert：** 这就是我们的目标。
**Lex Frdiman：** 在时间线上滚动，只是滚动，停滞。
**Nathan Lambert：** 滚动掌握着世界的现状。
**Dylan Patel：** 这是一个积极的结果，对吗？就像如果我有食物管，我在滚动，我很高兴。
**Lex Frdiman：** 在向宇宙扩张的过程中，这是一个积极的结果。好吧，这是一个充满乐趣的时刻，感谢你们推动人类可能的前沿，感谢你们今天的发言。这很有趣。
**Nathan Lambert：** 谢谢你邀请我们。
**Dylan Patel：** 谢谢你邀请我们。
**Lex Frdiman：** 感谢您收听与 Dylan Patel 和 Nathan Lambert 的对话，以支持本播客。请在描述中查看我们的赞助商。现在让我把理查德 · 费曼的一些话留给你们。对于一项成功的技术来说，现实必须优先于公共关系，因为自然不会被愚弄。感谢您的聆听，希望下次再见。
